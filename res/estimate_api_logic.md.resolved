# Logic API Estimate Story Point

## üéØ T·ªïng Quan

API estimate story point s·ª≠ d·ª•ng **Machine Learning** (XGBoost) k·∫øt h·ª£p v·ªõi **Embeddings** ƒë·ªÉ d·ª± ƒëo√°n ƒë·ªô ph·ª©c t·∫°p c·ªßa task, sau ƒë√≥ l√†m tr√≤n theo chu·∫©n **Planning Poker**.

---

## üîÑ Lu·ªìng X·ª≠ L√Ω

```mermaid
graph TD
    A[Input: title + description] --> B[Combine Text]
    B --> C[Generate Embeddings]
    C --> D[Normalize Vector]
    D --> E[Extract Features]
    E --> F[Scale Features]
    F --> G[Combine: Embeddings + Features]
    G --> H[XGBoost Prediction]
    H --> I[Raw Score]
    I --> J[Planning Poker Rounding]
    J --> K[Final Suggestion]
```

---

## üìù Chi Ti·∫øt Logic

### **B∆∞·ªõc 1: Nh·∫≠n Input**

```python
# Endpoint: POST /ai/llm/estimate_sp
{
  "title": "Implement user authentication",
  "description": "Create login/logout with JWT"
}
```

### **B∆∞·ªõc 2: K·∫øt H·ª£p Text**

```python
raw_text = f"{title} {description}"
# ‚Üí "Implement user authentication Create login/logout with JWT"
```

---

### **B∆∞·ªõc 3: Generate Embeddings**

**Model:** `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`

```python
# T·∫°o embedding vector (384 chi·ªÅu)
emb_list = self.embed_model.embed_query(text)
emb = np.array(emb_list, dtype=float)

# Normalize th√†nh unit vector
norm = np.linalg.norm(emb)
if norm > 0:
    emb = emb / norm  # Vector ƒë∆°n v·ªã
```

> **L√Ω do normalize:** ƒê·∫£m b·∫£o c√°c vector c√≥ c√πng ƒë·ªô d√†i, ch·ªâ so s√°nh h∆∞·ªõng (semantic meaning)

---

### **B∆∞·ªõc 4: Tr√≠ch Xu·∫•t Features B·ªï Sung**

```python
# 2 features th·ªß c√¥ng
word_count = len(text.split())      # S·ªë t·ª´
char_count = len(text)              # S·ªë k√Ω t·ª±

extra = pd.DataFrame([[word_count, char_count]], 
                     columns=["word_count", "char_count"])
```

---

### **B∆∞·ªõc 5: Scale Features**

**Scaler:** MinMaxScaler (ƒë√£ train tr∆∞·ªõc, l∆∞u trong [scaler.pkl](file://wsl.localhost/Ubuntu-24.04.3/home/doanngocnam/projects/taskai/ai_service/app/models/scaler.pkl))

```python
extra_scaled = self.scaler.transform(extra)
# Chuy·ªÉn v·ªÅ range [0, 1]
```

---

### **B∆∞·ªõc 6: K·∫øt H·ª£p Features**

```python
# Embedding (384 dim) + word_count + char_count = 386 features
X_input = np.hstack([emb.reshape(1, -1), extra_scaled])
# Shape: (1, 386)
```

---

### **B∆∞·ªõc 7: D·ª± ƒêo√°n v·ªõi XGBoost**

**Model:** [xgb_storypoint.pkl](file://wsl.localhost/Ubuntu-24.04.3/home/doanngocnam/projects/taskai/ai_service/app/models/xgb_storypoint.pkl) (XGBRegressor)

```python
pred = self.xgb_model.predict(X_input)[0]
# Output: s·ªë th·ª±c (v√≠ d·ª•: 7.23)
```

**Model ƒë√£ ƒë∆∞·ª£c train tr√™n:**
- Historical tasks v·ªõi story points th·ª±c t·∫ø
- Embeddings c·ªßa title + description
- Word count v√† char count

---

### **B∆∞·ªõc 8: L√†m Tr√≤n Theo Planning Poker**

**Planning Poker Scale:** `[0.5, 1, 2, 3, 5, 8, 13]`

```python
def suggest_story_point(value: float) -> str:
    STORY_POINTS = [0.5, 1, 2, 3, 5, 8, 13]
    
    # T√≠nh kho·∫£ng c√°ch ƒë·∫øn t·ª´ng gi√° tr·ªã
    diffs = [(abs(value - sp), sp) for sp in STORY_POINTS]
    diffs.sort(key=lambda x: x[0])
    
    best = diffs[0][1]      # G·∫ßn nh·∫•t
    second = diffs[1][1]    # G·∫ßn th·ª© 2
    
    # N·∫øu n·∫±m gi·ªØa 2 gi√° tr·ªã (kho·∫£ng c√°ch < 0.4)
    if abs(value - best) < 0.4 and abs(value - second) < 0.4:
        return f"{best} - {second}"  # V√≠ d·ª•: "5 - 8"
    
    return str(best)  # V√≠ d·ª•: "8"
```

**V√≠ d·ª•:**
- `value = 7.23` ‚Üí g·∫ßn nh·∫•t l√† `8` v√† `5`
- `|7.23 - 8| = 0.77 > 0.4` ‚Üí ch·ªâ tr·∫£ v·ªÅ `"8"`
- `value = 4.8` ‚Üí `|4.8 - 5| = 0.2`, `|4.8 - 3| = 1.8` ‚Üí tr·∫£ v·ªÅ `"5"`
- `value = 4.2` ‚Üí `|4.2 - 5| = 0.8`, `|4.2 - 3| = 1.2` ‚Üí tr·∫£ v·ªÅ `"5"`

---

## üìä Output Format

```json
{
  "model_estimate": 7.23,           // Gi√° tr·ªã th√¥ t·ª´ XGBoost
  "suggested_story_point": "8"      // L√†m tr√≤n theo Planning Poker
}
```

Ho·∫∑c khi n·∫±m gi·ªØa 2 gi√° tr·ªã:
```json
{
  "model_estimate": 4.5,
  "suggested_story_point": "3 - 5"
}
```

---

## üîß Models & Files

### **Models ƒê∆∞·ª£c Load**

| Model | File | Purpose |
|-------|------|---------|
| **XGBoost** | `models/xgb_storypoint.pkl` | D·ª± ƒëo√°n story point |
| **Scaler** | `models/scaler.pkl` | Normalize word_count, char_count |
| **Embeddings** | HuggingFace model | Chuy·ªÉn text ‚Üí vector 384 chi·ªÅu |

### **Models Loader**

[models_loader.py](file:///\\wsl.localhost\Ubuntu-24.04.3\home\doanngocnam\projects\taskai\ai_service\app\services\models_loader.py)

```python
class ModelsLoader:
    @staticmethod
    def xgb_model():
        # Load t·ª´ models/xgb_storypoint.pkl
        
    @staticmethod
    def scaler():
        # Load t·ª´ models/scaler.pkl
        
    @staticmethod
    def embeddings():
        # Load sentence-transformers model
```

---

## üéì Ki·∫øn Th·ª©c N·ªÅn

### **T·∫°i Sao D√πng Embeddings?**

- **Semantic Understanding:** Hi·ªÉu nghƒ©a c·ªßa task, kh√¥ng ch·ªâ t·ª´ kh√≥a
- **Multilingual:** Model h·ªó tr·ª£ c·∫£ ti·∫øng Vi·ªát v√† ti·∫øng Anh
- **Transfer Learning:** T·∫≠n d·ª•ng ki·∫øn th·ª©c t·ª´ millions of sentences

### **T·∫°i Sao Th√™m word_count & char_count?**

- **Complexity Indicator:** Task d√†i th∆∞·ªùng ph·ª©c t·∫°p h∆°n
- **Complement Embeddings:** B·ªï sung th√¥ng tin v·ªÅ ƒë·ªô d√†i m√¥ t·∫£
- **Improve Accuracy:** K·∫øt h·ª£p semantic + statistical features

### **T·∫°i Sao D√πng XGBoost?**

- **Tabular Data:** Ph√π h·ª£p v·ªõi d·ªØ li·ªáu d·∫°ng b·∫£ng (features)
- **Non-linear Patterns:** H·ªçc ƒë∆∞·ª£c m·ªëi quan h·ªá ph·ª©c t·∫°p
- **Fast Inference:** D·ª± ƒëo√°n nhanh, ph√π h·ª£p production

---

## üß™ V√≠ D·ª• Th·ª±c T·∫ø

### **Case 1: Task ƒê∆°n Gi·∫£n**

```json
{
  "title": "Fix typo in README",
  "description": "Update documentation"
}
```

**Process:**
1. Embeddings ‚Üí low complexity vector
2. word_count = 5, char_count = 35
3. XGBoost ‚Üí `1.2`
4. Planning Poker ‚Üí `"1"`

---

### **Case 2: Task Ph·ª©c T·∫°p**

```json
{
  "title": "Implement real-time chat with WebSocket",
  "description": "Build bidirectional communication system with Redis pub/sub, handle reconnection, message queue, typing indicators, and read receipts"
}
```

**Process:**
1. Embeddings ‚Üí high complexity vector
2. word_count = 23, char_count = 150
3. XGBoost ‚Üí `12.7`
4. Planning Poker ‚Üí `"13"`

---

### **Case 3: N·∫±m Gi·ªØa 2 Gi√° Tr·ªã**

```json
{
  "title": "Add user profile page",
  "description": "Create page to display and edit user information"
}
```

**Process:**
1. XGBoost ‚Üí `4.6`
2. `|4.6 - 5| = 0.4`, `|4.6 - 3| = 1.6`
3. Ch·ªâ g·∫ßn `5` ‚Üí `"5"`

---

## üöÄ Performance

- **Inference Time:** ~100-200ms (CPU)
- **Accuracy:** Ph·ª• thu·ªôc v√†o training data quality
- **Scalability:** C√≥ th·ªÉ batch predict nhi·ªÅu tasks

---

## üîç Debugging Tips

### **Ki·ªÉm Tra Models ƒê√£ Load**

```python
if not self.xgb_model or not self.embed_model or not hasattr(self, "scaler"):
    raise RuntimeError("Model, embeddings ho·∫∑c scaler ch∆∞a ƒë∆∞·ª£c load!")
```

### **Log Intermediate Values**

```python
print(f"Embeddings shape: {emb.shape}")
print(f"Features: word_count={word_count}, char_count={char_count}")
print(f"X_input shape: {X_input.shape}")
print(f"Raw prediction: {pred}")
```

---

## üìå L∆∞u √ù Quan Tr·ªçng

> [!IMPORTANT]
> - Models ph·∫£i ƒë∆∞·ª£c load **tr∆∞·ªõc** khi API ƒë∆∞·ª£c g·ªçi
> - Embeddings model ch·∫°y tr√™n **CPU** (c√≥ th·ªÉ ch·∫≠m l·∫ßn ƒë·∫ßu)
> - XGBoost model c·∫ßn **ƒë√∫ng 386 features** (384 embeddings + 2 extra)

> [!WARNING]
> - N·∫øu text qu√° ng·∫Øn (< 3 t·ª´), prediction c√≥ th·ªÉ kh√¥ng ch√≠nh x√°c
> - Model ch·ªâ t·ªët n·∫øu training data ƒë·ªß ƒëa d·∫°ng

> [!TIP]
> - ƒê·ªÉ c·∫£i thi·ªán accuracy, c·∫ßn retrain model v·ªõi data m·ªõi
> - C√≥ th·ªÉ th√™m features nh∆∞: task type, priority, s·ªë subtasks
