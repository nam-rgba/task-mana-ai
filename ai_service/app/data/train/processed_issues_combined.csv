Project_ID,Issue_Key,Type,Status,Priority,Title,Description,Description_Text,Description_Code,Story_Point
1,XD-3765,FEATURE,Done,MEDIUM,"""Fix stream failover ""","""See https://github.com/spring-projects/spring-xd/issues/1924""","""""""See https://github.com/spring-projects/spring-xd/issues/1924""""""",,8.0
1,XD-3748,BUG,Done,MEDIUM,"""Unable to register the JMX bean MessageHistory from Spring Integration""","""If I try to use <int:message-history/> when developing a Spring XD module, it fails when try to export the JMX bean. I've seen that the naming strategy used is org.springframework.xd.dirt.module.jmx.ModuleObjectNamingStrategy  The stackTrace: {code} 2016-02-24T10:40:39+0000 1.3.1.RELEASE ERROR DeploymentsPathChildrenCache-0 container.DeploymentListener - Exception deploying module org.springframework.jmx.export.UnableToRegisterMBeanException: Unable to register MBean [org.springframework.integration.history.MessageHistoryConfigurer@24902b5f] with key 'messageHistoryConfigurer'; nested exception is javax.management.MalformedObjectNameException: Key properties cannot be empty  at org.springframework.integration.monitor.IntegrationMBeanExporter.registerBeanInstance(IntegrationMBeanExporter.java:375) ~[spring-integration-jmx-4.2.5.RELEASE.jar:na]  at org.springframework.integration.monitor.IntegrationMBeanExporter.afterSingletonsInstantiated(IntegrationMBeanExporter.java:288) ~[spring-integration-jmx-4.2.5.RELEASE.jar:na]  at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:792) ~[spring-beans-4.2.4.RELEASE.jar:4.2.4.RELEASE]  at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:839) ~[spring-context-4.2.4.RELEASE.jar:4.2.4.RELEASE]  at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:538) ~[spring-context-4.2.4.RELEASE.jar:4.2.4.RELEASE]  at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:686) ~[spring-boot-1.2.3.RELEASE.jar:1.2.3.RELEASE]  at org.springframework.boot.SpringApplication.run(SpringApplication.java:320) ~[spring-boot-1.2.3.RELEASE.jar:1.2.3.RELEASE]  at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:139) ~[spring-boot-1.2.3.RELEASE.jar:1.2.3.RELEASE]  at org.springframework.xd.module.core.SimpleModule.initialize(SimpleModule.java:213) ~[spring-xd-module-1.3.1.RELEASE.jar:1.3.1.RELEASE]  at org.springframework.xd.dirt.module.ModuleDeployer.doDeploy(ModuleDeployer.java:217) ~[spring-xd-dirt-1.3.1.RELEASE.jar:1.3.1.RELEASE]  at org.springframework.xd.dirt.module.ModuleDeployer.deploy(ModuleDeployer.java:200) ~[spring-xd-dirt-1.3.1.RELEASE.jar:1.3.1.RELEASE]  at org.springframework.xd.dirt.server.container.DeploymentListener.deployModule(DeploymentListener.java:365) [spring-xd-dirt-1.3.1.RELEASE.jar:1.3.1.RELEASE]  at org.springframework.xd.dirt.server.container.DeploymentListener.deployStreamModule(DeploymentListener.java:334) [spring-xd-dirt-1.3.1.RELEASE.jar:1.3.1.RELEASE]  at org.springframework.xd.dirt.server.container.DeploymentListener.onChildAdded(DeploymentListener.java:181) [spring-xd-dirt-1.3.1.RELEASE.jar:1.3.1.RELEASE]  at org.springframework.xd.dirt.server.container.DeploymentListener.childEvent(DeploymentListener.java:149) [spring-xd-dirt-1.3.1.RELEASE.jar:1.3.1.RELEASE]  at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:509) [curator-recipes-2.6.0.jar:na]  at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:503) [curator-recipes-2.6.0.jar:na]  at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92) [curator-framework-2.6.0.jar:na]  at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297) [guava-16.0.1.jar:na]  at org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:83) [curator-framework-2.6.0.jar:na]  at org.apache.curator.framework.recipes.cache.PathChildrenCache.callListeners(PathChildrenCache.java:500) [curator-recipes-2.6.0.jar:na]  at org.apache.curator.framework.recipes.cache.EventOperation.invoke(EventOperation.java:35) [curator-recipes-2.6.0.jar:na]  at org.apache.curator.framework.recipes.cache.PathChildrenCache$10.run(PathChildrenCache.java:762) [curator-recipes-2.6.0.jar:na]  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_72]  at java.util.concurrent.FutureTask.run(FutureTask.java:266) [na:1.8.0_72]  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_72]  at java.util.concurrent.FutureTask.run(FutureTask.java:266) [na:1.8.0_72]  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_72]  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_72]  at java.lang.Thread.run(Thread.java:745) [na:1.8.0_72] Caused by: javax.management.MalformedObjectNameException: Key properties cannot be empty  at javax.management.ObjectName.construct(ObjectName.java:483) ~[na:1.8.0_72]  at javax.management.ObjectName.<init>(ObjectName.java:1382) ~[na:1.8.0_72]  at javax.management.ObjectName.getInstance(ObjectName.java:1273) ~[na:1.8.0_72]  at org.springframework.jmx.support.ObjectNameManager.getInstance(ObjectNameManager.java:62) ~[spring-context-4.2.4.RELEASE.jar:4.2.4.RELEASE]  at org.springframework.xd.dirt.module.jmx.ModuleObjectNamingStrategy.getObjectName(ModuleObjectNamingStrategy.java:50) ~[spring-xd-dirt-1.3.1.RELEASE.jar:1.3.1.RELEASE]  at org.springframework.jmx.export.MBeanExporter.getObjectName(MBeanExporter.java:751) ~[spring-context-4.2.4.RELEASE.jar:4.2.4.RELEASE] {code}""","""""""If I try to use <int:message-history/> when developing a Spring XD module, it fails when try to export the JMX bean. I've seen that the naming strategy used is org.springframework.xd.dirt.module.jmx.ModuleObjectNamingStrategy  The stackTrace: """"""",""" 2016-02-24T10:40:39+0000 1.3.1.RELEASE ERROR DeploymentsPathChildrenCache-0 container.DeploymentListener - Exception deploying module org.springframework.jmx.export.UnableToRegisterMBeanException: Unable to register MBean [org.springframework.integration.history.MessageHistoryConfigurer@24902b5f] with key 'messageHistoryConfigurer'; nested exception is javax.management.MalformedObjectNameException: Key properties cannot be empty  at org.springframework.integration.monitor.IntegrationMBeanExporter.registerBeanInstance(IntegrationMBeanExporter.java:375) ~[spring-integration-jmx-4.2.5.RELEASE.jar:na]  at org.springframework.integration.monitor.IntegrationMBeanExporter.afterSingletonsInstantiated(IntegrationMBeanExporter.java:288) ~[spring-integration-jmx-4.2.5.RELEASE.jar:na]  at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:792) ~[spring-beans-4.2.4.RELEASE.jar:4.2.4.RELEASE]  at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:839) ~[spring-context-4.2.4.RELEASE.jar:4.2.4.RELEASE]  at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:538) ~[spring-context-4.2.4.RELEASE.jar:4.2.4.RELEASE]  at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:686) ~[spring-boot-1.2.3.RELEASE.jar:1.2.3.RELEASE]  at org.springframework.boot.SpringApplication.run(SpringApplication.java:320) ~[spring-boot-1.2.3.RELEASE.jar:1.2.3.RELEASE]  at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:139) ~[spring-boot-1.2.3.RELEASE.jar:1.2.3.RELEASE]  at org.springframework.xd.module.core.SimpleModule.initialize(SimpleModule.java:213) ~[spring-xd-module-1.3.1.RELEASE.jar:1.3.1.RELEASE]  at org.springframework.xd.dirt.module.ModuleDeployer.doDeploy(ModuleDeployer.java:217) ~[spring-xd-dirt-1.3.1.RELEASE.jar:1.3.1.RELEASE]  at org.springframework.xd.dirt.module.ModuleDeployer.deploy(ModuleDeployer.java:200) ~[spring-xd-dirt-1.3.1.RELEASE.jar:1.3.1.RELEASE]  at org.springframework.xd.dirt.server.container.DeploymentListener.deployModule(DeploymentListener.java:365) [spring-xd-dirt-1.3.1.RELEASE.jar:1.3.1.RELEASE]  at org.springframework.xd.dirt.server.container.DeploymentListener.deployStreamModule(DeploymentListener.java:334) [spring-xd-dirt-1.3.1.RELEASE.jar:1.3.1.RELEASE]  at org.springframework.xd.dirt.server.container.DeploymentListener.onChildAdded(DeploymentListener.java:181) [spring-xd-dirt-1.3.1.RELEASE.jar:1.3.1.RELEASE]  at org.springframework.xd.dirt.server.container.DeploymentListener.childEvent(DeploymentListener.java:149) [spring-xd-dirt-1.3.1.RELEASE.jar:1.3.1.RELEASE]  at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:509) [curator-recipes-2.6.0.jar:na]  at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:503) [curator-recipes-2.6.0.jar:na]  at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92) [curator-framework-2.6.0.jar:na]  at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297) [guava-16.0.1.jar:na]  at org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:83) [curator-framework-2.6.0.jar:na]  at org.apache.curator.framework.recipes.cache.PathChildrenCache.callListeners(PathChildrenCache.java:500) [curator-recipes-2.6.0.jar:na]  at org.apache.curator.framework.recipes.cache.EventOperation.invoke(EventOperation.java:35) [curator-recipes-2.6.0.jar:na]  at org.apache.curator.framework.recipes.cache.PathChildrenCache$10.run(PathChildrenCache.java:762) [curator-recipes-2.6.0.jar:na]  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_72]  at java.util.concurrent.FutureTask.run(FutureTask.java:266) [na:1.8.0_72]  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_72]  at java.util.concurrent.FutureTask.run(FutureTask.java:266) [na:1.8.0_72]  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_72]  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_72]  at java.lang.Thread.run(Thread.java:745) [na:1.8.0_72] Caused by: javax.management.MalformedObjectNameException: Key properties cannot be empty  at javax.management.ObjectName.construct(ObjectName.java:483) ~[na:1.8.0_72]  at javax.management.ObjectName.<init>(ObjectName.java:1382) ~[na:1.8.0_72]  at javax.management.ObjectName.getInstance(ObjectName.java:1273) ~[na:1.8.0_72]  at org.springframework.jmx.support.ObjectNameManager.getInstance(ObjectNameManager.java:62) ~[spring-context-4.2.4.RELEASE.jar:4.2.4.RELEASE]  at org.springframework.xd.dirt.module.jmx.ModuleObjectNamingStrategy.getObjectName(ModuleObjectNamingStrategy.java:50) ~[spring-xd-dirt-1.3.1.RELEASE.jar:1.3.1.RELEASE]  at org.springframework.jmx.export.MBeanExporter.getObjectName(MBeanExporter.java:751) ~[spring-context-4.2.4.RELEASE.jar:4.2.4.RELEASE] """,1.0
1,XD-3747,IMPROVEMENT,Done,MEDIUM,"""Rabbit Bus: Expose ChannelCacheSize on CachingConnectionFactory""","""http://stackoverflow.com/questions/35563064/processing-messages-through-namedchannels-with-prefetch-1/35584333#35584333""","""""""http://stackoverflow.com/questions/35563064/processing-messages-through-namedchannels-with-prefetch-1/35584333#35584333""""""",,1.0
1,XD-3746,IMPROVEMENT,Done,MEDIUM,"""Update Spring Framework to 4.2.4""","""Update Spring Framework to 4.2.4""","""Update Spring Framework to 4.2.4""",,1.0
1,XD-3745,IMPROVEMENT,Done,MEDIUM,"""Update Spring-AMQP to 3.6, RabbitMQ Client to 1.5.4""","""Update to amqp-client 3.6.0 and spring-amqp 1.5.4""","""""""Update to amqp-client 3.6.0 and spring-amqp 1.5.4""""""",,1.0
1,XD-3744,IMPROVEMENT,Done,MEDIUM,"""Suppress DeliveryMode Header in RabbitMQ Source""","""Related to XD-2567 which fixed this problem, but only in the bus.  {quote} 2016-02-19T18:25:24-0500 1.2.1.RELEASE WARN SimpleAsyncTaskExecutor-1 support.DefaultAmqpHeaderMapper - skipping header 'amqp_deliveryMode' since it is not of expected type [class org.springframework.amqp.core.MessageDeliveryMode], it is [class org.springframework.amqp.core.MessageDeliveryMode] {quote}""","""""""Related to XD-2567 which fixed this problem, but only in the bus.  {quote} 2016-02-19T18:25:24-0500 1.2.1.RELEASE WARN SimpleAsyncTaskExecutor-1 support.DefaultAmqpHeaderMapper - skipping header 'amqp_deliveryMode' since it is not of expected type [class org.springframework.amqp.core.MessageDeliveryMode], it is [class org.springframework.amqp.core.MessageDeliveryMode] {quote}""""""",,1.0
1,XD-3743,BUG,Done,MEDIUM,"""Update to Spring Integration 4.2.5 When Available (Fix Metrics)""","""See INT-3956""","""""""See INT-3956""""""",,1.0
1,XD-3742,IMPROVEMENT,Done,MEDIUM,"""Enable in line SSL properties as an alternative to external properties files""","""The following XD components have been identified to support SSL via an `sslProperties` property which points to the location of a properties file. The properties encryption extension for 1.3.1 does not currently apply to these:    * Rabbit Message Bus   * Rabbit Source   * Rabbit Sink   * Http Source (NettyHttpInboundChannelAdapter).   This can be made to work in the case of Rabbit since the latest RabbitConnectionFactoryBean supports individual SSL properties settings as an alternative to the properties file. The http source may be extended to use the same approach.""","""""""The following XD components have been identified to support SSL via an `sslProperties` property which points to the location of a properties file. The properties encryption extension for 1.3.1 does not currently apply to these:    * Rabbit Message Bus   * Rabbit Source   * Rabbit Sink   * Http Source (NettyHttpInboundChannelAdapter).   This can be made to work in the case of Rabbit since the latest RabbitConnectionFactoryBean supports individual SSL properties settings as an alternative to the properties file. The http source may be extended to use the same approach.""""""",,3.0
1,XD-3741,BUG,Done,HIGH,"""[Flo] Stream creation/definitions doesn't show any component""",""" As a Flo for Spring XD user, I would like to be able to create a new stream using the graphicat UI.   This flow should be shown in a graphical way also in definition tab. !http://example.com/image.png! Right now it doesn't happen due to a javascript error.  {code} TypeError: this.node.getTransformToElement is not a function     at Object.VElement.bbox (http://localhost:9393/admin-ui/lib/joint/src/vectorizer.js:323:36)     at joint.dia.ElementView.joint.dia.CellView.extend.positionRelative (http://localhost:9393/admin-ui/lib/joint/dist/joint.all.clean.js:2740:51)     at null.<anonymous> (http://localhost:9393/admin-ui/lib/joint/dist/joint.all.clean.js:2710:18)     at http://localhost:9393/admin-ui/lib/lodash/lodash.compat.js:1177:23     at eval (eval at createIterator (http://localhost:9393/admin-ui/lib/lodash/lodash.compat.js:1:0), <anonymous>:10:9)     at Function.forEach (http://localhost:9393/admin-ui/lib/lodash/lodash.compat.js:3645:9)     at joint.dia.ElementView.joint.dia.CellView.extend.update (http://localhost:9393/admin-ui/lib/joint/dist/joint.all.clean.js:2700:11)     at bound [as update] (http://localhost:9393/admin-ui/lib/lodash/lodash.compat.js:1005:21)     at joint.dia.ElementView.joint.dia.CellView.extend.render (http://localhost:9393/admin-ui/lib/joint/dist/joint.all.clean.js:2903:14)     at joint.dia.Paper.Backbone.View.extend.addCell (http://localhost:9393/admin-ui/lib/joint/dist/joint.all.clean.js:5004:14)(anonymous function) @ :9393/admin-ui/lib/angular/angular.js:11500 :9393!attachment-name.jpg|thumbnail! {code} """,""""""" As a Flo for Spring XD user, I would like to be able to create a new stream using the graphicat UI.   This flow should be shown in a graphical way also in definition tab. !http://example.com/image.png! Right now it doesn't happen due to a javascript error.   """"""",""" TypeError: this.node.getTransformToElement is not a function     at Object.VElement.bbox (http://localhost:9393/admin-ui/lib/joint/src/vectorizer.js:323:36)     at joint.dia.ElementView.joint.dia.CellView.extend.positionRelative (http://localhost:9393/admin-ui/lib/joint/dist/joint.all.clean.js:2740:51)     at null.<anonymous> (http://localhost:9393/admin-ui/lib/joint/dist/joint.all.clean.js:2710:18)     at http://localhost:9393/admin-ui/lib/lodash/lodash.compat.js:1177:23     at eval (eval at createIterator (http://localhost:9393/admin-ui/lib/lodash/lodash.compat.js:1:0), <anonymous>:10:9)     at Function.forEach (http://localhost:9393/admin-ui/lib/lodash/lodash.compat.js:3645:9)     at joint.dia.ElementView.joint.dia.CellView.extend.update (http://localhost:9393/admin-ui/lib/joint/dist/joint.all.clean.js:2700:11)     at bound [as update] (http://localhost:9393/admin-ui/lib/lodash/lodash.compat.js:1005:21)     at joint.dia.ElementView.joint.dia.CellView.extend.render (http://localhost:9393/admin-ui/lib/joint/dist/joint.all.clean.js:2903:14)     at joint.dia.Paper.Backbone.View.extend.addCell (http://localhost:9393/admin-ui/lib/joint/dist/joint.all.clean.js:5004:14)(anonymous function) @ :9393/admin-ui/lib/angular/angular.js:11500 :9393!attachment-name.jpg|thumbnail! """,1.0
1,XD-3740,BUG,Done,MEDIUM,"""Kafka message bus maxWait property is not set up""","""The maxWait property from server.yml in the message bus section for kafka is not propagated through the code, it is ignored.""","""""""The maxWait property from server.yml in the message bus section for kafka is not propagated through the code, it is ignored.""""""",,1.0
1,XD-3739,BUG,Done,HIGH,"""Incorrect refresh period for groovy scripts""","""All modules that allow groovy implementations (filter, script, transform, router, tcpclient) allow automatic refresh of the script when it changes. In the XD documentation it is stated that this refresh occurs every minute eg for filter at http://docs.spring.io/spring-xd/docs/1.3.0.RELEASE/reference/html/#filter """"The script is checked for updates every 60 seconds, so it may be replaced in a running system. """"   This set up can be seen in the spring xml for the modules - eg (again for filter)  {code:xml} <filter input-channel=""""to.script"""" output-channel=""""output"""">  <int-groovy:script location=""""${script:filter.groovy}"""" script-variable-generator=""""variableGenerator"""" refresh-check-delay=""""60""""/> </filter> {code}  However from the spring integration documentation http://docs.spring.io/spring-integration/docs/4.2.4.RELEASE/reference/html/messaging-endpoints-chapter.html#scripting-config it specifies that the refresh-check-delay parameter is actually in milliseconds - ie the above XD configuration would recheck the script every 60 milliseconds which may be a performance concern as it will be checking the lastmodified time of the script file.   Ideally this parameter would be configurable - in our case we would usually eliminate the refresh check altogether (set to -1) as our scripts will not change (or if they did a redeploy of the module would pick it up)  ""","""""""All modules that allow groovy implementations (filter, script, transform, router, tcpclient) allow automatic refresh of the script when it changes. In the XD documentation it is stated that this refresh occurs every minute eg for filter at http://docs.spring.io/spring-xd/docs/1.3.0.RELEASE/reference/html/#filter """"""""The script is checked for updates every 60 seconds, so it may be replaced in a running system. """"""""   This set up can be seen in the spring xml for the modules - eg (again for filter)    However from the spring integration documentation http://docs.spring.io/spring-integration/docs/4.2.4.RELEASE/reference/html/messaging-endpoints-chapter.html#scripting-config it specifies that the refresh-check-delay parameter is actually in milliseconds - ie the above XD configuration would recheck the script every 60 milliseconds which may be a performance concern as it will be checking the lastmodified time of the script file.   Ideally this parameter would be configurable - in our case we would usually eliminate the refresh check altogether (set to -1) as our scripts will not change (or if they did a redeploy of the module would pick it up)  """"""",""" <filter input-channel=""""""""to.script"""""""" output-channel=""""""""output"""""""">  <int-groovy:script location=""""""""${script:filter.groovy}"""""""" script-variable-generator=""""""""variableGenerator"""""""" refresh-check-delay=""""""""60""""""""/> </filter> """,5.0
1,XD-3738,IMPROVEMENT,Done,MEDIUM,"""Encrypt secret information in XD configuration files""","""Spring XD keeps passwords in text files such sas servers.yml, properties files, and module configuration files. Some users have requested a way to store encrypted values rather than clear text.  XD should provide a """"hook"""" for users to provide a custom component to detect encrypted property values and decrypt them during container, admin, and module initialization.""","""""""Spring XD keeps passwords in text files such sas servers.yml, properties files, and module configuration files. Some users have requested a way to store encrypted values rather than clear text.  XD should provide a """"""""hook"""""""" for users to provide a custom component to detect encrypted property values and decrypt them during container, admin, and module initialization.""""""",,2.0
1,XD-3737,BUG,Done,MEDIUM,"""REST - Do not redirect after logout""","""In the following PR we removed the *RestLogoutSuccessHandler*.   https://github.com/spring-projects/spring-xd/pull/1562  This is necessary, though, for REST calls and the Admin UI. Otherwise some weird UI behavior might occur due to the HTTP redirect.""","""""""In the following PR we removed the *RestLogoutSuccessHandler*.   https://github.com/spring-projects/spring-xd/pull/1562  This is necessary, though, for REST calls and the Admin UI. Otherwise some weird UI behavior might occur due to the HTTP redirect.""""""",,1.0
1,XD-3736,IMPROVEMENT,Done,MEDIUM,"""Rabbit Pub/Sub Consumers Should Support Concurrency""","""PubSub consumers can support concurrency since the threads are competing consumers on the queue.  ""","""""""PubSub consumers can support concurrency since the threads are competing consumers on the queue.  """"""",,2.0
1,XD-3734,BUG,Done,MEDIUM,"""AutoBindDLQ Incompatible with Partitioned Streams (Producer Side).""","""See http://stackoverflow.com/questions/34817906/spring-xd-rabbitmq-partitioned-stream-deployment-in-failure""","""""""See http://stackoverflow.com/questions/34817906/spring-xd-rabbitmq-partitioned-stream-deployment-in-failure""""""",,1.0
1,XD-3733,IMPROVEMENT,Done,MEDIUM,"""Document redis pool properties in servers.yml""","""Add spring.redis.pool.*  properties to server.yml, commented out to show default values., e.g.,      maxIdle: 8,    minIdle: 0,     maxActive: 8,    maxWait: -1 ""","""""""Add spring.redis.pool.*  properties to server.yml, commented out to show default values., e.g.,      maxIdle: 8,    minIdle: 0,     maxActive: 8,    maxWait: -1 """"""",,1.0
1,XD-3732,BUG,Done,MEDIUM,"""Overrides to servers.yml file aren't taken into account""","""As a developer, I'm adding new overrides to {{server.yml}} file; however, the overridden properties do not reflect even after the restart of server. ""","""""""As a developer, I'm adding new overrides to {{server.yml}} file; however, the overridden properties do not reflect even after the restart of server. """"""",,2.0
1,XD-3731,IMPROVEMENT,Done,MEDIUM,"""Clean Up Compiler/Javadoc Warnings""","""{noformat} /Users/<USER>Development/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/batch/tasklet/JobLaunchingTasklet.java:96: warning: [rawtypes] found raw type: DomainRepository    DomainRepository instanceRepository, String jobName,    ^   missing type arguments for generic class DomainRepository<T,ID>   where T,ID are type-variables:     T extends Object declared in interface DomainRepository     ID extends Serializable,Comparable<ID> declared in interface DomainRepository /Users/<USER>Development/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/batch/tasklet/JobLaunchingTasklet.java:116: warning: [rawtypes] found raw type: DomainRepository    DomainRepository instanceRepository, String jobName,    ^   missing type arguments for generic class DomainRepository<T,ID>   where T,ID are type-variables:     T extends Object declared in interface DomainRepository     ID extends Serializable,Comparable<ID> declared in interface DomainRepository /Users/<USER>Development/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/batch/tasklet/JobLaunchingTasklet.java:127: warning: [unchecked] unchecked conversion   this.instanceRepository = instanceRepository;                             ^   required: DomainRepository<JobDefinition,String>   found:    DomainRepository /Users/<USER>Development/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/module/ModuleException.java:23: warning: [serial] serializable class ModuleException has no definition of serialVersionUID public class ModuleException extends RuntimeException {        ^ /Users/<USER>Development/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/module/support/ModuleDefinitionService.java:130: warning: [try] explicit call to close() on an auto-closeable resource    target.close();          ^ /Users/<USER>Development/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/stream/StreamException.java:23: warning: [serial] serializable class StreamException has no definition of serialVersionUID public class StreamException extends RuntimeException {        ^ /Users/<USER>.gradle/caches/modules-2/files-2.1/org.kitesdk/kite-data-core/1.0.0/512563fec38547cf446fbd221069799df1ab158d/kite-data-core-1.0.0.jar(org/kitesdk/data/DatasetDescriptor.class): warning: Cannot find annotation method 'value()' in type 'SuppressWarnings': class file for edu.umd.cs.findbugs.annotations.SuppressWarnings not found /Users/<USER>.gradle/caches/modules-2/files-2.1/org.kitesdk/kite-data-core/1.0.0/512563fec38547cf446fbd221069799df1ab158d/kite-data-core-1.0.0.jar(org/kitesdk/data/DatasetDescriptor.class): warning: Cannot find annotation method 'justification()' in type 'SuppressWarnings' 3 warnings /Users/<USER>Development/spring-xd/spring-xd-tuple/src/test/java/org/springframework/xd/tuple/TupleJsonMarshallerTests.java:77: warning: [unchecked] unchecked cast   List<Tuple> body = (List<Tuple>) tuple.getValue(""""body"""");                                                  ^   required: List<Tuple>   found:    Object :api /Users/<USER>Development/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/server/admin/deployment/zk/ContainerListener.java:145: warning - Tag @link: reference not found: DepartedContainerDeployer /Users/<USER>Development/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/server/admin/deployment/zk/ContainerListener.java:145: warning - Tag @link: reference not found: DepartedContainerDeployer /Users/<USER>Development/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/job/dao/XdJdbcSearchableJobExecutionDao.java:151: warning - @return tag has no arguments. /Users/<USER>Development/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/job/dao/XdJdbcSearchableJobExecutionDao.java:166: warning - @return tag has no arguments. /Users/<USER>Development/spring-xd/spring-xd-rest-domain/src/main/java/org/springframework/xd/rest/domain/JobExecutionInfoResource.java:252: warning - @return tag cannot be used in method with void return type. /Users/<USER>Development/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/server/admin/deployment/zk/ContainerListener.java:145: warning - Tag @link: reference not found: DepartedContainerDeployer {noformat}""","""""""""""""",""" /Users/grussell/Development/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/batch/tasklet/JobLaunchingTasklet.java:96: warning: [rawtypes] found raw type: DomainRepository    DomainRepository instanceRepository, String jobName,    ^   missing type arguments for generic class DomainRepository<T,ID>   where T,ID are type-variables:     T extends Object declared in interface DomainRepository     ID extends Serializable,Comparable<ID> declared in interface DomainRepository /Users/grussell/Development/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/batch/tasklet/JobLaunchingTasklet.java:116: warning: [rawtypes] found raw type: DomainRepository    DomainRepository instanceRepository, String jobName,    ^   missing type arguments for generic class DomainRepository<T,ID>   where T,ID are type-variables:     T extends Object declared in interface DomainRepository     ID extends Serializable,Comparable<ID> declared in interface DomainRepository /Users/grussell/Development/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/batch/tasklet/JobLaunchingTasklet.java:127: warning: [unchecked] unchecked conversion   this.instanceRepository = instanceRepository;                             ^   required: DomainRepository<JobDefinition,String>   found:    DomainRepository /Users/grussell/Development/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/module/ModuleException.java:23: warning: [serial] serializable class ModuleException has no definition of serialVersionUID public class ModuleException extends RuntimeException {        ^ /Users/grussell/Development/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/module/support/ModuleDefinitionService.java:130: warning: [try] explicit call to close() on an auto-closeable resource    target.close();          ^ /Users/grussell/Development/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/stream/StreamException.java:23: warning: [serial] serializable class StreamException has no definition of serialVersionUID public class StreamException extends RuntimeException {        ^ /Users/grussell/.gradle/caches/modules-2/files-2.1/org.kitesdk/kite-data-core/1.0.0/512563fec38547cf446fbd221069799df1ab158d/kite-data-core-1.0.0.jar(org/kitesdk/data/DatasetDescriptor.class): warning: Cannot find annotation method 'value()' in type 'SuppressWarnings': class file for edu.umd.cs.findbugs.annotations.SuppressWarnings not found /Users/grussell/.gradle/caches/modules-2/files-2.1/org.kitesdk/kite-data-core/1.0.0/512563fec38547cf446fbd221069799df1ab158d/kite-data-core-1.0.0.jar(org/kitesdk/data/DatasetDescriptor.class): warning: Cannot find annotation method 'justification()' in type 'SuppressWarnings' 3 warnings /Users/grussell/Development/spring-xd/spring-xd-tuple/src/test/java/org/springframework/xd/tuple/TupleJsonMarshallerTests.java:77: warning: [unchecked] unchecked cast   List<Tuple> body = (List<Tuple>) tuple.getValue(""""""""body"""""""");                                                  ^   required: List<Tuple>   found:    Object :api /Users/grussell/Development/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/server/admin/deployment/zk/ContainerListener.java:145: warning - Tag @link: reference not found: DepartedContainerDeployer /Users/grussell/Development/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/server/admin/deployment/zk/ContainerListener.java:145: warning - Tag @link: reference not found: DepartedContainerDeployer /Users/grussell/Development/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/job/dao/XdJdbcSearchableJobExecutionDao.java:151: warning - @return tag has no arguments. /Users/grussell/Development/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/job/dao/XdJdbcSearchableJobExecutionDao.java:166: warning - @return tag has no arguments. /Users/grussell/Development/spring-xd/spring-xd-rest-domain/src/main/java/org/springframework/xd/rest/domain/JobExecutionInfoResource.java:252: warning - @return tag cannot be used in method with void return type. /Users/grussell/Development/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/server/admin/deployment/zk/ContainerListener.java:145: warning - Tag @link: reference not found: DepartedContainerDeployer """,1.0
1,XD-3730,BUG,Done,URGENT,"""NPE in spring-integration when using kafka as message bus when using aggrzgation module""","""as stated in https://jira.spring.io/browse/INT-3908 sprint-integration in springxd can't use kafka as message bus in most case. Could it spring-xd integrat this fix for us to use it?""","""""""as stated in https://jira.spring.io/browse/INT-3908 sprint-integration in springxd can't use kafka as message bus in most case. Could it spring-xd integrat this fix for us to use it?""""""",,3.0
1,XD-3725,BUG,Done,URGENT,"""EmbeddedHeadersMessageConverter Buffer Overflow""","""See https://github.com/spring-projects/spring-xd/issues/1871""","""""""See https://github.com/spring-projects/spring-xd/issues/1871""""""",,1.0
1,XD-3721,BUG,Done,HIGH,"""XD Admin UI log out does not function properly""","""I am using XD 1.2.1.RELEASE. I have following environment variables   XD_CONFIG_NAME = mycompany And  SPRING_PROFILE_ACTIVE= prod, admin  i have XD configuration file (mycompany-prod.yml) with following security configuration  # Config to enable security on administration endpoints (consider adding ssl) spring:   profiles: prod security:   basic:     enabled: true # false to disable security settings (default)     realm: SpringXD xd:   security:     authentication:       file:         enabled: true          users:           xdadmin: pwd, ROLE_ADMIN,ROLE_VIEW,ROLE_CREATE  I get a login screen, login works alright. When i logout - i still see all the tabs and contents in all the tabs. See the attached screenshot. ""","""""""I am using XD 1.2.1.RELEASE. I have following environment variables   XD_CONFIG_NAME = mycompany And  SPRING_PROFILE_ACTIVE= prod, admin  i have XD configuration file (mycompany-prod.yml) with following security configuration  # Config to enable security on administration endpoints (consider adding ssl) spring:   profiles: prod security:   basic:     enabled: true # false to disable security settings (default)     realm: SpringXD xd:   security:     authentication:       file:         enabled: true          users:           xdadmin: pwd, ROLE_ADMIN,ROLE_VIEW,ROLE_CREATE  I get a login screen, login works alright. When i logout - i still see all the tabs and contents in all the tabs. See the attached screenshot. """"""",,1.0
1,XD-3719,BUG,Done,HIGH,"""Spring flo issue with unexpected char""","""In Flo when creating a stream if you use asterisk you get an error. See the image attached.""","""""""In Flo when creating a stream if you use asterisk you get an error. See the image attached.""""""",,2.0
1,XD-3718,IMPROVEMENT,Done,MEDIUM,"""Kafka message bus must accept partitioning properties for named queues""","""As a user, I want to be able to provide the partitioning logic for a named destination, so that I can control the ordering of outbound messages.""","""""""As a user, I want to be able to provide the partitioning logic for a named destination, so that I can control the ordering of outbound messages.""""""",,1.0
1,XD-3716,IMPROVEMENT,Done,MEDIUM,"""Support Configuring the RabbitMessageBus MessagePropertiesConverter LongString Limit""","""http://stackoverflow.com/questions/34053997/passing-headerinformation-as-jsonobject-in-header-in-spring-xd""","""""""http://stackoverflow.com/questions/34053997/passing-headerinformation-as-jsonobject-in-header-in-spring-xd""""""",,2.0
1,XD-3715,FEATURE,Done,MEDIUM,"""Move k8s SPI to a separate repo""","""As a developer, I'd like to move k8s SPI to it's own repo.""","""""""As a developer, I'd like to move k8s SPI to it's own repo.""""""",,5.0
1,XD-3714,FEATURE,Done,MEDIUM,"""Upgrade XD Ambari release to 1.3 ""","""As a developer, I'd like to upgrade Spring XD's ambari plugin to 1.3 release.""","""""""As a developer, I'd like to upgrade Spring XD's ambari plugin to 1.3 release.""""""",,3.0
1,XD-3709,BUG,Done,MEDIUM,"""Duplicate MBean Names With router Sink""","""For some reason, the Integration {{MBeanExporterHelper}} is not preventing the standard context {{MBeanExporter}} from exporting the {{AbstractMessageRouter}}. This should be suppressed (when an IMBE is present) because it's annotated {{@IntegrationManagedResource}}.  Causes {{InstanceAlreadyExistsException}}.  Workaround in the stack overflow answer.  http://stackoverflow.com/questions/33838502/error-deploying-more-than-one-stream-with-a-router-1-3-0  Could be an SI issue, but investigation needed. However, we should probably include the stream/job name in all MBeans for the stream (as is done for the integration exporter).""","""""""For some reason, the Integration {{MBeanExporterHelper}} is not preventing the standard context {{MBeanExporter}} from exporting the {{AbstractMessageRouter}}. This should be suppressed (when an IMBE is present) because it's annotated {{@IntegrationManagedResource}}.  Causes {{InstanceAlreadyExistsException}}.  Workaround in the stack overflow answer.  http://stackoverflow.com/questions/33838502/error-deploying-more-than-one-stream-with-a-router-1-3-0  Could be an SI issue, but investigation needed. However, we should probably include the stream/job name in all MBeans for the stream (as is done for the integration exporter).""""""",,1.0
1,XD-3708,FEATURE,Done,MEDIUM,"""Document limitations with HSQL when using composed jobs""","""As a developer, I'd want to document the limitations of HSQL DB when using composed jobs. ""","""""""As a developer, I'd want to document the limitations of HSQL DB when using composed jobs. """"""",,1.0
1,XD-3706,BUG,Done,MEDIUM,"""Counter sink does not accept SpEL expressions""","""As a user, I'm trying to use {{counter}} sink with {SpEL}} expression, but I'm not able to use them in combination. It [throws|https://github.com/spring-cloud/spring-cloud-stream-modules/blob/master/counter-sink/src/main/java/org/springframework/cloud/stream/module/metrics/CounterSinkProperties.java#L77] {{exactly one of 'name' and 'nameExpression' must be set}} as error message.  ""","""""""As a user, I'm trying to use {{counter}} sink with {SpEL}} expression, but I'm not able to use them in combination. It [throws|https://github.com/spring-cloud/spring-cloud-stream-modules/blob/master/counter-sink/src/main/java/org/springframework/cloud/stream/module/metrics/CounterSinkProperties.java#L77] {{exactly one of 'name' and 'nameExpression' must be set}} as error message.  """"""",,1.0
1,XD-3705,FEATURE,Done,MEDIUM,"""Bump Boot and spring-cloud-build Versions""","""As a developer, I'd like to upgrade Boot and Spring Cloud Build revisions, so I can leverage the latest updates.""","""""""As a developer, I'd like to upgrade Boot and Spring Cloud Build revisions, so I can leverage the latest updates.""""""",,5.0
1,XD-3702,FEATURE,Done,MEDIUM,"""Support partitioning for Kafka even if count == 1""","""As a developer, I want to be able to set a partitioning key for the Kafka bus even when there is a single downstream module, so that I can take advantage of the native Kafka partitioning and message ordering support.""","""""""As a developer, I want to be able to set a partitioning key for the Kafka bus even when there is a single downstream module, so that I can take advantage of the native Kafka partitioning and message ordering support.""""""",,3.0
1,XD-3701,FEATURE,Done,MEDIUM,"""Improve Shell Connection Diagnostics""","""When a problem occurs connecting to admin, we just get {{Unable to contact Data Flow Admin}} even if the connection is successful and some problem occurs when interpreting the result.  The exception is eaten.  Log an error including the exception.  Currently investigating an NPE in DataFlowTemplate @ line 77.""","""""""When a problem occurs connecting to admin, we just get {{Unable to contact Data Flow Admin}} even if the connection is successful and some problem occurs when interpreting the result.  The exception is eaten.  Log an error including the exception.  Currently investigating an NPE in DataFlowTemplate @ line 77.""""""",,1.0
1,XD-3699,IMPROVEMENT,Done,MEDIUM,"""Remove hardcoded buildpack commit reference""","""As a developer, I'd like to remove the hardcoded buildpack reference since the latest 1.6.2 ER release includes all the features required by Data Flow. ""","""""""As a developer, I'd like to remove the hardcoded buildpack reference since the latest 1.6.2 ER release includes all the features required by Data Flow. """"""",,1.0
1,XD-3698,BUG,Done,MEDIUM,"""Execution list page includes child jobs in pagination scope""","""As a user, I created a composed job with over 10 child jobs in the workflow; I expected to see 'a' job in the execution list page without any pagination, but instead I noticed empty pagination to skip to next page.""","""""""As a user, I created a composed job with over 10 child jobs in the workflow; I expected to see 'a' job in the execution list page without any pagination, but instead I noticed empty pagination to skip to next page.""""""",,1.0
1,XD-3697,BUG,Done,MEDIUM,"""Output modules cannot use minPartitionCount when sending to named channels""","""If the output module is connected to a named channel, cannot be set up the property minPartitionCount, it is giving an exception.  Streams: stream create f --definition """"queue:foo > transform --expression=payload+'-foo' | log""""  stream create b --definition """"queue:bar > transform --expression=payload+'-bar' | log"""" stream deploy --name f --properties """"module.transform.count=2"""" stream deploy --name b --properties """"module.transform.count=2""""  stream create r --definition """"time | router --expression=payload.contains('10')?'queue:foo':'queue:bar'"""" stream deploy --name r --properties """"module.router.producer.minPartitionCount=20""""  The error is: Caused by: java.lang.IllegalArgumentException: KafkaMessageBus does not support producer property: minPartitionCount for queue:bar. at org.springframework.xd.dirt.integration.bus.MessageBusSupport.validateProperties(MessageBusSupport.java:781) ~[spring-xd-messagebus-spi-1.2.1.RELEASE.jar:1.2.1.RELEASE]""","""""""If the output module is connected to a named channel, cannot be set up the property minPartitionCount, it is giving an exception.  Streams: stream create f --definition """"""""queue:foo > transform --expression=payload+'-foo' | log""""""""  stream create b --definition """"""""queue:bar > transform --expression=payload+'-bar' | log"""""""" stream deploy --name f --properties """"""""module.transform.count=2"""""""" stream deploy --name b --properties """"""""module.transform.count=2""""""""  stream create r --definition """"""""time | router --expression=payload.contains('10')?'queue:foo':'queue:bar'"""""""" stream deploy --name r --properties """"""""module.router.producer.minPartitionCount=20""""""""  The error is: Caused by: java.lang.IllegalArgumentException: KafkaMessageBus does not support producer property: minPartitionCount for queue:bar. at org.springframework.xd.dirt.integration.bus.MessageBusSupport.validateProperties(MessageBusSupport.java:781) ~[spring-xd-messagebus-spi-1.2.1.RELEASE.jar:1.2.1.RELEASE]""""""",,1.0
1,XD-3696,FEATURE,Done,MEDIUM,"""Upgrade to SI 4.2.2.GA""","""As a developer, I'd like to upgrade to SI 4.2.2.GA release, so I can leverage the latest improvements.""","""""""As a developer, I'd like to upgrade to SI 4.2.2.GA release, so I can leverage the latest improvements.""""""",,1.0
1,XD-3695,FEATURE,Done,MEDIUM,"""Upgrade to SHDP 2.2.1.GA""","""As a developer, I'd like to upgrade to 2.2.1 GA release, so I can leverage the latest improvements without breaking backwards compatibility. SHDP 2.3.0 uses Boot 1.3 and HDP and CDH versions that drop older Hive support. To avoid breaking changes we should instead use SHDP 2.2.1 that has backported any improvements that we need as well as move Spring and Hadoop versions to more recent ones.""","""""""As a developer, I'd like to upgrade to 2.2.1 GA release, so I can leverage the latest improvements without breaking backwards compatibility. SHDP 2.3.0 uses Boot 1.3 and HDP and CDH versions that drop older Hive support. To avoid breaking changes we should instead use SHDP 2.2.1 that has backported any improvements that we need as well as move Spring and Hadoop versions to more recent ones.""""""",,1.0
1,XD-3693,FEATURE,Done,MEDIUM,"""Add Timestamp to XD Message History""","""I don't recall why [this commit | https://github.com/garyrussell/spring-xd/commit/ba15a1390f7e448dbc723ee76a45c2e239e0994e] was not applied to master but having the timestamp for each step in the history will be useful.  See [this github issue | https://github.com/spring-projects/spring-xd-modules/issues/24#issuecomment-154436643]. ""","""""""I don't recall why [this commit | https://github.com/garyrussell/spring-xd/commit/ba15a1390f7e448dbc723ee76a45c2e239e0994e] was not applied to master but having the timestamp for each step in the history will be useful.  See [this github issue | https://github.com/spring-projects/spring-xd-modules/issues/24#issuecomment-154436643]. """"""",,1.0
1,XD-3692,FEATURE,Done,MEDIUM,"""Optimize YARN deployer""","""As a developer, I'd like to optimize YARN deployer, so I can deploy stream and the modules part of the definition rapidly.""","""""""As a developer, I'd like to optimize YARN deployer, so I can deploy stream and the modules part of the definition rapidly.""""""",,5.0
1,XD-3691,BUG,Done,MEDIUM,"""Ensure Job definitions are escaped in UI""","""If using the definition <aaa || bbb> where the definition starts with a """"<"""" and ends with a """">"""" the definition for the composed job does not appear on the definition page.""","""""""If using the definition <aaa || bbb> where the definition starts with a """"""""<"""""""" and ends with a """""""">"""""""" the definition for the composed job does not appear on the definition page.""""""",,2.0
1,XD-3690,FEATURE,Done,MEDIUM,"""Improve """"Server Configuration - Database Configuration"""" section""","""Make it more clear what drivers need to be copied where. See - https://github.com/spring-projects/spring-xd/issues/1653""","""""""Make it more clear what drivers need to be copied where. See - https://github.com/spring-projects/spring-xd/issues/1653""""""",,1.0
1,XD-3689,FEATURE,Done,MEDIUM,"""Update default configs to support Composed Jobs""","""Users want the ability to use Composed Jobs (specifically parallel Jobs) without having to update the configurations for the hsqldb and the Isolation Level for spring batch.  These should be set by default. ""","""""""Users want the ability to use Composed Jobs (specifically parallel Jobs) without having to update the configurations for the hsqldb and the Isolation Level for spring batch.  These should be set by default. """"""",,3.0
1,XD-3687,FEATURE,Done,MEDIUM,"""Update Docs to add configs changes for Composed jobs""","""Need to add the following instructions to setup the configurations for the Batch Repo to Composed Job Docs to support parallel jobs: 1) uncomment and change the following from  : ```spring:   batch: # Configure other Spring Batch repository values.  Most are typically not needed     isolationLevel: ISOLATION_SERIALIZATION ``` to ```spring:   batch: # Configure other Spring Batch repository values.  Most are typically not needed     isolationLevel: ISOLATION_READ_COMMITTED ```   And update the hsqldb datasource to: spring:   datasource:     url: jdbc:hsqldb:hsql://${hsql.server.host:localhost}:${hsql.server.port:9101}/${hsql.server.dbname:xdjob};sql.enforce_strict_size=true;hsqldb.tx=mvcc""","""""""Need to add the following instructions to setup the configurations for the Batch Repo to Composed Job Docs to support parallel jobs: 1) uncomment and change the following from  : ```spring:   batch: # Configure other Spring Batch repository values.  Most are typically not needed     isolationLevel: ISOLATION_SERIALIZATION ``` to ```spring:   batch: # Configure other Spring Batch repository values.  Most are typically not needed     isolationLevel: ISOLATION_READ_COMMITTED ```   And update the hsqldb datasource to: spring:   datasource:     url: jdbc:hsqldb:hsql://${hsql.server.host:localhost}:${hsql.server.port:9101}/${hsql.server.dbname:xdjob};sql.enforce_strict_size=true;hsqldb.tx=mvcc""""""",,1.0
1,XD-3686,BUG,Done,MEDIUM,"""log4j/log4j-over-slf4j logging issue""","""I got below error when executing modules on yarn and it was written in appmaster stderr output. {code} Exception in thread """"Thread-2"""" java.lang.NoClassDefFoundError: org/apache/log4j/spi/ThrowableInformation         at org.apache.log4j.spi.LoggingEvent.<init>(LoggingEvent.java:165)         at org.apache.log4j.Category.forcedLog(Category.java:391)         at org.apache.log4j.Category.log(Category.java:856)         at org.slf4j.impl.Log4jLoggerAdapter.log(Log4jLoggerAdapter.java:595)         at org.apache.commons.logging.impl.SLF4JLocationAwareLog.warn(SLF4JLocationAwareLog.java:192)         at org.springframework.context.support.AbstractApplicationContext.doClose(AbstractApplicationContext.java:969)         at org.springframework.boot.context.embedded.EmbeddedWebApplicationContext.doClose(EmbeddedWebApplicationContext.java:150)         at org.springframework.context.support.AbstractApplicationContext$1.run(AbstractApplicationContext.java:893) {code}  `LoggingEvent` is found from both `log4j-over-slf4j-1.7.12.jar` and `log4j-1.2.17.jar`. I suppose it depends on which one is used first to load this class.  Here's what we have in admin and appmaster jar files(spring-cloud-dataflow-yarn-build-tests is my local new sub-project to run tests on a hadoop minicluster): {code} unzip -l target/spring-cloud-dataflow-yarn-build-tests/spring-cloud-dataflow-yarn-appmaster-1.0.0.BUILD-SNAPSHOT.jar|grep jar|grep -i log     62050  2013-05-16 22:04   lib/commons-logging-1.1.3.jar    489884  2012-05-06 13:24   lib/log4j-1.2.17.jar      8860  2015-03-26 21:56   lib/slf4j-log4j12-1.7.12.jar      2234  2015-09-03 16:30   lib/spring-boot-starter-logging-1.3.0.M5.jar     24567  2015-03-26 21:57   lib/log4j-over-slf4j-1.7.12.jar     40824  2015-08-18 12:39   lib/tomcat-embed-logging-juli-8.0.26.jar     66802  2015-05-28 09:49   lib/jboss-logging-3.3.0.Final.jar {code}  {code} unzip -l spring-cloud-dataflow-admin/target/spring-cloud-dataflow-admin-1.0.0.BUILD-SNAPSHOT.jar |grep jar|grep -i log     62050  2013-05-16 22:04   lib/commons-logging-1.1.3.jar    489884  2012-05-06 13:24   lib/log4j-1.2.17.jar     40824  2015-08-18 12:39   lib/tomcat-embed-logging-juli-8.0.26.jar     66802  2015-05-28 09:49   lib/jboss-logging-3.3.0.Final.jar      2234  2015-09-03 16:30   lib/spring-boot-starter-logging-1.3.0.M5.jar    280928  2015-03-24 12:06   lib/logback-classic-1.1.3.jar    455041  2015-03-24 12:05   lib/logback-core-1.1.3.jar     24567  2015-03-26 21:57   lib/log4j-over-slf4j-1.7.12.jar {code}  Error went away when I removed `log4j-over-slf4j-1.7.12.jar` from maven deps for yarn appmaster jar. I suppose we have same issue with admin server. ""","""""""I got below error when executing modules on yarn and it was written in appmaster stderr output.   `LoggingEvent` is found from both `log4j-over-slf4j-1.7.12.jar` and `log4j-1.2.17.jar`. I suppose it depends on which one is used first to load this class.  Here's what we have in admin and appmaster jar files(spring-cloud-dataflow-yarn-build-tests is my local new sub-project to run tests on a hadoop minicluster):     Error went away when I removed `log4j-over-slf4j-1.7.12.jar` from maven deps for yarn appmaster jar. I suppose we have same issue with admin server. """"""",""" Exception in thread """"""""Thread-2"""""""" java.lang.NoClassDefFoundError: org/apache/log4j/spi/ThrowableInformation         at org.apache.log4j.spi.LoggingEvent.<init>(LoggingEvent.java:165)         at org.apache.log4j.Category.forcedLog(Category.java:391)         at org.apache.log4j.Category.log(Category.java:856)         at org.slf4j.impl.Log4jLoggerAdapter.log(Log4jLoggerAdapter.java:595)         at org.apache.commons.logging.impl.SLF4JLocationAwareLog.warn(SLF4JLocationAwareLog.java:192)         at org.springframework.context.support.AbstractApplicationContext.doClose(AbstractApplicationContext.java:969)         at org.springframework.boot.context.embedded.EmbeddedWebApplicationContext.doClose(EmbeddedWebApplicationContext.java:150)         at org.springframework.context.support.AbstractApplicationContext$1.run(AbstractApplicationContext.java:893)  unzip -l target/spring-cloud-dataflow-yarn-build-tests/spring-cloud-dataflow-yarn-appmaster-1.0.0.BUILD-SNAPSHOT.jar|grep jar|grep -i log     62050  2013-05-16 22:04   lib/commons-logging-1.1.3.jar    489884  2012-05-06 13:24   lib/log4j-1.2.17.jar      8860  2015-03-26 21:56   lib/slf4j-log4j12-1.7.12.jar      2234  2015-09-03 16:30   lib/spring-boot-starter-logging-1.3.0.M5.jar     24567  2015-03-26 21:57   lib/log4j-over-slf4j-1.7.12.jar     40824  2015-08-18 12:39   lib/tomcat-embed-logging-juli-8.0.26.jar     66802  2015-05-28 09:49   lib/jboss-logging-3.3.0.Final.jar  unzip -l spring-cloud-dataflow-admin/target/spring-cloud-dataflow-admin-1.0.0.BUILD-SNAPSHOT.jar |grep jar|grep -i log     62050  2013-05-16 22:04   lib/commons-logging-1.1.3.jar    489884  2012-05-06 13:24   lib/log4j-1.2.17.jar     40824  2015-08-18 12:39   lib/tomcat-embed-logging-juli-8.0.26.jar     66802  2015-05-28 09:49   lib/jboss-logging-3.3.0.Final.jar      2234  2015-09-03 16:30   lib/spring-boot-starter-logging-1.3.0.M5.jar    280928  2015-03-24 12:06   lib/logback-classic-1.1.3.jar    455041  2015-03-24 12:05   lib/logback-core-1.1.3.jar     24567  2015-03-26 21:57   lib/log4j-over-slf4j-1.7.12.jar """,1.0
1,XD-3685,BUG,Done,MEDIUM,"""Job Definitions page fails to display definitions if page ""","""In this scenario we created 30 jobs that can be used for a composed job.   if the composed job uses jobs in its composition that are not present on the first page of the of the result set the following exception is thrown.    {noformat} 2015-11-02T14:47:17-0500 1.3.0.SNAP ERROR qtp1587928736-26 rest.RestControllerAdvice - Caught exception while handling a request java.lang.IllegalStateException: Not all instances were looked at: fff  at org.springframework.xd.dirt.rest.XDController.enhanceWithDeployments(XDController.java:244) ~[spring-xd-dirt-1.3.0.BUILD-SNAPSHOT.jar:1.3.0.BUILD-SNAPSHOT]  at org.springframework.xd.dirt.rest.XDController.listValues(XDController.java:209) ~[spring-xd-dirt-1.3.0.BUILD-SNAPSHOT.jar:1.3.0.BUILD-SNAPSHOT]  at org.springframework.xd.dirt.rest.JobsController.list(JobsController.java:128) ~[spring-xd-dirt-1.3.0.BUILD-SNAPSHOT.jar:1.3.0.BUILD-SNAPSHOT]  at sun.reflect.GeneratedMethodAccessor133.invoke(Unknown Source) ~[na:na]  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.7.0_67]  at java.lang.reflect.Method.invoke(Method.java:606) ~[na:1.7.0_67]  at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:221) ~[spring-web-4.2.2.RELEASE.jar:4.2.2.RELEASE]  at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:137) ~[spring-web-4.2.2.RELEASE.jar:4.2.2.RELEASE]  at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:110) ~[spring-webmvc-4.2.2.RELEASE.jar:4.2.2.RELEASE]  at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:806) ~[spring-webmvc-4.2.2.RELEASE.jar:4.2.2.RELEASE]  at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:729) ~[spring-webmvc-4.2.2.RELEASE.jar:4.2.2.RELEASE]  at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:85) ~[spring-webmvc-4.2.2.RELEASE.jar:4.2.2.RELEASE]  at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:959) ~[spring-webmvc-4.2.2.RELEASE.jar:4.2.2.RELEASE]  at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:893) ~[spring-webmvc-4.2.2.RELEASE.jar:4.2.2.RELEASE]  at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:970) [spring-webmvc-4.2.2.RELEASE.jar:4.2.2.RELEASE]  at org.springframework.web.servlet.FrameworkServlet.doGet(FrameworkServlet.java:861) [spring-webmvc-4.2.2.RELEASE.jar:4.2.2.RELEASE]  at javax.servlet.http.HttpServlet.service(HttpServlet.java:735) [javax.servlet-3.0.0.v201112011016.jar:na]  at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:846) [spring-webmvc-4.2.2.RELEASE.jar:4.2.2.RELEASE]  at javax.servlet.http.HttpServlet.service(HttpServlet.java:848) [javax.servlet-3.0.0.v201112011016.jar:na]  at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:684) [jetty-servlet-8.1.14.v20131031.jar:8.1.14.v20131031]  at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1496) [jetty-servlet-8.1.14.v20131031.jar:8.1.14.v20131031]  at org.springframework.boot.actuate.autoconfigure.EndpointWebMvcAutoConfiguration$ApplicationContextHeaderFilter.doFilterInternal(EndpointWebMvcAutoConfiguration.java:291) [spring-boot-actuator-1.2.3.RELEASE.jar:1.2.3.RELEASE]  at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.2.2.RELEASE.jar:4.2.2.RELEASE]  at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467) [jetty-servlet-8.1.14.v20131031.jar:8.1.14.v20131031]  at org.springframework.web.filter.HiddenHttpMethodFilter.doFilterInternal(HiddenHttpMethodFilter.java:77) [spring-web-4.2.2.RELEASE.jar:4.2.2.RELEASE]  at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.2.2.RELEASE.jar:4.2.2.RELEASE]  at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467) [jetty-servlet-8.1.14.v20131031.jar:8.1.14.v20131031]  at org.springframework.web.filter.HttpPutFormContentFilter.doFilterInternal(HttpPutFormContentFilter.java:87) [spring-web-4.2.2.RELEASE.jar:4.2.2.RELEASE]  at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.2.2.RELEASE.jar:4.2.2.RELEASE]  at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467) [jetty-servlet-8.1.14.v20131031.jar:8.1.14.v20131031]  at org.springframework.boot.actuate.trace.WebRequestTraceFilter.doFilterInternal(WebRequestTraceFilter.java:102) [spring-boot-actuator-1.2.3.RELEASE.jar:1.2.3.RELEASE]  at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.2.2.RELEASE.jar:4.2.2.RELEASE]  at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467) [jetty-servlet-8.1.14.v20131031.jar:8.1.14.v20131031]  at org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:207) [spring-security-web-4.0.2.RELEASE.jar:4.0.2.RELEASE]  at org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:176) [spring-security-web-4.0.2.RELEASE.jar:4.0.2.RELEASE]  at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467) [jetty-servlet-8.1.14.v20131031.jar:8.1.14.v20131031]  at org.springframework.boot.actuate.autoconfigure.MetricFilterAutoConfiguration$MetricsFilter.doFilterInternal(MetricFilterAutoConfiguration.java:90) [spring-boot-actuator-1.2.3.RELEASE.jar:1.2.3.RELEASE]  at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.2.2.RELEASE.jar:4.2.2.RELEASE]  at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467) [jetty-servlet-8.1.14.v20131031.jar:8.1.14.v20131031]  at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:499) [jetty-servlet-8.1.14.v20131031.jar:8.1.14.v20131031]  at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:137) [jetty-server-8.1.14.v20131031.jar:8.1.14.v20131031]  at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:557) [jetty-security-8.1.14.v20131031.jar:8.1.14.v20131031]  at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:231) [jetty-server-8.1.14.v20131031.jar:8.1.14.v20131031]  at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1086) [jetty-server-8.1.14.v20131031.jar:8.1.14.v20131031]  at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:428) [jetty-servlet-8.1.14.v20131031.jar:8.1.14.v20131031]  at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:193) [jetty-server-8.1.14.v20131031.jar:8.1.14.v20131031]  at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1020) [jetty-server-8.1.14.v20131031.jar:8.1.14.v20131031]  at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135) [jetty-server-8.1.14.v20131031.jar:8.1.14.v20131031]  at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116) [jetty-server-8.1.14.v20131031.jar:8.1.14.v20131031]  at org.eclipse.jetty.server.Server.handle(Server.java:370) [jetty-server-8.1.14.v20131031.jar:8.1.14.v20131031]  at org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:494) [jetty-server-8.1.14.v20131031.jar:8.1.14.v20131031]  at org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:971) [jetty-server-8.1.14.v20131031.jar:8.1.14.v20131031]  at org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:1033) [jetty-server-8.1.14.v20131031.jar:8.1.14.v20131031]  at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:644) [jetty-http-8.1.14.v20131031.jar:8.1.14.v20131031]  at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235) [jetty-http-8.1.14.v20131031.jar:8.1.14.v20131031]  at org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82) [jetty-server-8.1.14.v20131031.jar:8.1.14.v20131031]  at org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:667) [jetty-io-8.1.14.v20131031.jar:8.1.14.v20131031]  at org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:52) [jetty-io-8.1.14.v20131031.jar:8.1.14.v20131031]  at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608) [jetty-util-8.1.14.v20131031.jar:8.1.14.v20131031]  at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543) [jetty-util-8.1.14.v20131031.jar:8.1.14.v20131031]  at java.lang.Thread.run(Thread.java:745) [na:1.7.0_67] {noformat}""","""""""In this scenario we created 30 jobs that can be used for a composed job.   if the composed job uses jobs in its composition that are not present on the first page of the of the result set the following exception is thrown.    """"""",""" 2015-11-02T14:47:17-0500 1.3.0.SNAP ERROR qtp1587928736-26 rest.RestControllerAdvice - Caught exception while handling a request java.lang.IllegalStateException: Not all instances were looked at: fff  at org.springframework.xd.dirt.rest.XDController.enhanceWithDeployments(XDController.java:244) ~[spring-xd-dirt-1.3.0.BUILD-SNAPSHOT.jar:1.3.0.BUILD-SNAPSHOT]  at org.springframework.xd.dirt.rest.XDController.listValues(XDController.java:209) ~[spring-xd-dirt-1.3.0.BUILD-SNAPSHOT.jar:1.3.0.BUILD-SNAPSHOT]  at org.springframework.xd.dirt.rest.JobsController.list(JobsController.java:128) ~[spring-xd-dirt-1.3.0.BUILD-SNAPSHOT.jar:1.3.0.BUILD-SNAPSHOT]  at sun.reflect.GeneratedMethodAccessor133.invoke(Unknown Source) ~[na:na]  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.7.0_67]  at java.lang.reflect.Method.invoke(Method.java:606) ~[na:1.7.0_67]  at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:221) ~[spring-web-4.2.2.RELEASE.jar:4.2.2.RELEASE]  at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:137) ~[spring-web-4.2.2.RELEASE.jar:4.2.2.RELEASE]  at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:110) ~[spring-webmvc-4.2.2.RELEASE.jar:4.2.2.RELEASE]  at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:806) ~[spring-webmvc-4.2.2.RELEASE.jar:4.2.2.RELEASE]  at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:729) ~[spring-webmvc-4.2.2.RELEASE.jar:4.2.2.RELEASE]  at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:85) ~[spring-webmvc-4.2.2.RELEASE.jar:4.2.2.RELEASE]  at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:959) ~[spring-webmvc-4.2.2.RELEASE.jar:4.2.2.RELEASE]  at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:893) ~[spring-webmvc-4.2.2.RELEASE.jar:4.2.2.RELEASE]  at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:970) [spring-webmvc-4.2.2.RELEASE.jar:4.2.2.RELEASE]  at org.springframework.web.servlet.FrameworkServlet.doGet(FrameworkServlet.java:861) [spring-webmvc-4.2.2.RELEASE.jar:4.2.2.RELEASE]  at javax.servlet.http.HttpServlet.service(HttpServlet.java:735) [javax.servlet-3.0.0.v201112011016.jar:na]  at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:846) [spring-webmvc-4.2.2.RELEASE.jar:4.2.2.RELEASE]  at javax.servlet.http.HttpServlet.service(HttpServlet.java:848) [javax.servlet-3.0.0.v201112011016.jar:na]  at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:684) [jetty-servlet-8.1.14.v20131031.jar:8.1.14.v20131031]  at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1496) [jetty-servlet-8.1.14.v20131031.jar:8.1.14.v20131031]  at org.springframework.boot.actuate.autoconfigure.EndpointWebMvcAutoConfiguration$ApplicationContextHeaderFilter.doFilterInternal(EndpointWebMvcAutoConfiguration.java:291) [spring-boot-actuator-1.2.3.RELEASE.jar:1.2.3.RELEASE]  at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.2.2.RELEASE.jar:4.2.2.RELEASE]  at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467) [jetty-servlet-8.1.14.v20131031.jar:8.1.14.v20131031]  at org.springframework.web.filter.HiddenHttpMethodFilter.doFilterInternal(HiddenHttpMethodFilter.java:77) [spring-web-4.2.2.RELEASE.jar:4.2.2.RELEASE]  at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.2.2.RELEASE.jar:4.2.2.RELEASE]  at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467) [jetty-servlet-8.1.14.v20131031.jar:8.1.14.v20131031]  at org.springframework.web.filter.HttpPutFormContentFilter.doFilterInternal(HttpPutFormContentFilter.java:87) [spring-web-4.2.2.RELEASE.jar:4.2.2.RELEASE]  at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.2.2.RELEASE.jar:4.2.2.RELEASE]  at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467) [jetty-servlet-8.1.14.v20131031.jar:8.1.14.v20131031]  at org.springframework.boot.actuate.trace.WebRequestTraceFilter.doFilterInternal(WebRequestTraceFilter.java:102) [spring-boot-actuator-1.2.3.RELEASE.jar:1.2.3.RELEASE]  at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.2.2.RELEASE.jar:4.2.2.RELEASE]  at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467) [jetty-servlet-8.1.14.v20131031.jar:8.1.14.v20131031]  at org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:207) [spring-security-web-4.0.2.RELEASE.jar:4.0.2.RELEASE]  at org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:176) [spring-security-web-4.0.2.RELEASE.jar:4.0.2.RELEASE]  at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467) [jetty-servlet-8.1.14.v20131031.jar:8.1.14.v20131031]  at org.springframework.boot.actuate.autoconfigure.MetricFilterAutoConfiguration$MetricsFilter.doFilterInternal(MetricFilterAutoConfiguration.java:90) [spring-boot-actuator-1.2.3.RELEASE.jar:1.2.3.RELEASE]  at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.2.2.RELEASE.jar:4.2.2.RELEASE]  at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467) [jetty-servlet-8.1.14.v20131031.jar:8.1.14.v20131031]  at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:499) [jetty-servlet-8.1.14.v20131031.jar:8.1.14.v20131031]  at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:137) [jetty-server-8.1.14.v20131031.jar:8.1.14.v20131031]  at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:557) [jetty-security-8.1.14.v20131031.jar:8.1.14.v20131031]  at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:231) [jetty-server-8.1.14.v20131031.jar:8.1.14.v20131031]  at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1086) [jetty-server-8.1.14.v20131031.jar:8.1.14.v20131031]  at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:428) [jetty-servlet-8.1.14.v20131031.jar:8.1.14.v20131031]  at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:193) [jetty-server-8.1.14.v20131031.jar:8.1.14.v20131031]  at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1020) [jetty-server-8.1.14.v20131031.jar:8.1.14.v20131031]  at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135) [jetty-server-8.1.14.v20131031.jar:8.1.14.v20131031]  at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116) [jetty-server-8.1.14.v20131031.jar:8.1.14.v20131031]  at org.eclipse.jetty.server.Server.handle(Server.java:370) [jetty-server-8.1.14.v20131031.jar:8.1.14.v20131031]  at org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:494) [jetty-server-8.1.14.v20131031.jar:8.1.14.v20131031]  at org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:971) [jetty-server-8.1.14.v20131031.jar:8.1.14.v20131031]  at org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:1033) [jetty-server-8.1.14.v20131031.jar:8.1.14.v20131031]  at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:644) [jetty-http-8.1.14.v20131031.jar:8.1.14.v20131031]  at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235) [jetty-http-8.1.14.v20131031.jar:8.1.14.v20131031]  at org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82) [jetty-server-8.1.14.v20131031.jar:8.1.14.v20131031]  at org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:667) [jetty-io-8.1.14.v20131031.jar:8.1.14.v20131031]  at org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:52) [jetty-io-8.1.14.v20131031.jar:8.1.14.v20131031]  at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608) [jetty-util-8.1.14.v20131031.jar:8.1.14.v20131031]  at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543) [jetty-util-8.1.14.v20131031.jar:8.1.14.v20131031]  at java.lang.Thread.run(Thread.java:745) [na:1.7.0_67] """,3.0
1,XD-3684,BUG,Done,MEDIUM,"""Job composition fails for large transitions""","""As a user, I'm trying to create a composed job with >20 steps/transitions using Rabbit as the message bus and it doesn't complete successfully.""","""""""As a user, I'm trying to create a composed job with >20 steps/transitions using Rabbit as the message bus and it doesn't complete successfully.""""""",,3.0
1,XD-3683,BUG,Done,MEDIUM,"""Fix composed job error message""","""As a user, I'm trying to compose a job just with one definition; however, I'm getting the following error message, which could be misinterpreted.  {code} xd:>job create salsa --definition timestampfile Successfully created job 'salsa' xd:>job create foo --definition """"salsa || salsa"""" Successfully created job 'foo' xd:>job create foo222 --definition """"salsa"""" Command failed org.springframework.xd.rest.client.impl.SpringXDException: Could not find module with name 'salsa' and type 'job' {code}""","""""""As a user, I'm trying to compose a job just with one definition; however, I'm getting the following error message, which could be misinterpreted.  """"""",""" xd:>job create salsa --definition timestampfile Successfully created job 'salsa' xd:>job create foo --definition """"""""salsa || salsa"""""""" Successfully created job 'foo' xd:>job create foo222 --definition """"""""salsa"""""""" Command failed org.springframework.xd.rest.client.impl.SpringXDException: Could not find module with name 'salsa' and type 'job' """,1.0
1,XD-3682,FEATURE,Done,MEDIUM,"""Add 'undeployed' status for Mesos SPI""","""As a developer, I'd like to add {{undeployed}} status for Mesos SPI, so I can represent the correct status instead of the current {{unknown}} state.""","""""""As a developer, I'd like to add {{undeployed}} status for Mesos SPI, so I can represent the correct status instead of the current {{unknown}} state.""""""",,3.0
1,XD-3681,FEATURE,Done,MEDIUM,"""Add 'undeployed' status for k8s SPI""","""As a developer, I'd like to add {{undeployed}} status for k8s SPI, so I can represent the correct status instead of the current {{unknown}} state.""","""""""As a developer, I'd like to add {{undeployed}} status for k8s SPI, so I can represent the correct status instead of the current {{unknown}} state.""""""",,1.0
1,XD-3680,FEATURE,Done,MEDIUM,"""Add consistent support for """"undeployed"""" state across the deployers""","""As a developer, I'd like to add support for {{undeployed}} status consistently across all the deployers, so I can present the correct status instead of the current {{unknown}}. This is applicable for existing streams without any deployment context associated with it. ""","""""""As a developer, I'd like to add support for {{undeployed}} status consistently across all the deployers, so I can present the correct status instead of the current {{unknown}}. This is applicable for existing streams without any deployment context associated with it. """"""",,1.0
1,XD-3679,FEATURE,Done,MEDIUM,"""Add 'undeployed' status for Lattice SPI""","""As a developer, I'd like to add {{undeployed}} status for Lattice SPI, so I can represent the correct status instead of the current {{unknown}} state.""","""""""As a developer, I'd like to add {{undeployed}} status for Lattice SPI, so I can represent the correct status instead of the current {{unknown}} state.""""""",,3.0
1,XD-3678,FEATURE,Done,MEDIUM,"""Add 'undeployed' status for CF SPI""","""As a developer, I'd like to add {{undeployed}} status for CF SPI, so I can represent the correct status instead of the current {{unknown}} state.""","""""""As a developer, I'd like to add {{undeployed}} status for CF SPI, so I can represent the correct status instead of the current {{unknown}} state.""""""",,3.0
1,XD-3675,FEATURE,Done,MEDIUM,"""Create admin artifact and CI build for Lattice""","""As a developer, I'd like to create separate repo for Lattice SPI, so I don't have to bundle all SPI variants under one admin project.""","""""""As a developer, I'd like to create separate repo for Lattice SPI, so I don't have to bundle all SPI variants under one admin project.""""""",,3.0
1,XD-3673,BUG,Done,MEDIUM,"""Multiple module instances produces duplicate messages ""","""As a follow-up from [XD-3613|https://jira.spring.io/browse/XD-3629], we would want to fix this experience for Kafka message bus.""","""""""As a follow-up from [XD-3613|https://jira.spring.io/browse/XD-3629], we would want to fix this experience for Kafka message bus.""""""",,5.0
1,XD-3672,FEATURE,Done,MEDIUM,"""Move Mesos SPI to a separate repo""","""As a developer, I'd like to submit a PR for existing work on Mesos SPI. ""","""""""As a developer, I'd like to submit a PR for existing work on Mesos SPI. """"""",,2.0
1,XD-3671,FEATURE,Done,MEDIUM,"""Spike: Explore options to scale modules from shell""","""As a user, I'd like to have direct shell commands to scale up/down a given module instance, so I can avoid SPI specific CLI commands that needs run outside of data flow.""","""""""As a user, I'd like to have direct shell commands to scale up/down a given module instance, so I can avoid SPI specific CLI commands that needs run outside of data flow.""""""",,5.0
1,XD-3670,FEATURE,Done,MEDIUM,"""Spike: Revisit the core design and document gaps""","""As a developer, I'd like to revisit the existing design and identify known limitations and/or the gaps. ""","""""""As a developer, I'd like to revisit the existing design and identify known limitations and/or the gaps. """"""",,5.0
1,XD-3669,FEATURE,Done,MEDIUM,"""Add Flo screenshots to Batch DSL section""","""As a user, I'd like Flo Graphs as screenshots while referring to the batch DSL, so it will be easy for me to relate to concepts. ""","""""""As a user, I'd like Flo Graphs as screenshots while referring to the batch DSL, so it will be easy for me to relate to concepts. """"""",,1.0
1,XD-3668,FEATURE,Done,MEDIUM,"""UI: Add SPI type and version to about section""","""As a user, I'd like to see the version and SPI type in the `about` section, so I can confirm which build of {{admin-ui}} I'm currently using. ""","""""""As a user, I'd like to see the version and SPI type in the `about` section, so I can confirm which build of {{admin-ui}} I'm currently using. """"""",,1.0
1,XD-3667,BUG,Done,MEDIUM,"""CF SPI REST calls are not working ""","""As a developer, I'd like to troubleshoot and fix {{root}} level access over CF SPI REST calls; they're broke at the moment.   Access for following calls fail:  {code}  href: """"https://s-c-dataflow-admin.cfapps.pez.pivotal.io:80/streams"""" href: """"https://s-c-dataflow-admin.cfapps.pez.pivotal.io:80/tasks"""" href: """"https://s-c-dataflow-admin.cfapps.pez.pivotal.io:80/metrics/counters"""" href: """"https://s-c-dataflow-admin.cfapps.pez.pivotal.io:80/metrics/counters/{name}"""", href: """"https://s-c-dataflow-admin.cfapps.pez.pivotal.io:80/modules"""" href: """"https://s-c-dataflow-admin.cfapps.pez.pivotal.io:80/completions/stream{?start,detailLevel}"""", {code}""","""""""As a developer, I'd like to troubleshoot and fix {{root}} level access over CF SPI REST calls; they're broke at the moment.   Access for following calls fail:  """"""","""  href: """"""""https://s-c-dataflow-admin.cfapps.pez.pivotal.io:80/streams"""""""" href: """"""""https://s-c-dataflow-admin.cfapps.pez.pivotal.io:80/tasks"""""""" href: """"""""https://s-c-dataflow-admin.cfapps.pez.pivotal.io:80/metrics/counters"""""""" href: """"""""https://s-c-dataflow-admin.cfapps.pez.pivotal.io:80/metrics/counters/{name}"""""""", href: """"""""https://s-c-dataflow-admin.cfapps.pez.pivotal.io:80/modules"""""""" href: """"""""https://s-c-dataflow-admin.cfapps.pez.pivotal.io:80/completions/stream{?start,detailLevel}"""""""", """,1.0
1,XD-3666,FEATURE,Done,MEDIUM,"""UI: [Spike] Study PUI theming scope""","""As a user, I'd like to use the admin-ui and flo with consistent look and feel. ""","""""""As a user, I'd like to use the admin-ui and flo with consistent look and feel. """"""",,1.0
1,XD-3665,BUG,Done,MEDIUM,"""UI: Task deployment page is not loading""","""As a user, I'm trying to load Task, Task Deployment, and Task Executions page, but I'm seeing an error {{(Error fetching data. Is the XD server running?)}} instead. ""","""""""As a user, I'm trying to load Task, Task Deployment, and Task Executions page, but I'm seeing an error {{(Error fetching data. Is the XD server running?)}} instead. """"""",,1.0
1,XD-3664,FEATURE,Done,MEDIUM,"""UI: Replace Job references with Task""","""As a developer, I'd like to replace all {{Job(s)}} references with {{Task(s)}}. ""","""""""As a developer, I'd like to replace all {{Job(s)}} references with {{Task(s)}}. """"""",,2.0
1,XD-3663,BUG,Done,MEDIUM,"""UI: Job modules page wouldn't load""","""As a user, I'm trying to load Job - Modules page in admin-ui, but I'm seeing exceptions in console and the page wouldn't load.   {code} Failed to convert value of type 'java.lang.String' to required type 'org.springframework.cloud.dataflow.core.ArtifactType'; nested exception is org.springframework.core.convert.ConversionFailedException: Failed to convert from type java.lang.String to type @org.springframework.web.bind.annotation.RequestParam org.springframework.cloud.dataflow.core.ArtifactType for value 'job'; nested exception is java.lang.IllegalArgumentException: No enum constant org.springframework.cloud.dataflow.core.ArtifactType.job {code}""","""""""As a user, I'm trying to load Job - Modules page in admin-ui, but I'm seeing exceptions in console and the page wouldn't load.   """"""",""" Failed to convert value of type 'java.lang.String' to required type 'org.springframework.cloud.dataflow.core.ArtifactType'; nested exception is org.springframework.core.convert.ConversionFailedException: Failed to convert from type java.lang.String to type @org.springframework.web.bind.annotation.RequestParam org.springframework.cloud.dataflow.core.ArtifactType for value 'job'; nested exception is java.lang.IllegalArgumentException: No enum constant org.springframework.cloud.dataflow.core.ArtifactType.job """,2.0
1,XD-3662,FEATURE,Done,MEDIUM,"""UI: Replace XD with Data Flow""","""As a developer, I'd like to replace all references of Spring XD with Spring Cloud Data Flow. ""","""""""As a developer, I'd like to replace all references of Spring XD with Spring Cloud Data Flow. """"""",,2.0
1,XD-3659,FEATURE,Done,MEDIUM,"""Create admin artifact for each Hadoop distro""","""As a developer, I'd like to split {{admin}} artifact packaged with hadoop distro specific libraries, so I could avoid adding all variations of hadoop libraries under one project. ""","""""""As a developer, I'd like to split {{admin}} artifact packaged with hadoop distro specific libraries, so I could avoid adding all variations of hadoop libraries under one project. """"""",,5.0
1,XD-3656,FEATURE,Done,MEDIUM,"""Add 'undeployed' status for YARN SPI""","""As a developer, I'd like to add {{undeployed}} status for YARN SPI, so I can represent the correct status instead of the current {{unknown}} state.""","""""""As a developer, I'd like to add {{undeployed}} status for YARN SPI, so I can represent the correct status instead of the current {{unknown}} state.""""""",,3.0
1,XD-3655,MAINTENANCE,Done,MEDIUM,"""Document admin-ui improvements""","""Document admin-ui improvements""","""Document admin-ui improvements""",,1.0
1,XD-3654,FEATURE,Done,MEDIUM,"""Documentation: Flo for XD Batch""","""As a user, I'd like to refer to 'job orchestration' documentation, so I can use it as guideline for building batch workflows.  ""","""""""As a user, I'd like to refer to 'job orchestration' documentation, so I can use it as guideline for building batch workflows.  """"""",,3.0
1,XD-3653,BUG,Done,HIGH,"""Admin UI does not load on master build""","""As a user, I cannot use {{admin-ui}} on the master build. It won't come up. ""","""""""As a user, I cannot use {{admin-ui}} on the master build. It won't come up. """"""",,2.0
1,XD-3652,BUG,Done,HIGH,"""The shell processor module cannot be stopped while blocked in receive()""","""Both lifecycle and send/receive methods are synchronized, so if the shell command processor is blocked reading from the script's input - e.g. when no proper terminator is sent by the script, the stop() method can't acquire the object lock and proceed stopping the instance, and therefore the module. ""","""""""Both lifecycle and send/receive methods are synchronized, so if the shell command processor is blocked reading from the script's input - e.g. when no proper terminator is sent by the script, the stop() method can't acquire the object lock and proceed stopping the instance, and therefore the module. """"""",,5.0
1,XD-3651,IMPROVEMENT,Done,MEDIUM,"""The JsonStringToTupleConverter converts all values to String""","""As a module developer I would like the JsonStringToTupleConverter in the Spring Cloud Streams project to maintain the types provided in the JSON string and not convert everything to a String representation.""","""""""As a module developer I would like the JsonStringToTupleConverter in the Spring Cloud Streams project to maintain the types provided in the JSON string and not convert everything to a String representation.""""""",,1.0
1,XD-3649,FEATURE,Done,MEDIUM,"""Make SpEL usage consistent across all including custom modules""","""As a user, I'd like to use SpEL expressions inline at the stream definition level, so I can operate on the payload consistently while using any OOTB, including the custom modules. ""","""""""As a user, I'd like to use SpEL expressions inline at the stream definition level, so I can operate on the payload consistently while using any OOTB, including the custom modules. """"""",,8.0
1,XD-3648,BUG,Done,MEDIUM,"""Job Executions without Deployed Job (deleted) shall not be restartable""","""Job Executions without Deployed Job (deleted) shall not be restartable""","""Job Executions without Deployed Job (deleted) shall not be restartable""",,1.0
1,XD-3645,BUG,Done,MEDIUM,"""Tuple unable to serialize objects with nested arrays of objects""","""Serializing a tuple object with that have a nested array which contains objects (as a tuple) fails to serialize. The error is:  {noformat} Caused by: com.fasterxml.jackson.databind.JsonMappingException: No serializer found for class org.springframework.xd.tuple.DefaultTupleConversionService and no properties discovered to create BeanSerializer (to avoid exception, disable SerializationFeature.FAIL_ON_EMPTY_BEANS) ) (through reference chain: java.util.ArrayList[0]->org.springframework.xd.tuple.DefaultTuple[""""values""""]->java.util.UnmodifiableRandomAccessList[0]->org.springframework.xd.tuple.DefaultTuple[""""conversionService""""])  at com.fasterxml.jackson.databind.ser.impl.UnknownSerializer.failForEmpty(UnknownSerializer.java:59) ~[jackson-databind-2.4.5.jar:2.4.5]  at com.fasterxml.jackson.databind.ser.impl.UnknownSerializer.serialize(UnknownSerializer.java:26) ~[jackson-databind-2.4.5.jar:2.4.5]  at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:505) ~[jackson-databind-2.4.5.jar:2.4.5]  at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:639) ~[jackson-databind-2.4.5.jar:2.4.5]  at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:152) ~[jackson-databind-2.4.5.jar:2.4.5]  at com.fasterxml.jackson.databind.ser.impl.IndexedListSerializer.serializeContents(IndexedListSerializer.java:100) ~[jackson-databind-2.4.5.jar:2.4.5]  at com.fasterxml.jackson.databind.ser.impl.IndexedListSerializer.serializeContents(IndexedListSerializer.java:21) ~[jackson-databind-2.4.5.jar:2.4.5]  at com.fasterxml.jackson.databind.ser.std.AsArraySerializerBase.serialize(AsArraySerializerBase.java:183) ~[jackson-databind-2.4.5.jar:2.4.5]  at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:505) ~[jackson-databind-2.4.5.jar:2.4.5]  at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:639) ~[jackson-databind-2.4.5.jar:2.4.5]  at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:152) ~[jackson-databind-2.4.5.jar:2.4.5]  at com.fasterxml.jackson.databind.ser.impl.IndexedListSerializer.serializeContents(IndexedListSerializer.java:100) ~[jackson-databind-2.4.5.jar:2.4.5]  at com.fasterxml.jackson.databind.ser.impl.IndexedListSerializer.serializeContents(IndexedListSerializer.java:21) ~[jackson-databind-2.4.5.jar:2.4.5]  at com.fasterxml.jackson.databind.ser.std.AsArraySerializerBase.serialize(AsArraySerializerBase.java:183) ~[jackson-databind-2.4.5.jar:2.4.5]  at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider.serializeValue(DefaultSerializerProvider.java:128) ~[jackson-databind-2.4.5.jar:2.4.5]  at com.fasterxml.jackson.databind.ObjectMapper.writeValue(ObjectMapper.java:1902) ~[jackson-databind-2.4.5.jar:2.4.5]  at com.fasterxml.jackson.core.base.GeneratorBase.writeObject(GeneratorBase.java:280) ~[jackson-core-2.4.5.jar:2.4.5]  at com.fasterxml.jackson.databind.node.POJONode.serialize(POJONode.java:111) ~[jackson-databind-2.4.5.jar:2.4.5]  at com.fasterxml.jackson.databind.ser.std.SerializableSerializer.serialize(SerializableSerializer.java:44) ~[jackson-databind-2.4.5.jar:2.4.5]  at com.fasterxml.jackson.databind.ser.std.SerializableSerializer.serialize(SerializableSerializer.java:29) ~[jackson-databind-2.4.5.jar:2.4.5]  at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider.serializeValue(DefaultSerializerProvider.java:128) ~[jackson-databind-2.4.5.jar:2.4.5]  at com.fasterxml.jackson.databind.ObjectMapper.writeValue(ObjectMapper.java:1902) ~[jackson-databind-2.4.5.jar:2.4.5]  at com.fasterxml.jackson.core.base.GeneratorBase.writeObject(GeneratorBase.java:280) ~[jackson-core-2.4.5.jar:2.4.5]  at com.fasterxml.jackson.databind.node.POJONode.serialize(POJONode.java:111) ~[jackson-databind-2.4.5.jar:2.4.5]  at com.fasterxml.jackson.databind.node.ObjectNode.serialize(ObjectNode.java:264) ~[jackson-databind-2.4.5.jar:2.4.5]  at com.fasterxml.jackson.databind.ser.std.SerializableSerializer.serialize(SerializableSerializer.java:44) ~[jackson-databind-2.4.5.jar:2.4.5]  at com.fasterxml.jackson.databind.ser.std.SerializableSerializer.serialize(SerializableSerializer.java:29) ~[jackson-databind-2.4.5.jar:2.4.5]  at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider.serializeValue(DefaultSerializerProvider.java:128) ~[jackson-databind-2.4.5.jar:2.4.5]  at com.fasterxml.jackson.databind.ObjectMapper.writeValue(ObjectMapper.java:1902) ~[jackson-databind-2.4.5.jar:2.4.5]  at com.fasterxml.jackson.core.base.GeneratorBase.writeObject(GeneratorBase.java:280) ~[jackson-core-2.4.5.jar:2.4.5]  at com.fasterxml.jackson.databind.node.POJONode.serialize(POJONode.java:111) ~[jackson-databind-2.4.5.jar:2.4.5]  at com.fasterxml.jackson.databind.node.ObjectNode.serialize(ObjectNode.java:264) ~[jackson-databind-2.4.5.jar:2.4.5]  at com.fasterxml.jackson.databind.ser.std.SerializableSerializer.serialize(SerializableSerializer.java:44) ~[jackson-databind-2.4.5.jar:2.4.5]  at com.fasterxml.jackson.databind.ser.std.SerializableSerializer.serialize(SerializableSerializer.java:29) ~[jackson-databind-2.4.5.jar:2.4.5]  at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider.serializeValue(DefaultSerializerProvider.java:128) ~[jackson-databind-2.4.5.jar:2.4.5]  at com.fasterxml.jackson.databind.ObjectMapper.writeValue(ObjectMapper.java:1902) ~[jackson-databind-2.4.5.jar:2.4.5]  at com.fasterxml.jackson.core.base.GeneratorBase.writeObject(GeneratorBase.java:280) ~[jackson-core-2.4.5.jar:2.4.5]  at com.fasterxml.jackson.databind.node.POJONode.serialize(POJONode.java:111) ~[jackson-databind-2.4.5.jar:2.4.5]  at com.fasterxml.jackson.databind.node.ObjectNode.serialize(ObjectNode.java:264) ~[jackson-databind-2.4.5.jar:2.4.5]  at com.fasterxml.jackson.databind.ser.std.SerializableSerializer.serialize(SerializableSerializer.java:44) ~[jackson-databind-2.4.5.jar:2.4.5]  at com.fasterxml.jackson.databind.ser.std.SerializableSerializer.serialize(SerializableSerializer.java:29) ~[jackson-databind-2.4.5.jar:2.4.5]  at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider.serializeValue(DefaultSerializerProvider.java:128) ~[jackson-databind-2.4.5.jar:2.4.5]  at com.fasterxml.jackson.databind.ObjectMapper._configAndWriteValue(ObjectMapper.java:2881) ~[jackson-databind-2.4.5.jar:2.4.5]  at com.fasterxml.jackson.databind.ObjectMapper.writeValueAsString(ObjectMapper.java:2338) ~[jackson-databind-2.4.5.jar:2.4.5]  at org.springframework.xd.tuple.TupleToJsonStringConverter.convert(TupleToJsonStringConverter.java:37) ~[spring-xd-tuple-1.3.0.M1.jar:1.3.0.M1] {noformat} when the input string (read from a Kafka topic in my case) looks something like:  {noformat} {     """"body"""": [         {             """"dataType"""": """"har"""",             """"har"""": {                 """"log"""": {                     """"browser"""": {                         """"name"""": """"Google Chrome"""",                         """"version"""": """"44.0.2403.155""""                     },                     """"creator"""": {                         """"name"""": """"My extension"""",                         """"version"""": """"0.23.6""""                     },                     """"pages"""": [                         {                             """"_requestTimings"""": {                                 """"blocked"""": -1,                                 """"connect"""": -1,                                 """"dns"""": -1,                                 """"receive"""": 11,                                 """"send"""": -1,                                 """"ssl"""": -1,                                 """"wait"""": 244                             },                             """"_requestUrl"""": """"https://google.com""""                         },                         {                             """"_requestTimings"""": {                                 """"blocked"""": -1,                                 """"connect"""": -1,                                 """"dns"""": -1,                                 """"receive"""": 11,                                 """"send"""": -1,                                 """"ssl"""": -1,                                 """"wait"""": 244                             },                             """"_requestUrl"""": """"https://google.com""""                         }                     ],                     """"version"""": """"1.2""""                 }             },             """"testId"""": 1         }     ],     """"bodyType"""": """"models.MultiMessage"""",     """"headers"""": {         """"appInstance"""": """"localhost/127.0.0.1:8080"""",         """"clientIp"""": """"0:0:0:0:0:0:0:1"""",         """"host"""": """"localhost:8080"""",         """"requestId"""": """"27acf948-33ff-491c-8be7-1beb4b8c95d9"""",         """"requestMethod"""": """"POST"""",         """"requestUrl"""": """"http://localhost:8080/har"""",         """"timestamp"""": 1445914510549,         """"userPrincipal"""": """"235""""     } } {noformat} If the inner array (the Pages array) is just an object, it works, when it is an array, it fails.   The stream used: kafka --topic=agent_mixed --outputType=application/x-xd-tuple | splitter --expression=payload.body | log""","""""""Serializing a tuple object with that have a nested array which contains objects (as a tuple) fails to serialize. The error is:   when the input string (read from a Kafka topic in my case) looks something like:   If the inner array (the Pages array) is just an object, it works, when it is an array, it fails.   The stream used: kafka --topic=agent_mixed --outputType=application/x-xd-tuple | splitter --expression=payload.body | log""""""",""" Caused by: com.fasterxml.jackson.databind.JsonMappingException: No serializer found for class org.springframework.xd.tuple.DefaultTupleConversionService and no properties discovered to create BeanSerializer (to avoid exception, disable SerializationFeature.FAIL_ON_EMPTY_BEANS) ) (through reference chain: java.util.ArrayList[0]->org.springframework.xd.tuple.DefaultTuple[""""""""values""""""""]->java.util.UnmodifiableRandomAccessList[0]->org.springframework.xd.tuple.DefaultTuple[""""""""conversionService""""""""])  at com.fasterxml.jackson.databind.ser.impl.UnknownSerializer.failForEmpty(UnknownSerializer.java:59) ~[jackson-databind-2.4.5.jar:2.4.5]  at com.fasterxml.jackson.databind.ser.impl.UnknownSerializer.serialize(UnknownSerializer.java:26) ~[jackson-databind-2.4.5.jar:2.4.5]  at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:505) ~[jackson-databind-2.4.5.jar:2.4.5]  at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:639) ~[jackson-databind-2.4.5.jar:2.4.5]  at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:152) ~[jackson-databind-2.4.5.jar:2.4.5]  at com.fasterxml.jackson.databind.ser.impl.IndexedListSerializer.serializeContents(IndexedListSerializer.java:100) ~[jackson-databind-2.4.5.jar:2.4.5]  at com.fasterxml.jackson.databind.ser.impl.IndexedListSerializer.serializeContents(IndexedListSerializer.java:21) ~[jackson-databind-2.4.5.jar:2.4.5]  at com.fasterxml.jackson.databind.ser.std.AsArraySerializerBase.serialize(AsArraySerializerBase.java:183) ~[jackson-databind-2.4.5.jar:2.4.5]  at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:505) ~[jackson-databind-2.4.5.jar:2.4.5]  at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:639) ~[jackson-databind-2.4.5.jar:2.4.5]  at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:152) ~[jackson-databind-2.4.5.jar:2.4.5]  at com.fasterxml.jackson.databind.ser.impl.IndexedListSerializer.serializeContents(IndexedListSerializer.java:100) ~[jackson-databind-2.4.5.jar:2.4.5]  at com.fasterxml.jackson.databind.ser.impl.IndexedListSerializer.serializeContents(IndexedListSerializer.java:21) ~[jackson-databind-2.4.5.jar:2.4.5]  at com.fasterxml.jackson.databind.ser.std.AsArraySerializerBase.serialize(AsArraySerializerBase.java:183) ~[jackson-databind-2.4.5.jar:2.4.5]  at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider.serializeValue(DefaultSerializerProvider.java:128) ~[jackson-databind-2.4.5.jar:2.4.5]  at com.fasterxml.jackson.databind.ObjectMapper.writeValue(ObjectMapper.java:1902) ~[jackson-databind-2.4.5.jar:2.4.5]  at com.fasterxml.jackson.core.base.GeneratorBase.writeObject(GeneratorBase.java:280) ~[jackson-core-2.4.5.jar:2.4.5]  at com.fasterxml.jackson.databind.node.POJONode.serialize(POJONode.java:111) ~[jackson-databind-2.4.5.jar:2.4.5]  at com.fasterxml.jackson.databind.ser.std.SerializableSerializer.serialize(SerializableSerializer.java:44) ~[jackson-databind-2.4.5.jar:2.4.5]  at com.fasterxml.jackson.databind.ser.std.SerializableSerializer.serialize(SerializableSerializer.java:29) ~[jackson-databind-2.4.5.jar:2.4.5]  at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider.serializeValue(DefaultSerializerProvider.java:128) ~[jackson-databind-2.4.5.jar:2.4.5]  at com.fasterxml.jackson.databind.ObjectMapper.writeValue(ObjectMapper.java:1902) ~[jackson-databind-2.4.5.jar:2.4.5]  at com.fasterxml.jackson.core.base.GeneratorBase.writeObject(GeneratorBase.java:280) ~[jackson-core-2.4.5.jar:2.4.5]  at com.fasterxml.jackson.databind.node.POJONode.serialize(POJONode.java:111) ~[jackson-databind-2.4.5.jar:2.4.5]  at com.fasterxml.jackson.databind.node.ObjectNode.serialize(ObjectNode.java:264) ~[jackson-databind-2.4.5.jar:2.4.5]  at com.fasterxml.jackson.databind.ser.std.SerializableSerializer.serialize(SerializableSerializer.java:44) ~[jackson-databind-2.4.5.jar:2.4.5]  at com.fasterxml.jackson.databind.ser.std.SerializableSerializer.serialize(SerializableSerializer.java:29) ~[jackson-databind-2.4.5.jar:2.4.5]  at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider.serializeValue(DefaultSerializerProvider.java:128) ~[jackson-databind-2.4.5.jar:2.4.5]  at com.fasterxml.jackson.databind.ObjectMapper.writeValue(ObjectMapper.java:1902) ~[jackson-databind-2.4.5.jar:2.4.5]  at com.fasterxml.jackson.core.base.GeneratorBase.writeObject(GeneratorBase.java:280) ~[jackson-core-2.4.5.jar:2.4.5]  at com.fasterxml.jackson.databind.node.POJONode.serialize(POJONode.java:111) ~[jackson-databind-2.4.5.jar:2.4.5]  at com.fasterxml.jackson.databind.node.ObjectNode.serialize(ObjectNode.java:264) ~[jackson-databind-2.4.5.jar:2.4.5]  at com.fasterxml.jackson.databind.ser.std.SerializableSerializer.serialize(SerializableSerializer.java:44) ~[jackson-databind-2.4.5.jar:2.4.5]  at com.fasterxml.jackson.databind.ser.std.SerializableSerializer.serialize(SerializableSerializer.java:29) ~[jackson-databind-2.4.5.jar:2.4.5]  at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider.serializeValue(DefaultSerializerProvider.java:128) ~[jackson-databind-2.4.5.jar:2.4.5]  at com.fasterxml.jackson.databind.ObjectMapper.writeValue(ObjectMapper.java:1902) ~[jackson-databind-2.4.5.jar:2.4.5]  at com.fasterxml.jackson.core.base.GeneratorBase.writeObject(GeneratorBase.java:280) ~[jackson-core-2.4.5.jar:2.4.5]  at com.fasterxml.jackson.databind.node.POJONode.serialize(POJONode.java:111) ~[jackson-databind-2.4.5.jar:2.4.5]  at com.fasterxml.jackson.databind.node.ObjectNode.serialize(ObjectNode.java:264) ~[jackson-databind-2.4.5.jar:2.4.5]  at com.fasterxml.jackson.databind.ser.std.SerializableSerializer.serialize(SerializableSerializer.java:44) ~[jackson-databind-2.4.5.jar:2.4.5]  at com.fasterxml.jackson.databind.ser.std.SerializableSerializer.serialize(SerializableSerializer.java:29) ~[jackson-databind-2.4.5.jar:2.4.5]  at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider.serializeValue(DefaultSerializerProvider.java:128) ~[jackson-databind-2.4.5.jar:2.4.5]  at com.fasterxml.jackson.databind.ObjectMapper._configAndWriteValue(ObjectMapper.java:2881) ~[jackson-databind-2.4.5.jar:2.4.5]  at com.fasterxml.jackson.databind.ObjectMapper.writeValueAsString(ObjectMapper.java:2338) ~[jackson-databind-2.4.5.jar:2.4.5]  at org.springframework.xd.tuple.TupleToJsonStringConverter.convert(TupleToJsonStringConverter.java:37) ~[spring-xd-tuple-1.3.0.M1.jar:1.3.0.M1]  {     """"""""body"""""""": [         {             """"""""dataType"""""""": """"""""har"""""""",             """"""""har"""""""": {                 """"""""log"""""""": {                     """"""""browser"""""""": {                         """"""""name"""""""": """"""""Google Chrome"""""""",                         """"""""version"""""""": """"""""44.0.2403.155""""""""                     },                     """"""""creator"""""""": {                         """"""""name"""""""": """"""""My extension"""""""",                         """"""""version"""""""": """"""""0.23.6""""""""                     },                     """"""""pages"""""""": [                         {                             """"""""_requestTimings"""""""": {                                 """"""""blocked"""""""": -1,                                 """"""""connect"""""""": -1,                                 """"""""dns"""""""": -1,                                 """"""""receive"""""""": 11,                                 """"""""send"""""""": -1,                                 """"""""ssl"""""""": -1,                                 """"""""wait"""""""": 244                             },                             """"""""_requestUrl"""""""": """"""""https://google.com""""""""                         },                         {                             """"""""_requestTimings"""""""": {                                 """"""""blocked"""""""": -1,                                 """"""""connect"""""""": -1,                                 """"""""dns"""""""": -1,                                 """"""""receive"""""""": 11,                                 """"""""send"""""""": -1,                                 """"""""ssl"""""""": -1,                                 """"""""wait"""""""": 244                             },                             """"""""_requestUrl"""""""": """"""""https://google.com""""""""                         }                     ],                     """"""""version"""""""": """"""""1.2""""""""                 }             },             """"""""testId"""""""": 1         }     ],     """"""""bodyType"""""""": """"""""models.MultiMessage"""""""",     """"""""headers"""""""": {         """"""""appInstance"""""""": """"""""localhost/127.0.0.1:8080"""""""",         """"""""clientIp"""""""": """"""""0:0:0:0:0:0:0:1"""""""",         """"""""host"""""""": """"""""localhost:8080"""""""",         """"""""requestId"""""""": """"""""27acf948-33ff-491c-8be7-1beb4b8c95d9"""""""",         """"""""requestMethod"""""""": """"""""POST"""""""",         """"""""requestUrl"""""""": """"""""http://localhost:8080/har"""""""",         """"""""timestamp"""""""": 1445914510549,         """"""""userPrincipal"""""""": """"""""235""""""""     } } """,2.0
1,XD-3644,FEATURE,Done,MEDIUM,"""Add test coverage for batch DSL and XML generation variants""","""As a developer, I'd like to enhance test coverage to capture DSL and XML generation variants. ""","""""""As a developer, I'd like to enhance test coverage to capture DSL and XML generation variants. """"""",,5.0
1,XD-3641,FEATURE,Done,MEDIUM,"""Job composition improvements""","""As a developer, I'd like to review and refactor {{JobLaunchingTasklet}}, so I can improve performance characteristics. ""","""""""As a developer, I'd like to review and refactor {{JobLaunchingTasklet}}, so I can improve performance characteristics. """"""",,3.0
1,XD-3639,FEATURE,Done,MEDIUM,"""Create bridge processor""","""See https://github.com/spring-cloud/spring-cloud-dataflow/issues/128  This is needed to support """"channel > channel"""" type constructs""","""""""See https://github.com/spring-cloud/spring-cloud-dataflow/issues/128  This is needed to support """"""""channel > channel"""""""" type constructs""""""",,2.0
1,XD-3637,FEATURE,Done,MEDIUM,"""Upgrade to SI 4.2.1""","""As a developer, I'd like to upgrade to SI 4.2.1 release, so I can take advantage of the latest improvements.""","""""""As a developer, I'd like to upgrade to SI 4.2.1 release, so I can take advantage of the latest improvements.""""""",,1.0
1,XD-3636,FEATURE,Done,MEDIUM,"""Add support for global """"options"""" in DSL""","""As a Flo user, I'd like to have {{timeout}} and {{pollInterval}} as global options at the DSL level, so I can override the defaults at will. ""","""""""As a Flo user, I'd like to have {{timeout}} and {{pollInterval}} as global options at the DSL level, so I can override the defaults at will. """"""",,1.0
1,XD-3635,FEATURE,Done,MEDIUM,"""Resolve remaining gaps with CI""","""As a developer, I'd like to resolve remaining gaps wrt CI pipelines for Data Flow and the family, so I can continuously evaluate functionalities on every commit.""","""""""As a developer, I'd like to resolve remaining gaps wrt CI pipelines for Data Flow and the family, so I can continuously evaluate functionalities on every commit.""""""",,8.0
1,XD-3633,FEATURE,Done,MEDIUM,"""Add SFTP source to default registry""","""As a user, I'd like to use SFTP source module, so I can create streaming pipeline with it. However, I cannot see SFTP as OOTB module listed on: {{module list}} and as well as the module bits are not available in [maven repo|http://repo.spring.io/libs-snapshot/org/springframework/cloud/stream/module/]. ""","""""""As a user, I'd like to use SFTP source module, so I can create streaming pipeline with it. However, I cannot see SFTP as OOTB module listed on: {{module list}} and as well as the module bits are not available in [maven repo|http://repo.spring.io/libs-snapshot/org/springframework/cloud/stream/module/]. """"""",,1.0
1,XD-3632,BUG,Done,LOW,"""Reactor message handlers log completions at error level""","""(copied from https://github.com/spring-projects/spring-xd/issues/1810):  While testing a reactive processor that I was building, I saw the following in my test environment's logs:  {noformat} 2015-10-19 18:33:22.594 +1100 INFO/MetadataDrivenFlatFileSplitter:114 - Start splitting file=/tmp/junit8525530428026993137/junit6584105040601814728.tmp 2015-10-19 18:33:22.612 +1100 INFO/MetadataDrivenFlatFileSplitter:86 - Done splitting file=/tmp/junit8525530428026993137/junit6584105040601814728.tmp 2015-10-19 18:33:23.833 +1100 ERROR/BroadcasterMessageHandler:153 - Consumer completed for [{push}] 2015-10-19 18:33:23.834 +1100 ERROR/BroadcasterMessageHandler:153 - Consumer completed for [{push}] 2015-10-19 18:33:23.835 +1100 ERROR/BroadcasterMessageHandler:153 - Consumer completed for [{push}] 2015-10-19 18:33:23.839 +1100 ERROR/BroadcasterMessageHandler:153 - Consumer completed for [{push}] 2015-10-19 18:33:23.839 +1100 ERROR/BroadcasterMessageHandler:153 - Consumer completed for [{push}] 2015-10-19 18:33:23.840 +1100 ERROR/BroadcasterMessageHandler:153 - Consumer completed for [{push}] {noformat}  Completions don't really seem like error events. Perhaps this could be changed to INFO?  (will open a PR shortly)""","""""""(copied from https://github.com/spring-projects/spring-xd/issues/1810):  While testing a reactive processor that I was building, I saw the following in my test environment's logs:    Completions don't really seem like error events. Perhaps this could be changed to INFO?  (will open a PR shortly)""""""",""" 2015-10-19 18:33:22.594 +1100 INFO/MetadataDrivenFlatFileSplitter:114 - Start splitting file=/tmp/junit8525530428026993137/junit6584105040601814728.tmp 2015-10-19 18:33:22.612 +1100 INFO/MetadataDrivenFlatFileSplitter:86 - Done splitting file=/tmp/junit8525530428026993137/junit6584105040601814728.tmp 2015-10-19 18:33:23.833 +1100 ERROR/BroadcasterMessageHandler:153 - Consumer completed for [{push}] 2015-10-19 18:33:23.834 +1100 ERROR/BroadcasterMessageHandler:153 - Consumer completed for [{push}] 2015-10-19 18:33:23.835 +1100 ERROR/BroadcasterMessageHandler:153 - Consumer completed for [{push}] 2015-10-19 18:33:23.839 +1100 ERROR/BroadcasterMessageHandler:153 - Consumer completed for [{push}] 2015-10-19 18:33:23.839 +1100 ERROR/BroadcasterMessageHandler:153 - Consumer completed for [{push}] 2015-10-19 18:33:23.840 +1100 ERROR/BroadcasterMessageHandler:153 - Consumer completed for [{push}] """,0.0
1,XD-3631,FEATURE,Done,MEDIUM,"""Upgrade GF sink to 8.2""","""As a user, I'd like to use the latest release of {{gemfire}} sink, so I can create a streaming pipeline to land data in gemfire. ""","""""""As a user, I'd like to use the latest release of {{gemfire}} sink, so I can create a streaming pipeline to land data in gemfire. """"""",,2.0
1,XD-3629,BUG,Done,MEDIUM,"""Turning on HA via Ambari plugin requires custom configuration""","""As a user, I'd like to enable HA on {{namenode}} without having to enable custom configuration.   More details [here|https://github.com/spring-projects/spring-xd-ambari/issues/6].""","""""""As a user, I'd like to enable HA on {{namenode}} without having to enable custom configuration.   More details [here|https://github.com/spring-projects/spring-xd-ambari/issues/6].""""""",,3.0
1,XD-3627,FEATURE,Done,MEDIUM,"""Get rid of XDRuntimeException""","""As a developer, I'd like to get rid off {{XDRuntimeException}} from XD.""","""""""As a developer, I'd like to get rid off {{XDRuntimeException}} from XD.""""""",,1.0
1,XD-3624,FEATURE,Done,MEDIUM,"""Add support to build Admin with individual SPI deployers""","""As a s-c-d developer, I'd like to break the build lifecycle to bundle SPI deployers individually, so I don't have to build {{admin}} with all the deployer variations as one whole thing.""","""""""As a s-c-d developer, I'd like to break the build lifecycle to bundle SPI deployers individually, so I don't have to build {{admin}} with all the deployer variations as one whole thing.""""""",,8.0
1,XD-3622,FEATURE,Done,MEDIUM,"""Port File as s-c-s source""","""As a developer, I'd like to port {{file}} module from XD to s-c-s repo, so I can use it as {{source}} module to build streaming pipeline.""","""""""As a developer, I'd like to port {{file}} module from XD to s-c-s repo, so I can use it as {{source}} module to build streaming pipeline.""""""",,3.0
1,XD-3621,BUG,Done,HIGH,"""Add support for custom headers with the Kafka bus""","""Currently, the Kafka Message Bus does not have the ability to configure a set of custom headers to persist in the `embeddedHeaders` mode (only the message-bus specific) message headers are persisted. ""","""""""Currently, the Kafka Message Bus does not have the ability to configure a set of custom headers to persist in the `embeddedHeaders` mode (only the message-bus specific) message headers are persisted. """"""",,3.0
1,XD-3619,FEATURE,Done,MEDIUM,"""Study YARN SPI gaps""","""As a s-c-d user, I'd like to deploy data flow on YARN, so I can reuse the existing Hadoop cluster and leverage data flow features to build streaming or batch pipelines.""","""""""As a s-c-d user, I'd like to deploy data flow on YARN, so I can reuse the existing Hadoop cluster and leverage data flow features to build streaming or batch pipelines.""""""",,2.0
1,XD-3618,FEATURE,Done,MEDIUM,"""Add """"runtime info"""" shell command""","""As a s-c-d user, I'd like to have {{runtime info}} as shell command, so I can use this to list the details about the module such as {{host}}, {{port}} and the like.""","""""""As a s-c-d user, I'd like to have {{runtime info}} as shell command, so I can use this to list the details about the module such as {{host}}, {{port}} and the like.""""""",,5.0
1,XD-3617,FEATURE,Done,MEDIUM,"""Update build to use SHDP 2.3.0.RC1""","""Update build to use SHDP 2.3.0.RC1""","""Update build to use SHDP 2.3.0.RC1""",,2.0
1,XD-3616,FEATURE,Done,MEDIUM,"""Add standardized way to pass props from Deployers/Admin to ModuleLauncher""","""There is a need to customize the ModuleLauncher behavior (itself, NOT pass options to modules that are launched, which is already supported) for example to set the location of the maven repository.  ""","""""""There is a need to customize the ModuleLauncher behavior (itself, NOT pass options to modules that are launched, which is already supported) for example to set the location of the maven repository.  """"""",,3.0
1,XD-3615,FEATURE,Done,MEDIUM,"""Modules/SCD Deployers: How to provide """"cloud connector"""" support""","""Currently, s-c-s modules all come with baked in support for multiple cloud binding technologies:  {code:xml}   <!-- Lattice core dependency that activates cloud,lattice profiles when running on Lattice -->   <dependency>    <groupId>org.springframework.cloud</groupId>    <artifactId>spring-cloud-lattice-core</artifactId>    <version>${spring-cloud-lattice.version}</version>    <optional>true</optional>   </dependency>   <!-- Cloud connector dependencies -->   <!-- Lattice connector dependency to create services info from lattice -->   <dependency>    <groupId>org.springframework.cloud</groupId>    <artifactId>spring-cloud-lattice-connector</artifactId>    <version>${spring-cloud-lattice.version}</version>    <optional>true</optional>   </dependency>   <!-- CF connector dependency to create services info from CF -->   <dependency>    <groupId>org.springframework.cloud</groupId>    <artifactId>spring-cloud-cloudfoundry-connector</artifactId>    <optional>true</optional>   </dependency>   <!-- dependency to connect to detected cloud services -->   <dependency>    <groupId>org.springframework.cloud</groupId>    <artifactId>spring-cloud-spring-service-connector</artifactId>    <optional>true</optional>   </dependency> {code}  Should the deployers add those at runtime instead?""","""""""Currently, s-c-s modules all come with baked in support for multiple cloud binding technologies:    Should the deployers add those at runtime instead?""""""","""   <!-- Lattice core dependency that activates cloud,lattice profiles when running on Lattice -->   <dependency>    <groupId>org.springframework.cloud</groupId>    <artifactId>spring-cloud-lattice-core</artifactId>    <version>${spring-cloud-lattice.version}</version>    <optional>true</optional>   </dependency>   <!-- Cloud connector dependencies -->   <!-- Lattice connector dependency to create services info from lattice -->   <dependency>    <groupId>org.springframework.cloud</groupId>    <artifactId>spring-cloud-lattice-connector</artifactId>    <version>${spring-cloud-lattice.version}</version>    <optional>true</optional>   </dependency>   <!-- CF connector dependency to create services info from CF -->   <dependency>    <groupId>org.springframework.cloud</groupId>    <artifactId>spring-cloud-cloudfoundry-connector</artifactId>    <optional>true</optional>   </dependency>   <!-- dependency to connect to detected cloud services -->   <dependency>    <groupId>org.springframework.cloud</groupId>    <artifactId>spring-cloud-spring-service-connector</artifactId>    <optional>true</optional>   </dependency> """,5.0
1,XD-3613,BUG,Done,URGENT,"""Multiple module instances consuming from taps or topics get duplicate messages on redis Message Bus""","""If I deploy more than one instance of a module (eg using module.name.count > 1 or module.name.count =0) that consumes from a tap or topic then I get duplicate messages if Im using Redis as the message bus. It looks like this is the same issue as XD-3100 but the fix for that only fixed Rabbit as the message bus.  This is easy to reproduce on a 2 container cluster using a Redis Message Bus:  Create and deploy streams as follows:  {code} stream create --definition """"http | log"""" --name httpLog stream deploy --name httpLog --properties """"module.*.count=0"""" stream create --definition """"tap:stream:httpLog > transform --expression='payload.toString() + \"""" TAPPED\""""' | log"""" --name httpLogTap  stream deploy --name httpLogTap --properties """"module.*.count=0"""" {code}  On container 1 send a message:  {code} curl --data """"test message 001"""" http://localhost:9000/httpLog {code}  Container 1 logs are then:  {code} 2015-10-13 14:16:28.853  INFO 22774 --- [ol-28-thread-18] xd.sink.httpLog                          : test message 001 2015-10-13 14:16:28.855  INFO 22774 --- [enerContainer-4] xd.sink.httpLogTap                       : test message 001 TAPPED {code}  and container 2:  {code} 2015-10-13 14:16:28.859  INFO 22719 --- [enerContainer-4] xd.sink.httpLogTap                       : test message 001 TAPPED {code}  Ie the tapped message is duplicated (picked up by both tap module instances)  Similarly for topics create and deploy these streams:  {code} stream create --definition """"http > topic:mytopic"""" --name httpTopic stream deploy --name httpTopic --properties """"module.*.count=0"""" stream create --definition """"topic:mytopic > transform --expression='payload.toString() + \"""" TOPIC CONSUMER 1\""""' | log"""" --name topicConsumer1 stream deploy --name topicConsumer1 --properties """"module.*.count=0"""" stream create --definition """"topic:mytopic > transform --expression='payload.toString() + \"""" TOPIC CONSUMER 2\""""' | log"""" --name topicConsumer2 stream deploy --name topicConsumer2 --properties """"module.*.count=0"""" {code}  On container 1 send a message:  {code} curl --data """"test message 002"""" http://localhost:9000/httpLog {code}  Container 1 logs are then:  {code} 2015-10-13 14:34:23.168  INFO 22774 --- [enerContainer-2] xd.sink.topicConsumer2                   : test message 002 TOPIC CONSUMER 2 2015-10-13 14:34:23.172  INFO 22774 --- [enerContainer-2] xd.sink.topicConsumer1                   : test message 002 TOPIC CONSUMER 1 {code}  and container 2:  {code} 2015-10-13 14:34:23.173  INFO 22719 --- [enerContainer-2] xd.sink.topicConsumer2                   : test message 002 TOPIC CONSUMER 2 2015-10-13 14:34:23.177  INFO 22719 --- [enerContainer-2] xd.sink.topicConsumer1                   : test message 002 TOPIC CONSUMER 1 {code}  Ie the topic message is picked up by each instance of the module in each stream. In this case I would expect each stream to pick up the message once  ie I would get a single output for each stream   test message 002 TOPIC CONSUMER 2  once (on either container) test message 002 TOPIC CONSUMER 1  once (on either container)""","""""""If I deploy more than one instance of a module (eg using module.name.count > 1 or module.name.count =0) that consumes from a tap or topic then I get duplicate messages if Im using Redis as the message bus. It looks like this is the same issue as XD-3100 but the fix for that only fixed Rabbit as the message bus.  This is easy to reproduce on a 2 container cluster using a Redis Message Bus:  Create and deploy streams as follows:    On container 1 send a message:    Container 1 logs are then:    and container 2:    Ie the tapped message is duplicated (picked up by both tap module instances)  Similarly for topics create and deploy these streams:    On container 1 send a message:    Container 1 logs are then:    and container 2:    Ie the topic message is picked up by each instance of the module in each stream. In this case I would expect each stream to pick up the message once  ie I would get a single output for each stream   test message 002 TOPIC CONSUMER 2  once (on either container) test message 002 TOPIC CONSUMER 1  once (on either container)""""""",""" stream create --definition """"""""http | log"""""""" --name httpLog stream deploy --name httpLog --properties """"""""module.*.count=0"""""""" stream create --definition """"""""tap:stream:httpLog > transform --expression='payload.toString() + \"""""""" TAPPED\""""""""' | log"""""""" --name httpLogTap  stream deploy --name httpLogTap --properties """"""""module.*.count=0""""""""  curl --data """"""""test message 001"""""""" http://localhost:9000/httpLog  2015-10-13 14:16:28.853  INFO 22774 --- [ol-28-thread-18] xd.sink.httpLog                          : test message 001 2015-10-13 14:16:28.855  INFO 22774 --- [enerContainer-4] xd.sink.httpLogTap                       : test message 001 TAPPED  2015-10-13 14:16:28.859  INFO 22719 --- [enerContainer-4] xd.sink.httpLogTap                       : test message 001 TAPPED  stream create --definition """"""""http > topic:mytopic"""""""" --name httpTopic stream deploy --name httpTopic --properties """"""""module.*.count=0"""""""" stream create --definition """"""""topic:mytopic > transform --expression='payload.toString() + \"""""""" TOPIC CONSUMER 1\""""""""' | log"""""""" --name topicConsumer1 stream deploy --name topicConsumer1 --properties """"""""module.*.count=0"""""""" stream create --definition """"""""topic:mytopic > transform --expression='payload.toString() + \"""""""" TOPIC CONSUMER 2\""""""""' | log"""""""" --name topicConsumer2 stream deploy --name topicConsumer2 --properties """"""""module.*.count=0""""""""  curl --data """"""""test message 002"""""""" http://localhost:9000/httpLog  2015-10-13 14:34:23.168  INFO 22774 --- [enerContainer-2] xd.sink.topicConsumer2                   : test message 002 TOPIC CONSUMER 2 2015-10-13 14:34:23.172  INFO 22774 --- [enerContainer-2] xd.sink.topicConsumer1                   : test message 002 TOPIC CONSUMER 1  2015-10-13 14:34:23.173  INFO 22719 --- [enerContainer-2] xd.sink.topicConsumer2                   : test message 002 TOPIC CONSUMER 2 2015-10-13 14:34:23.177  INFO 22719 --- [enerContainer-2] xd.sink.topicConsumer1                   : test message 002 TOPIC CONSUMER 1 """,5.0
1,XD-3610,BUG,Done,MEDIUM,"""Kafka source and sink headers shouldn't interfere with bus functionality""","""The Kafka sink should not make use of the message headers sent by the Kafka receivers in the Kafka bus.   Similarly, the headers received from the Kafka source should not be propagated when sending to the Kakfa bus.   https://github.com/spring-projects/spring-xd/issues/1804""","""""""The Kafka sink should not make use of the message headers sent by the Kafka receivers in the Kafka bus.   Similarly, the headers received from the Kafka source should not be propagated when sending to the Kakfa bus.   https://github.com/spring-projects/spring-xd/issues/1804""""""",,1.0
1,XD-3608,FEATURE,Done,MEDIUM,"""Port rich-gauge as s-c-s sink""","""As a developer, I'd like to port {{rich-gauge}} module from XD to s-c-s repo, so I can use it as {{sink}} module to build streaming pipeline.""","""""""As a developer, I'd like to port {{rich-gauge}} module from XD to s-c-s repo, so I can use it as {{sink}} module to build streaming pipeline.""""""",,2.0
1,XD-3607,FEATURE,Done,MEDIUM,"""Port Gauge as s-c-s sink""","""As a developer, I'd like to port {{gauge}} module from XD to s-c-s repo, so I can use it as {{sink}} module to build streaming pipeline.""","""""""As a developer, I'd like to port {{gauge}} module from XD to s-c-s repo, so I can use it as {{sink}} module to build streaming pipeline.""""""",,2.0
1,XD-3606,FEATURE,Done,MEDIUM,"""Port aggregate-counter as s-c-s sink""","""As a developer, I'd like to port {{aggregate-counter}} module from XD to s-c-s repo, so I can use it as {{sink}} module to build streaming pipeline.""","""""""As a developer, I'd like to port {{aggregate-counter}} module from XD to s-c-s repo, so I can use it as {{sink}} module to build streaming pipeline.""""""",,2.0
1,XD-3605,FEATURE,Done,MEDIUM,"""Port field-value-counter as s-c-s sink""","""As a developer, I'd like to port {{field-value-counter}} module from XD to s-c-s repo, so I can use it as {{sink}} module to build streaming pipeline.""","""""""As a developer, I'd like to port {{field-value-counter}} module from XD to s-c-s repo, so I can use it as {{sink}} module to build streaming pipeline.""""""",,2.0
1,XD-3603,FEATURE,Done,MEDIUM,"""Implement parser for Job DSL""","""As a XD user, I'd like to have job DSL as an option, so I can leverage the DSL to create comprehensive workflows and orchestrate jobs. ""","""""""As a XD user, I'd like to have job DSL as an option, so I can leverage the DSL to create comprehensive workflows and orchestrate jobs. """"""",,0.0
1,XD-3602,FEATURE,Done,MEDIUM,"""Port Log as s-c-s sink""","""As a developer, I'd like to port {{Log}} module from XD to s-c-s repo, so I can use it as {{sink}} modules to build streaming pipeline.""","""""""As a developer, I'd like to port {{Log}} module from XD to s-c-s repo, so I can use it as {{sink}} modules to build streaming pipeline.""""""",,2.0
1,XD-3601,FEATURE,Done,MEDIUM,"""JMX MBean name clash when using labels with s-c-d deployment""","""We need to make sure that JMX MBean names are unique, even in the case of labeled modules.  The following stream fails for example: """"http | filter | filter2: filter | log""""  A good candidate could be stream name (group) + module label.""","""""""We need to make sure that JMX MBean names are unique, even in the case of labeled modules.  The following stream fails for example: """"""""http | filter | filter2: filter | log""""""""  A good candidate could be stream name (group) + module label.""""""",,3.0
1,XD-3598,BUG,Done,MEDIUM,"""Set Bean Name in ConsumerEndpointFactoryBean""","""{{LocalMessageBus}} and {{CompositeModule}}.""","""""""{{LocalMessageBus}} and {{CompositeModule}}.""""""",,1.0
1,XD-3597,FEATURE,Done,MEDIUM,"""Separate Lifecycle of Input and Output adapter endpoints""","""Described in https://github.com/spring-cloud/spring-cloud-stream/issues/144  As a developer, I want Input enpoints to be started after all the beans in the context, so that received messages can be delivered to components.  ""","""""""Described in https://github.com/spring-cloud/spring-cloud-stream/issues/144  As a developer, I want Input enpoints to be started after all the beans in the context, so that received messages can be delivered to components.  """"""",,3.0
1,XD-3596,FEATURE,Done,MEDIUM,"""Prevent streams with duplicate name""","""As a s-c-d user, I should be prevented from creating streams with duplicate name. I'd expect streams to have unique names all the time. ""","""""""As a s-c-d user, I should be prevented from creating streams with duplicate name. I'd expect streams to have unique names all the time. """"""",,1.0
1,XD-3595,FEATURE,Done,MEDIUM,"""Add test coverage for StreamController""","""As a s-c-d developer, I'd like to add test coverage for {{StreamController}}, so I can verify API contracts at build time. ""","""""""As a s-c-d developer, I'd like to add test coverage for {{StreamController}}, so I can verify API contracts at build time. """"""",,3.0
1,XD-3594,FEATURE,Done,MEDIUM,"""Add support for named channels""","""As an s-c-d user, I'd like to have the option to use _named channels_, so I can create streaming pipelines without source or sink modules. ""","""""""As an s-c-d user, I'd like to have the option to use _named channels_, so I can create streaming pipelines without source or sink modules. """"""",,3.0
1,XD-3593,FEATURE,Done,MEDIUM,"""Add support to register artifacts as libraries""","""As a SCDF user, I want to be able to register artifacts as libraries, so that I can reference them in include and exclude statements.""","""""""As a SCDF user, I want to be able to register artifacts as libraries, so that I can reference them in include and exclude statements.""""""",,2.0
1,XD-3592,FEATURE,Done,MEDIUM,"""Harmonize REST features between deployment profiles""","""As an s-c-d user, I'd like to take advantage of admin running on variety of platforms such as Lattice, YARN or CF. I'd like to access REST APIs consistently across these platforms.""","""""""As an s-c-d user, I'd like to take advantage of admin running on variety of platforms such as Lattice, YARN or CF. I'd like to access REST APIs consistently across these platforms.""""""",,5.0
1,XD-3591,BUG,Done,MEDIUM,"""Accessing Admin REST APIs on CF returns unexpected results""","""As an s-c-d user, I'm trying to access {{admin}} REST endpoints running on CF but I'm getting SSL authentication errors. ""","""""""As an s-c-d user, I'm trying to access {{admin}} REST endpoints running on CF but I'm getting SSL authentication errors. """"""",,1.0
1,XD-3588,OTHER,Done,MEDIUM,"""Summary of features to support Lattice based deployment""","""Summary of features to support Lattice based deployment""","""Summary of features to support Lattice based deployment""",,0.0
1,XD-3587,FEATURE,Done,MEDIUM,"""Move Rabbit @Rule to a separate repo""","""As an s-c-d developer, I'd like to move rabbit {{@Rule}} to a separate repo, so I can consume the test fixtures in different projects.""","""""""As an s-c-d developer, I'd like to move rabbit {{@Rule}} to a separate repo, so I can consume the test fixtures in different projects.""""""",,1.0
1,XD-3586,FEATURE,Done,MEDIUM,"""Move Kafka @Rule to a separate repo""","""As an s-c-d developer, I'd like to move kafka {{@Rule}} to a separate repo, so I can consume the test fixtures in different projects.""","""""""As an s-c-d developer, I'd like to move kafka {{@Rule}} to a separate repo, so I can consume the test fixtures in different projects.""""""",,1.0
1,XD-3585,FEATURE,Done,MEDIUM,"""Move Redis @Rule to a separate repo""","""As an s-c-d developer, I'd like to move redis {{@Rule}} to a separate repo, so I can consume the test fixtures in different projects.""","""""""As an s-c-d developer, I'd like to move redis {{@Rule}} to a separate repo, so I can consume the test fixtures in different projects.""""""",,1.0
1,XD-3583,FEATURE,Done,MEDIUM,"""Implement Mesos SPI""","""As an s-c-d user, I'd like to deploy s-c-d on Mesos.""","""""""As an s-c-d user, I'd like to deploy s-c-d on Mesos.""""""",,2.0
1,XD-3582,FEATURE,Done,MEDIUM,"""Add support for tab completion in shell""","""As an s-c-d user, I'd like to have tab completion on shell, so I can interact with the modules and its available options.""","""""""As an s-c-d user, I'd like to have tab completion on shell, so I can interact with the modules and its available options.""""""",,8.0
1,XD-3581,FEATURE,Done,MEDIUM,"""Add support for Tuple and JSON SpEL property accessors in spring-cloud-stream""","""As a spring-cloud-stream user, I'd like to build stream definitions using dot-delimited syntax for resolving properties for Tuple and JSON.""","""""""As a spring-cloud-stream user, I'd like to build stream definitions using dot-delimited syntax for resolving properties for Tuple and JSON.""""""",,3.0
1,XD-3580,FEATURE,Done,MEDIUM,"""Spike: Explore options to setup bare-metal deployment of s-c-d using Lattice""","""As a s-c-d developer, I'd like to explore options to bootstrap and setup Lattice based infrastructure for s-c-d's bare metal deployment.""","""""""As a s-c-d developer, I'd like to explore options to bootstrap and setup Lattice based infrastructure for s-c-d's bare metal deployment.""""""",,5.0
1,XD-3578,FEATURE,Done,MEDIUM,"""Add support to restart job composition""","""As an XD user, I'd like have support restart an existing composed job, so I could re-launch it at will.""","""""""As an XD user, I'd like have support restart an existing composed job, so I could re-launch it at will.""""""",,1.0
1,XD-3576,FEATURE,Done,MEDIUM,"""Add support to retrieve job details ""","""As an XD user, I'd like to click to go the detail page of the job page whether or not the selected entity is singular or part of a composed job.""","""""""As an XD user, I'd like to click to go the detail page of the job page whether or not the selected entity is singular or part of a composed job.""""""",,5.0
1,XD-3575,FEATURE,Done,MEDIUM,"""Add visual representation of job workflow in executions list page""","""As an XD user, I'd like to be able to visually differentiate between job-composition workflow and single job.""","""""""As an XD user, I'd like to be able to visually differentiate between job-composition workflow and single job.""""""",,5.0
1,XD-3574,FEATURE,Done,MEDIUM,"""Include job-composition graph in REST endpoint""","""As an XD user, I'd like to have a REST endpoint that returns job composition graph, so I can use it to build visual representation of parent-child relationship.  ""","""""""As an XD user, I'd like to have a REST endpoint that returns job composition graph, so I can use it to build visual representation of parent-child relationship.  """"""",,3.0
1,XD-3573,FEATURE,Done,MEDIUM,"""Include job-composition flag in REST endpoint""","""As an XD user, I'd like to have a REST endpoint that returns job composition {{flag}}, so I can use it to differentiate visual representation between parent-child relationship and standalone jobs. ""","""""""As an XD user, I'd like to have a REST endpoint that returns job composition {{flag}}, so I can use it to differentiate visual representation between parent-child relationship and standalone jobs. """"""",,2.0
1,XD-3572,FEATURE,Done,MEDIUM,"""Port analytic-pmml as s-c-s processor""","""As a Spring XD developer, I'd like to port {{analytic-pmml}} module from XD to s-c-s repo, so I can use it as {{processor}} module to build streaming pipeline.""","""""""As a Spring XD developer, I'd like to port {{analytic-pmml}} module from XD to s-c-s repo, so I can use it as {{processor}} module to build streaming pipeline.""""""",,5.0
1,XD-3571,FEATURE,Done,MEDIUM,"""Port Cassandra as s-c-s sink""","""As a Spring XD developer, I'd like to move {{cassandra}} module from XD to s-c-s repo, so I can use it as {{sink}} to build streaming pipeline.""","""""""As a Spring XD developer, I'd like to move {{cassandra}} module from XD to s-c-s repo, so I can use it as {{sink}} to build streaming pipeline.""""""",,5.0
1,XD-3570,FEATURE,Done,MEDIUM,"""Readme has conflicting CF information""","""In https://github.com/spring-cloud/spring-cloud-dataflow/blob/master/README.md#running-on-cloud-foundry the section starting 'Now we can configure the app' needs to be revised - the information is both out of date and, even if up-to-date, misleading (it includes some values as if they are universal, when they are really just examples).""","""""""In https://github.com/spring-cloud/spring-cloud-dataflow/blob/master/README.md#running-on-cloud-foundry the section starting 'Now we can configure the app' needs to be revised - the information is both out of date and, even if up-to-date, misleading (it includes some values as if they are universal, when they are really just examples).""""""",,1.0
1,XD-3569,FEATURE,Done,HIGH,"""ResourceModuleRegistry doesn't support HA namenode for hdfs custom module location""","""As an XD module developer I would like to use HDFS for my custom module location even when my namenode is configured for HA.   We had an issue filed in the `spring-xd-ambari` project:  """"It seems like custom module doesn't pickup namenode HA? and still use NameNodeProxies.createNonHAProxy?""""  see: https://github.com/spring-projects/spring-xd-ambari/issues/14""","""""""As an XD module developer I would like to use HDFS for my custom module location even when my namenode is configured for HA.   We had an issue filed in the `spring-xd-ambari` project:  """"""""It seems like custom module doesn't pickup namenode HA? and still use NameNodeProxies.createNonHAProxy?""""""""  see: https://github.com/spring-projects/spring-xd-ambari/issues/14""""""",,3.0
1,XD-3567,BUG,Done,URGENT,"""Fix classpath and servlet container issues""","""Several issues with 1.3.0.M1 staged version  - we now use Tomcat instead of Jetty which prevent s xd-admin from starting on YARN  - we now have Guava 18.0 on classpath instead of 16.0.1  - xd-yarn push doesn't work, hadoop client for 2.7.1 needs Servlet API   - updating Hadoop to 2.7.1 instead of 2.6.0   -- this causes Curator to also update to 2.7.1 which throws exception on startup ""","""""""Several issues with 1.3.0.M1 staged version  - we now use Tomcat instead of Jetty which prevent s xd-admin from starting on YARN  - we now have Guava 18.0 on classpath instead of 16.0.1  - xd-yarn push doesn't work, hadoop client for 2.7.1 needs Servlet API   - updating Hadoop to 2.7.1 instead of 2.6.0   -- this causes Curator to also update to 2.7.1 which throws exception on startup """"""",,3.0
1,XD-3566,FEATURE,Done,MEDIUM,"""TwitterStream test must use unique name to prevent test collision""","""XD Developer does not want the the twitter stream acceptance tests to interfere with other tests.""","""""""XD Developer does not want the the twitter stream acceptance tests to interfere with other tests.""""""",,3.0
1,XD-3565,FEATURE,Done,MEDIUM,"""Add support for multiple binders per binder type""","""As a developer, I want to be able to connect to multiple external systems for the same binding type, so that I can read data from a system and write it to another.""","""""""As a developer, I want to be able to connect to multiple external systems for the same binding type, so that I can read data from a system and write it to another.""""""",,5.0
1,XD-3563,FEATURE,Done,MEDIUM,"""Create BinderFactory abstraction""","""As a developer, I want to have a {{BinderFactory}} abstraction, so that I can support multiple binder types in the future.""","""""""As a developer, I want to have a {{BinderFactory}} abstraction, so that I can support multiple binder types in the future.""""""",,5.0
1,XD-3560,FEATURE,Done,MEDIUM,"""Better printing of array default valuesin documentation""","""When a default value is an array, the current behavior (using toString()) not only produces useless results (like `[Ljava.lang.String;@2638011`) but also constantly changing results.""","""""""When a default value is an array, the current behavior (using toString()) not only produces useless results (like `[Ljava.lang.String;@2638011`) but also constantly changing results.""""""",,5.0
1,XD-3559,FEATURE,Done,MEDIUM,"""Add support to restart job composition""","""h2. Narrative As a XD user, I'd like to restart the composed job workflow from Shell/UI. ""","""""""h2. Narrative As a XD user, I'd like to restart the composed job workflow from Shell/UI. """"""",,5.0
1,XD-3558,FEATURE,Done,MEDIUM,"""Add ability to launch job composition""","""h2. Narrative Verify that the job launch works as we expect for the composed job. ""","""""""h2. Narrative Verify that the job launch works as we expect for the composed job. """"""",,1.0
1,XD-3556,FEATURE,Done,MEDIUM,"""Develop tasklet to execute a Job""","""h2. Narrative As the system, I would like a way to launch a previously deployed job module from another job module.  h2.  Back story For the composed job story, we will have a driver job that consists of each step that represents the execution of a job.  This story is the creation of a {{Tasklet}} that will launch the child job, and upon it's completion, set the results of the driver's step to that of the slave job's results. ""","""""""h2. Narrative As the system, I would like a way to launch a previously deployed job module from another job module.  h2.  Back story For the composed job story, we will have a driver job that consists of each step that represents the execution of a job.  This story is the creation of a {{Tasklet}} that will launch the child job, and upon it's completion, set the results of the driver's step to that of the slave job's results. """"""",,5.0
1,XD-3554,FEATURE,Done,MEDIUM,"""Spike: Destroy composed job""","""As an XD developer, I'd like to explore options to remove composed job, so I can clean-up unused resources and memory footprints. ""","""""""As an XD developer, I'd like to explore options to remove composed job, so I can clean-up unused resources and memory footprints. """"""",,8.0
1,XD-3553,FEATURE,Done,MEDIUM,"""Spike: Create composed job module""","""h2. Narrative As an XD developer, I'd like to explore options to represent composed job, so I can create a workflow to orchestrate multiple jobs.  h2.  Back story For the composed job story, we need to create a """"real"""" job module to be expressed in XML, so that we can take advantage of the job execution tasklet in XD-3556, so that each job can be executed as a step in the composed job.   ""","""""""h2. Narrative As an XD developer, I'd like to explore options to represent composed job, so I can create a workflow to orchestrate multiple jobs.  h2.  Back story For the composed job story, we need to create a """"""""real"""""""" job module to be expressed in XML, so that we can take advantage of the job execution tasklet in XD-3556, so that each job can be executed as a step in the composed job.   """"""",,0.0
1,XD-3552,FEATURE,Done,MEDIUM,"""Improve automated documentation generation process for modules to handle array arguments""","""For example the generated value for the cassandra sink results in   {{-$$entityBasePackages$$:: $$the base packages to scan for entities annotated with Table annotations$$ ($$String;$$, default: `[Ljava.lang.String;@2638011`)}}  where the default value changes each time the build is run. ""","""""""For example the generated value for the cassandra sink results in   {{-$$entityBasePackages$$:: $$the base packages to scan for entities annotated with Table annotations$$ ($$String;$$, default: `[Ljava.lang.String;@2638011`)}}  where the default value changes each time the build is run. """"""",,3.0
1,XD-3549,FEATURE,Done,MEDIUM,"""Port Splitter as s-c-s processor""","""As a Spring XD developer, I'd like to port {{splitter}} module from XD to s-c-s repo, so I can use it as {{processor}} module to build streaming pipeline.""","""""""As a Spring XD developer, I'd like to port {{splitter}} module from XD to s-c-s repo, so I can use it as {{processor}} module to build streaming pipeline.""""""",,2.0
1,XD-3548,FEATURE,Done,MEDIUM,"""Port Shell as s-c-s processor""","""As a Spring XD developer, I'd like to port {{shell}} module from XD to s-c-s repo, so I can use it as {{processor}} module to build streaming pipeline.""","""""""As a Spring XD developer, I'd like to port {{shell}} module from XD to s-c-s repo, so I can use it as {{processor}} module to build streaming pipeline.""""""",,2.0
1,XD-3547,FEATURE,Done,MEDIUM,"""Port Script as s-c-s processor""","""As a Spring XD developer, I'd like to port {{script}} module from XD to s-c-s repo, so I can use it as {{processor}} module to build streaming pipeline.""","""""""As a Spring XD developer, I'd like to port {{script}} module from XD to s-c-s repo, so I can use it as {{processor}} module to build streaming pipeline.""""""",,2.0
1,XD-3546,FEATURE,Done,MEDIUM,"""Port Object to JSON as s-c-s processor""","""As a Spring XD developer, I'd like to port {{object-to-json}} module from XD to s-c-s repo, so I can use it as {{processor}} module to build streaming pipeline.""","""""""As a Spring XD developer, I'd like to port {{object-to-json}} module from XD to s-c-s repo, so I can use it as {{processor}} module to build streaming pipeline.""""""",,2.0
1,XD-3545,FEATURE,Done,MEDIUM,"""Port JSON to Tuple as s-c-s processor""","""As a Spring XD developer, I'd like to port {{json-to-tuple}} module from XD to s-c-s repo, so I can use it as {{processor}} module to build streaming pipeline.""","""""""As a Spring XD developer, I'd like to port {{json-to-tuple}} module from XD to s-c-s repo, so I can use it as {{processor}} module to build streaming pipeline.""""""",,2.0
1,XD-3544,FEATURE,Done,MEDIUM,"""Port HTTP-Client as s-c-s processor""","""As a Spring XD developer, I'd like to port {{http-client}} module from XD to s-c-s repo, so I can use it as {{processor}} module to build streaming pipeline.""","""""""As a Spring XD developer, I'd like to port {{http-client}} module from XD to s-c-s repo, so I can use it as {{processor}} module to build streaming pipeline.""""""",,2.0
1,XD-3543,FEATURE,Done,MEDIUM,"""Port aggregator as s-c-s processor""","""As a Spring XD developer, I'd like to port {{aggregator}} module from XD to s-c-s repo, so I can use it as {{processor}} module to build streaming pipeline.""","""""""As a Spring XD developer, I'd like to port {{aggregator}} module from XD to s-c-s repo, so I can use it as {{processor}} module to build streaming pipeline.""""""",,2.0
1,XD-3542,FEATURE,Done,MEDIUM,"""Port JDBC as s-c-s source""","""As a Spring XD developer, I'd like to move {{jdbc}} module from XD to s-c-s repo, so I can use it as source to build streaming pipeline. ""","""""""As a Spring XD developer, I'd like to move {{jdbc}} module from XD to s-c-s repo, so I can use it as source to build streaming pipeline. """"""",,2.0
1,XD-3541,FEATURE,Done,MEDIUM,"""Port TCP as s-c-s sink""","""As a Spring XD developer, I'd like to move {{tcp}} module from XD to s-c-s repo, so I can use it as sink to build streaming pipeline. ""","""""""As a Spring XD developer, I'd like to move {{tcp}} module from XD to s-c-s repo, so I can use it as sink to build streaming pipeline. """"""",,2.0
1,XD-3540,FEATURE,Done,MEDIUM,"""Port Splunk as s-c-s sink""","""As a Spring XD developer, I'd like to move {{splunk}} module from XD to s-c-s repo, so I can use it as sink to build streaming pipeline. ""","""""""As a Spring XD developer, I'd like to move {{splunk}} module from XD to s-c-s repo, so I can use it as sink to build streaming pipeline. """"""",,2.0
1,XD-3539,FEATURE,Done,MEDIUM,"""Port Shell as s-c-s sink""","""As a Spring XD developer, I'd like to move {{shell}} module from XD to s-c-s repo, so I can use it as sink to build streaming pipeline. ""","""""""As a Spring XD developer, I'd like to move {{shell}} module from XD to s-c-s repo, so I can use it as sink to build streaming pipeline. """"""",,2.0
1,XD-3538,FEATURE,Done,MEDIUM,"""Port NULL as s-c-s sink""","""As a Spring XD developer, I'd like to move {{null}} module from XD to s-c-s repo, so I can use it as sink to build streaming pipeline. ""","""""""As a Spring XD developer, I'd like to move {{null}} module from XD to s-c-s repo, so I can use it as sink to build streaming pipeline. """"""",,2.0
1,XD-3537,FEATURE,Done,MEDIUM,"""Port MQTT as s-c-s sink""","""As a Spring XD developer, I'd like to move {{mqtt}} module from XD to s-c-s repo, so I can use it as sink to build streaming pipeline. ""","""""""As a Spring XD developer, I'd like to move {{mqtt}} module from XD to s-c-s repo, so I can use it as sink to build streaming pipeline. """"""",,2.0
1,XD-3536,FEATURE,Done,MEDIUM,"""Port MongoDB as s-c-s sink""","""As a Spring XD developer, I'd like to move {{mongo}} module from XD to s-c-s repo, so I can use it as sink to build streaming pipeline. ""","""""""As a Spring XD developer, I'd like to move {{mongo}} module from XD to s-c-s repo, so I can use it as sink to build streaming pipeline. """"""",,2.0
1,XD-3535,FEATURE,Done,MEDIUM,"""Port Mail as s-c-s sink""","""As a Spring XD developer, I'd like to move {{mail}} module from XD to s-c-s repo, so I can use it as sink to build streaming pipeline. ""","""""""As a Spring XD developer, I'd like to move {{mail}} module from XD to s-c-s repo, so I can use it as sink to build streaming pipeline. """"""",,2.0
1,XD-3534,FEATURE,Done,MEDIUM,"""Port HDFS DataSet as s-c-s sink""","""As a Spring XD developer, I'd like to move {{hdfs-dataset}} module from XD to s-c-s repo, so I can use it as sink to build streaming pipeline. ""","""""""As a Spring XD developer, I'd like to move {{hdfs-dataset}} module from XD to s-c-s repo, so I can use it as sink to build streaming pipeline. """"""",,5.0
1,XD-3533,FEATURE,Done,MEDIUM,"""Port GPFDIST as s-c-s sink""","""As a Spring XD developer, I'd like to move {{gpfdist}} module from XD to s-c-s repo, so I can use it as sink to build streaming pipeline. ""","""""""As a Spring XD developer, I'd like to move {{gpfdist}} module from XD to s-c-s repo, so I can use it as sink to build streaming pipeline. """"""",,2.0
1,XD-3532,FEATURE,Done,MEDIUM,"""Port TCP Client as s-c-s source""","""As a Spring XD developer, I'd like to move {{tcp-client}} module from XD to s-c-s repo, so I can use it as source to build streaming pipeline. ""","""""""As a Spring XD developer, I'd like to move {{tcp-client}} module from XD to s-c-s repo, so I can use it as source to build streaming pipeline. """"""",,2.0
1,XD-3531,FEATURE,Done,MEDIUM,"""Port TCP as s-c-s source""","""As a Spring XD developer, I'd like to move {{tcp}} module from XD to s-c-s repo, so I can use it as source to build streaming pipeline.""","""""""As a Spring XD developer, I'd like to move {{tcp}} module from XD to s-c-s repo, so I can use it as source to build streaming pipeline.""""""",,2.0
1,XD-3530,FEATURE,Done,MEDIUM,"""Port Tail as s-c-s source""","""As a Spring XD developer, I'd like to move {{mail}} module from XD to s-c-s repo, so I can use it as source to build streaming pipeline.""","""""""As a Spring XD developer, I'd like to move {{mail}} module from XD to s-c-s repo, so I can use it as source to build streaming pipeline.""""""",,2.0
1,XD-3529,FEATURE,Done,MEDIUM,"""Port syslog as s-c-s source""","""As a Spring XD developer, I'd like to move {{syslog}} module from XD to s-c-s repo, so I can use it as source to build streaming pipeline. ""","""""""As a Spring XD developer, I'd like to move {{syslog}} module from XD to s-c-s repo, so I can use it as source to build streaming pipeline. """"""",,2.0
1,XD-3528,FEATURE,Done,MEDIUM,"""Port STDOUT as s-c-s source""","""As a Spring XD developer, I'd like to move {{stdout}} module from XD to s-c-s repo, so I can use it as source to build streaming pipeline.""","""""""As a Spring XD developer, I'd like to move {{stdout}} module from XD to s-c-s repo, so I can use it as source to build streaming pipeline.""""""",,2.0
1,XD-3527,FEATURE,Done,MEDIUM,"""Port ReactorIP as s-c-s source""","""As a Spring XD developer, I'd like to move {{reactor-ip}} module from XD to s-c-s repo, so I can use it as source to build streaming pipeline.""","""""""As a Spring XD developer, I'd like to move {{reactor-ip}} module from XD to s-c-s repo, so I can use it as source to build streaming pipeline.""""""",,2.0
1,XD-3526,FEATURE,Done,MEDIUM,"""Port MQTT as s-c-s source""","""As a Spring XD developer, I'd like to move {{mqtt}} module from XD to s-c-s repo, so I can use it as source to build streaming pipeline. ""","""""""As a Spring XD developer, I'd like to move {{mqtt}} module from XD to s-c-s repo, so I can use it as source to build streaming pipeline. """"""",,2.0
1,XD-3525,FEATURE,Done,MEDIUM,"""Port MongoDB as s-c-s source""","""As a Spring XD developer, I'd like to move {{mongo}} module from XD to s-c-s-m repo, so I can use it as source to build streaming pipeline. ""","""""""As a Spring XD developer, I'd like to move {{mongo}} module from XD to s-c-s-m repo, so I can use it as source to build streaming pipeline. """"""",,2.0
1,XD-3524,FEATURE,Done,MEDIUM,"""Port Mail as s-c-s source""","""As a Spring XD developer, I'd like to move {{mail}} module from XD to s-c-s-m repo, so I can use it as source to build streaming pipeline. ""","""""""As a Spring XD developer, I'd like to move {{mail}} module from XD to s-c-s-m repo, so I can use it as source to build streaming pipeline. """"""",,2.0
1,XD-3523,FEATURE,Done,MEDIUM,"""Port JMS as s-c-s source""","""As a Spring XD developer, I'd like to move {{jms}} module from XD to s-c-s-m repo, so I can use it as source to build streaming pipeline. ""","""""""As a Spring XD developer, I'd like to move {{jms}} module from XD to s-c-s-m repo, so I can use it as source to build streaming pipeline. """"""",,2.0
1,XD-3522,FEATURE,Done,MEDIUM,"""Add dynamic addition to module registry""","""As an s-c-d user, I'd like to contribute modules that immediately reflects in module registry, so I can create stream or task definitions using the shell/rest-api's.   Currently the registry isn't flexible, as it is pretty much [hard-coded at registry bootstrap level|https://github.com/spring-cloud/spring-cloud-dataflow/blob/master/spring-cloud-dataflow-admin/src/main/java/org/springframework/cloud/dataflow/admin/config/ModuleRegistryPopulator.java#L75]. ""","""""""As an s-c-d user, I'd like to contribute modules that immediately reflects in module registry, so I can create stream or task definitions using the shell/rest-api's.   Currently the registry isn't flexible, as it is pretty much [hard-coded at registry bootstrap level|https://github.com/spring-cloud/spring-cloud-dataflow/blob/master/spring-cloud-dataflow-admin/src/main/java/org/springframework/cloud/dataflow/admin/config/ModuleRegistryPopulator.java#L75]. """"""",,8.0
1,XD-3521,FEATURE,Done,MEDIUM,"""Add support to upload custom modules ""","""As an s-c-d user, I'd like to upload custom modules using shell/rest-api, so I can contribute modules and create streaming/batch pipelines. ""","""""""As an s-c-d user, I'd like to upload custom modules using shell/rest-api, so I can contribute modules and create streaming/batch pipelines. """"""",,8.0
1,XD-3519,FEATURE,Done,MEDIUM,"""Add TAP support for Rabbit binder""","""As an s-c-d user, I'd like to {{tap}} the primary pipeline, so I can fork the same data and do some ad-hoc analysis without impacting the original stream.""","""""""As an s-c-d user, I'd like to {{tap}} the primary pipeline, so I can fork the same data and do some ad-hoc analysis without impacting the original stream.""""""",,8.0
1,XD-3517,FEATURE,Done,MEDIUM,"""Document direct binding ""","""As an s-c-d user, I'd like to refer to documentation on """"direct binding"""", so I can use it as a reference to deploy a stream that includes directly bound modules.   Example: {code} java -jar spring-cloud-stream-module-launcher/target/spring-cloud-stream-module-launcher-1.0.0.BUILD-SNAPSHOT.jar --modules=org.springframework.cloud.stream.module:time-source:1.0.0.BUILD-SNAPSHOT,org.springframework.cloud.stream.module:filter-processor:1.0.0.BUILD-SNAPSHOT,org.springframework.cloud.stream.module:filter-processor:1.0.0.BUILD-SNAPSHOT --args.0.fixedDelay=7 --args.1.expression='payload.contains(""""6"""")' --aggregate=true --spring.cloud.stream.bindings.output=filtered {code} ""","""""""As an s-c-d user, I'd like to refer to documentation on """"""""direct binding"""""""", so I can use it as a reference to deploy a stream that includes directly bound modules.   Example:  """"""",""" java -jar spring-cloud-stream-module-launcher/target/spring-cloud-stream-module-launcher-1.0.0.BUILD-SNAPSHOT.jar --modules=org.springframework.cloud.stream.module:time-source:1.0.0.BUILD-SNAPSHOT,org.springframework.cloud.stream.module:filter-processor:1.0.0.BUILD-SNAPSHOT,org.springframework.cloud.stream.module:filter-processor:1.0.0.BUILD-SNAPSHOT --args.0.fixedDelay=7 --args.1.expression='payload.contains(""""""""6"""""""")' --aggregate=true --spring.cloud.stream.bindings.output=filtered """,2.0
1,XD-3516,FEATURE,Done,MEDIUM,"""Document partitioning through deployment properties""","""As an s-c-d user, I'd like to have documentation on deployment manifest, so I could refer to the relevant bits on {{partitions}}. I'd like to understand how streams withe   ""","""""""As an s-c-d user, I'd like to have documentation on deployment manifest, so I could refer to the relevant bits on {{partitions}}. I'd like to understand how streams withe   """"""",,2.0
1,XD-3512,FEATURE,Done,MEDIUM,"""Implement Gemfire message-channel binder""","""As a s-c-s user, I'd like to have {{Gemfire}} message-channel binder, so I can use {{Gemfire}} as the messaging middleware for low latency use-cases. ""","""""""As a s-c-s user, I'd like to have {{Gemfire}} message-channel binder, so I can use {{Gemfire}} as the messaging middleware for low latency use-cases. """"""",,8.0
1,XD-3509,BUG,Done,MEDIUM,"""CORS issue when trying to use HTTP in singlenode""","""When I'm trying to send a json object to spring-xd I get the following error even though I opened up requests to allow all.   XMLHttpRequest cannot load http://localhost:9000/. No 'Access-Control-Allow-Origin' header is present on the requested resource. Origin 'http://localhost:3000' is therefore not allowed access.  Config:  spring:   profiles: singlenode xd:   transport: local   ui:      allow_origin: """"*""""""","""""""When I'm trying to send a json object to spring-xd I get the following error even though I opened up requests to allow all.   XMLHttpRequest cannot load http://localhost:9000/. No 'Access-Control-Allow-Origin' header is present on the requested resource. Origin 'http://localhost:3000' is therefore not allowed access.  Config:  spring:   profiles: singlenode xd:   transport: local   ui:      allow_origin: """"""""*""""""""""""""",,3.0
1,XD-3508,BUG,Done,MEDIUM,"""Refactor to replace codec implementation with SI library""","""As a XD developer, I'd like to refactor and replace {{codec}} code from XD with SI library, so I don't have to maintain duplicate code.""","""""""As a XD developer, I'd like to refactor and replace {{codec}} code from XD with SI library, so I don't have to maintain duplicate code.""""""","""""""As a XD developer, I'd like to refactor and replace {{codec}} code from XD with SI library, so I don't have to maintain duplicate code.""""""",3.0
1,XD-3506,BUG,Done,MEDIUM,"""UI - Container List - Module Properties - Escape Passwords""","""UI - Container List - Module Properties - Escape Passwords""","""UI - Container List - Module Properties - Escape Passwords""",,1.0
1,XD-3505,BUG,Done,MEDIUM,"""Admin app crashes with SSL certification errors""","""As a s-c-d user, I'm unable to push admin app to CF due to SSL certification errors while bootstrapping.   Consider adding [CF trusted certificate|https://github.com/pivotal-cf/cloudfoundry-certificate-truster] as a CF SPI dependency.  Adding CF trusted certificate as dependency doesn't help either:  {code} > Fri Sep 25 2015 12:55:32 GMT-0400 (EDT) [App/0] ERR Caused by: sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target > Fri Sep 25 2015 12:55:32 GMT-0400 (EDT) [App/0] ERR at sun.security.validator.PKIXValidator.doBuild(PKIXValidator.java:387) > Fri Sep 25 2015 12:55:32 GMT-0400 (EDT) [App/0] ERR at sun.security.validator.PKIXValidator.engineValidate(PKIXValidator.java:292) > Fri Sep 25 2015 12:55:32 GMT-0400 (EDT) [App/0] ERR at sun.security.validator.Validator.validate(Validator.java:260) > Fri Sep 25 2015 12:55:32 GMT-0400 (EDT) [App/0] ERR at sun.security.ssl.X509TrustManagerImpl.validate(X509TrustManagerImpl.java:324) > Fri Sep 25 2015 12:55:32 GMT-0400 (EDT) [App/0] ERR at sun.security.ssl.X509TrustManagerImpl.checkTrusted(X509TrustManagerImpl.java:229) > Fri Sep 25 2015 12:55:32 GMT-... {code}""","""""""As a s-c-d user, I'm unable to push admin app to CF due to SSL certification errors while bootstrapping.   Consider adding [CF trusted certificate|https://github.com/pivotal-cf/cloudfoundry-certificate-truster] as a CF SPI dependency.  Adding CF trusted certificate as dependency doesn't help either:  """"""",""" > Fri Sep 25 2015 12:55:32 GMT-0400 (EDT) [App/0] ERR Caused by: sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target > Fri Sep 25 2015 12:55:32 GMT-0400 (EDT) [App/0] ERR at sun.security.validator.PKIXValidator.doBuild(PKIXValidator.java:387) > Fri Sep 25 2015 12:55:32 GMT-0400 (EDT) [App/0] ERR at sun.security.validator.PKIXValidator.engineValidate(PKIXValidator.java:292) > Fri Sep 25 2015 12:55:32 GMT-0400 (EDT) [App/0] ERR at sun.security.validator.Validator.validate(Validator.java:260) > Fri Sep 25 2015 12:55:32 GMT-0400 (EDT) [App/0] ERR at sun.security.ssl.X509TrustManagerImpl.validate(X509TrustManagerImpl.java:324) > Fri Sep 25 2015 12:55:32 GMT-0400 (EDT) [App/0] ERR at sun.security.ssl.X509TrustManagerImpl.checkTrusted(X509TrustManagerImpl.java:229) > Fri Sep 25 2015 12:55:32 GMT-... """,3.0
1,XD-3504,FEATURE,Done,MEDIUM,"""Support for multiple connections to the same binder implementation""","""As a s-c-s user, I'd like to have the option to use more than one binder connection factory, so I can mix and match where I consume and publish data.   More details [here|https://github.com/spring-cloud/spring-cloud-stream/issues/140].""","""""""As a s-c-s user, I'd like to have the option to use more than one binder connection factory, so I can mix and match where I consume and publish data.   More details [here|https://github.com/spring-cloud/spring-cloud-stream/issues/140].""""""",,5.0
1,XD-3503,FEATURE,Done,MEDIUM,"""Document the setting of the CORS allow_origin property""","""We do set a default value in: xd/lib/spring-xd-dirt-1.2.1.RELEASE.jar/application.yml  {code} ... xd:   data:     home: file:${XD_HOME}/data   config:     home: file:${XD_HOME}/config   module:     home: file:${XD_HOME}/modules   customModule:     home: file:${XD_HOME}/custom-modules   ui:     home: file:${XD_HOME}/spring-xd-ui/dist/     allow_origin: http://localhost:9889 ... {code}  We need to document how this property can be used to allow for external services to use the XD Rest API and how you can customize it using **servers.yml**""","""""""We do set a default value in: xd/lib/spring-xd-dirt-1.2.1.RELEASE.jar/application.yml    We need to document how this property can be used to allow for external services to use the XD Rest API and how you can customize it using **servers.yml**""""""",""" ... xd:   data:     home: file:${XD_HOME}/data   config:     home: file:${XD_HOME}/config   module:     home: file:${XD_HOME}/modules   customModule:     home: file:${XD_HOME}/custom-modules   ui:     home: file:${XD_HOME}/spring-xd-ui/dist/     allow_origin: http://localhost:9889 ... """,2.0
1,XD-3502,FEATURE,Done,MEDIUM,"""Upgrade SCSM hdfs sink to SHDP 2.3.0.M3""","""Upgrade SCSM hdfs sink to SHDP 2.3.0.M3""","""Upgrade SCSM hdfs sink to SHDP 2.3.0.M3""",,1.0
1,XD-3501,FEATURE,Done,MEDIUM,"""Admin UI container shutdown not working""","""As a user, I'm not able to shutdown {{container}} from Admin UI with the following stream definition deployed.  {code} stream create swagataTestIssue --definition """"jdbc --query='select employee_id, employee_name, employer from EMPLOYEE' --url='jdbc:oracle:thin:@//localhost:1521/orcl'  --username=springxd --password=xdpwd --driverClassName=oracle.jdbc.OracleDriver --testOnBorrow=false | hdfs --inputType=application/json """" --deploy  {code}  More details [here|https://issuetracker.springsource.com/browse/VESC-504].""","""""""As a user, I'm not able to shutdown {{container}} from Admin UI with the following stream definition deployed.    More details [here|https://issuetracker.springsource.com/browse/VESC-504].""""""",""" stream create swagataTestIssue --definition """"""""jdbc --query='select employee_id, employee_name, employer from EMPLOYEE' --url='jdbc:oracle:thin:@//localhost:1521/orcl'  --username=springxd --password=xdpwd --driverClassName=oracle.jdbc.OracleDriver --testOnBorrow=false | hdfs --inputType=application/json """""""" --deploy  """,2.0
1,XD-3494,FEATURE,Done,MEDIUM,"""Document how to use to BOM template""","""As a s-c-d developer, I'd like to document the use of BOM templates, so the general audience can use it as a reference to include external libraries dynamically.""","""""""As a s-c-d developer, I'd like to document the use of BOM templates, so the general audience can use it as a reference to include external libraries dynamically.""""""",,1.0
1,XD-3493,FEATURE,Done,MEDIUM,"""Update SI, Spring, and AMQP dependencies""","""As a XD developer, I'd like to upgrade to SI 4.2, Spring 4.2.1, and AMQP 1.5 dependencies, so I can take advantage of the latest improvements. ""","""""""As a XD developer, I'd like to upgrade to SI 4.2, Spring 4.2.1, and AMQP 1.5 dependencies, so I can take advantage of the latest improvements. """"""",,3.0
1,XD-3492,FEATURE,Done,MEDIUM,"""Move header-enricher to XD proper""","""As a XD developer, I'd like to move header-enricher from modules repo to XD proper. ""","""""""As a XD developer, I'd like to move header-enricher from modules repo to XD proper. """"""",,1.0
1,XD-3491,FEATURE,Done,MEDIUM,"""Move Cassandra sink to XD proper""","""Move Cassandra sink to XD proper""","""Move Cassandra sink to XD proper""",,3.0
1,XD-3589,FEATURE,Done,HIGH,"""Create Composed Job Module ""","""h2. Narrative As an XD developer, I need to be able to create a composed job module as XML from the DSL an store it in the Module File repository.  While the user uses the composed job as if it is a normal job including seeing only the DSL.  In the background the JobFactory will deploy the composed job module.   * When the user destroys the job the module will be deleted from the file module repository. * When the user creates the job a module will be created in the file Module repository. h2. Back story For the composed job story, we need to create a """"real"""" job module to be expressed in XML, so that we can take advantage of the job execution tasklet in XD-3556, so that each job can be executed as a step in the composed job.""","""""""h2. Narrative As an XD developer, I need to be able to create a composed job module as XML from the DSL an store it in the Module File repository.  While the user uses the composed job as if it is a normal job including seeing only the DSL.  In the background the JobFactory will deploy the composed job module.   * When the user destroys the job the module will be deleted from the file module repository. * When the user creates the job a module will be created in the file Module repository. h2. Back story For the composed job story, we need to create a """"""""real"""""""" job module to be expressed in XML, so that we can take advantage of the job execution tasklet in XD-3556, so that each job can be executed as a step in the composed job.""""""",,8.0
1,XD-3568,BUG,Done,HIGH,"""AdminServer fails on HDP 2.3""","""Submitting XD on YARN for HDP 2.3 fails due to some Solr issue in Boot - https://github.com/spring-projects/spring-boot/issues/2795  The xd-admin sysout is:  {code} Started : AdminServerApplication Documentation: https://github.com/spring-projects/spring-xd/wiki  02:51:36,624  ERROR main boot.SpringApplication - Application startup failed java.lang.IllegalStateException: Error processing condition on org.springframework.boot.autoconfigure.solr.SolrAutoConfiguration.solrServer  at org.springframework.boot.autoconfigure.condition.SpringBootCondition.matches(SpringBootCondition.java:58)  at org.springframework.context.annotation.ConditionEvaluator.shouldSkip(ConditionEvaluator.java:102)  at org.springframework.context.annotation.ConfigurationClassBeanDefinitionReader.loadBeanDefinitionsForBeanMethod(ConfigurationClassBeanDefinitionReader.java:178)  at org.springframework.context.annotation.ConfigurationClassBeanDefinitionReader.loadBeanDefinitionsForConfigurationClass(ConfigurationClassBeanDefinitionReader.java:140)  at org.springframework.context.annotation.ConfigurationClassBeanDefinitionReader.loadBeanDefinitions(ConfigurationClassBeanDefinitionReader.java:116)  at org.springframework.context.annotation.ConfigurationClassPostProcessor.processConfigBeanDefinitions(ConfigurationClassPostProcessor.java:333)  at org.springframework.context.annotation.ConfigurationClassPostProcessor.postProcessBeanDefinitionRegistry(ConfigurationClassPostProcessor.java:243)  at org.springframework.context.support.PostProcessorRegistrationDelegate.invokeBeanDefinitionRegistryPostProcessors(PostProcessorRegistrationDelegate.java:273)  at org.springframework.context.support.PostProcessorRegistrationDelegate.invokeBeanFactoryPostProcessors(PostProcessorRegistrationDelegate.java:98)  at org.springframework.context.support.AbstractApplicationContext.invokeBeanFactoryPostProcessors(AbstractApplicationContext.java:673)  at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:519)  at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:686)  at org.springframework.boot.SpringApplication.run(SpringApplication.java:320)  at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:139)  at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:129)  at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:129)  at org.springframework.xd.dirt.server.admin.AdminServerApplication.run(AdminServerApplication.java:95)  at org.springframework.xd.dirt.server.admin.AdminServerApplication.main(AdminServerApplication.java:79) Caused by: java.lang.IllegalArgumentException: @ConditionalOnMissingBean annotations must specify at least one bean (type, name or annotation)  at org.springframework.util.Assert.isTrue(Assert.java:68)  at org.springframework.boot.autoconfigure.condition.OnBeanCondition$BeanSearchSpec.<init>(OnBeanCondition.java:223)  at org.springframework.boot.autoconfigure.condition.OnBeanCondition.getMatchOutcome(OnBeanCondition.java:92)  at org.springframework.boot.autoconfigure.condition.SpringBootCondition.matches(SpringBootCondition.java:45)  ... 17 more 02:51:36,628   WARN main annotation.AnnotationConfigApplicationContext - Exception thrown from LifecycleProcessor on context close java.lang.IllegalStateException: LifecycleProcessor not initialized - call 'refresh' before invoking lifecycle methods via the context: org.springframework.context.annotation.AnnotationConfigApplicationContext@1cf1df22: startup date [Fri Oct 02 02:51:31 UTC 2015]; root of context hierarchy  at org.springframework.context.support.AbstractApplicationContext.getLifecycleProcessor(AbstractApplicationContext.java:414)  at org.springframework.context.support.AbstractApplicationContext.doClose(AbstractApplicationContext.java:966)  at org.springframework.context.support.AbstractApplicationContext.close(AbstractApplicationContext.java:925)  at org.springframework.boot.SpringApplication.run(SpringApplication.java:342)  at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:139)  at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:129)  at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:129)  at org.springframework.xd.dirt.server.admin.AdminServerApplication.run(AdminServerApplication.java:95)  at org.springframework.xd.dirt.server.admin.AdminServerApplication.main(AdminServerApplication.java:79) 02:51:36,642  ERROR main admin.AdminServerApplication - Error processing condition on org.springframework.boot.autoconfigure.solr.SolrAutoConfiguration.solrServer {code} ""","""""""Submitting XD on YARN for HDP 2.3 fails due to some Solr issue in Boot - https://github.com/spring-projects/spring-boot/issues/2795  The xd-admin sysout is:   """"""",""" Started : AdminServerApplication Documentation: https://github.com/spring-projects/spring-xd/wiki  02:51:36,624  ERROR main boot.SpringApplication - Application startup failed java.lang.IllegalStateException: Error processing condition on org.springframework.boot.autoconfigure.solr.SolrAutoConfiguration.solrServer  at org.springframework.boot.autoconfigure.condition.SpringBootCondition.matches(SpringBootCondition.java:58)  at org.springframework.context.annotation.ConditionEvaluator.shouldSkip(ConditionEvaluator.java:102)  at org.springframework.context.annotation.ConfigurationClassBeanDefinitionReader.loadBeanDefinitionsForBeanMethod(ConfigurationClassBeanDefinitionReader.java:178)  at org.springframework.context.annotation.ConfigurationClassBeanDefinitionReader.loadBeanDefinitionsForConfigurationClass(ConfigurationClassBeanDefinitionReader.java:140)  at org.springframework.context.annotation.ConfigurationClassBeanDefinitionReader.loadBeanDefinitions(ConfigurationClassBeanDefinitionReader.java:116)  at org.springframework.context.annotation.ConfigurationClassPostProcessor.processConfigBeanDefinitions(ConfigurationClassPostProcessor.java:333)  at org.springframework.context.annotation.ConfigurationClassPostProcessor.postProcessBeanDefinitionRegistry(ConfigurationClassPostProcessor.java:243)  at org.springframework.context.support.PostProcessorRegistrationDelegate.invokeBeanDefinitionRegistryPostProcessors(PostProcessorRegistrationDelegate.java:273)  at org.springframework.context.support.PostProcessorRegistrationDelegate.invokeBeanFactoryPostProcessors(PostProcessorRegistrationDelegate.java:98)  at org.springframework.context.support.AbstractApplicationContext.invokeBeanFactoryPostProcessors(AbstractApplicationContext.java:673)  at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:519)  at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:686)  at org.springframework.boot.SpringApplication.run(SpringApplication.java:320)  at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:139)  at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:129)  at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:129)  at org.springframework.xd.dirt.server.admin.AdminServerApplication.run(AdminServerApplication.java:95)  at org.springframework.xd.dirt.server.admin.AdminServerApplication.main(AdminServerApplication.java:79) Caused by: java.lang.IllegalArgumentException: @ConditionalOnMissingBean annotations must specify at least one bean (type, name or annotation)  at org.springframework.util.Assert.isTrue(Assert.java:68)  at org.springframework.boot.autoconfigure.condition.OnBeanCondition$BeanSearchSpec.<init>(OnBeanCondition.java:223)  at org.springframework.boot.autoconfigure.condition.OnBeanCondition.getMatchOutcome(OnBeanCondition.java:92)  at org.springframework.boot.autoconfigure.condition.SpringBootCondition.matches(SpringBootCondition.java:45)  ... 17 more 02:51:36,628   WARN main annotation.AnnotationConfigApplicationContext - Exception thrown from LifecycleProcessor on context close java.lang.IllegalStateException: LifecycleProcessor not initialized - call 'refresh' before invoking lifecycle methods via the context: org.springframework.context.annotation.AnnotationConfigApplicationContext@1cf1df22: startup date [Fri Oct 02 02:51:31 UTC 2015]; root of context hierarchy  at org.springframework.context.support.AbstractApplicationContext.getLifecycleProcessor(AbstractApplicationContext.java:414)  at org.springframework.context.support.AbstractApplicationContext.doClose(AbstractApplicationContext.java:966)  at org.springframework.context.support.AbstractApplicationContext.close(AbstractApplicationContext.java:925)  at org.springframework.boot.SpringApplication.run(SpringApplication.java:342)  at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:139)  at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:129)  at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:129)  at org.springframework.xd.dirt.server.admin.AdminServerApplication.run(AdminServerApplication.java:95)  at org.springframework.xd.dirt.server.admin.AdminServerApplication.main(AdminServerApplication.java:79) 02:51:36,642  ERROR main admin.AdminServerApplication - Error processing condition on org.springframework.boot.autoconfigure.solr.SolrAutoConfiguration.solrServer """,3.0
1,XD-3489,FEATURE,Done,MEDIUM,"""Add support to load Hadoop distribution of choice""","""As a s-c-d user, I'd like to have the option to choose Hadoop distribution of choice, so I can load the right Hadoop libraries in the CP.  ""","""""""As a s-c-d user, I'd like to have the option to choose Hadoop distribution of choice, so I can load the right Hadoop libraries in the CP.  """"""",,5.0
1,XD-3488,FEATURE,Done,MEDIUM,"""Refactor CF SPI with CF java-client library""","""As a s-c-d developer, I'd like to refactor CC SPI deployer with CF java-client, so I can improve the overall design and performance. ""","""""""As a s-c-d developer, I'd like to refactor CC SPI deployer with CF java-client, so I can improve the overall design and performance. """"""",,8.0
1,XD-3487,FEATURE,Done,MEDIUM,"""Add property override support for modules via external config file""","""As a s-c-d developer, I'd like to pass any overrides via external config file, so I can influence and override the default module configurations. (ex: module resolution from a different maven coordinate). ""","""""""As a s-c-d developer, I'd like to pass any overrides via external config file, so I can influence and override the default module configurations. (ex: module resolution from a different maven coordinate). """"""",,3.0
1,XD-3486,FEATURE,Done,MEDIUM,"""Spike: Study support for different binder-types for module channels""","""As a s-c-d developer, I'd like to add support for having different binder types for module's channels, so I can plug {{rabbit}}, {{redis}}, or {{kafka}} as the source or sink to read and write respectively.""","""""""As a s-c-d developer, I'd like to add support for having different binder types for module's channels, so I can plug {{rabbit}}, {{redis}}, or {{kafka}} as the source or sink to read and write respectively.""""""",,8.0
1,XD-3485,FEATURE,Done,MEDIUM,"""Port Rabbit as s-c-s sink""","""As a s-c-d developer, I'd like to move {{rabbit}} module from XD to s-c-s repo, so I can use it as {{sink}} to build streaming pipeline.""","""""""As a s-c-d developer, I'd like to move {{rabbit}} module from XD to s-c-s repo, so I can use it as {{sink}} to build streaming pipeline.""""""",,2.0
1,XD-3484,FEATURE,Done,MEDIUM,"""Port Kafka as s-c-s sink""","""As a s-c-d developer, I'd like to move {{kafka}} module from XD to s-c-s repo, so I can use it as {{sink}} to build streaming pipeline.""","""""""As a s-c-d developer, I'd like to move {{kafka}} module from XD to s-c-s repo, so I can use it as {{sink}} to build streaming pipeline.""""""",,2.0
1,XD-3483,FEATURE,Done,MEDIUM,"""Port Kafka as s-c-s source""","""As a s-c-d developer, I'd like to move {{kafka}} module from XD to s-c-s repo, so I can use it as {{source}} to build streaming pipeline.""","""""""As a s-c-d developer, I'd like to move {{kafka}} module from XD to s-c-s repo, so I can use it as {{source}} to build streaming pipeline.""""""",,2.0
1,XD-3482,FEATURE,Done,MEDIUM,"""Port JDBC as s-c-s sink""","""As a s-c-s-m developer, I'd like to move {{jdbc}} module from XD to s-c-s repo, so I can use it as {{sink}} to build streaming pipeline.   See also XD-2250""","""""""As a s-c-s-m developer, I'd like to move {{jdbc}} module from XD to s-c-s repo, so I can use it as {{sink}} to build streaming pipeline.   See also XD-2250""""""",,5.0
1,XD-3481,FEATURE,Done,MEDIUM,"""Bind message properties to modules""","""As a s-c-s developer, I'd like to support XD-like features where modules bind to incoming messages via expressions or other mechanism, so I can bind message properties to every microservice modules. ""","""""""As a s-c-s developer, I'd like to support XD-like features where modules bind to incoming messages via expressions or other mechanism, so I can bind message properties to every microservice modules. """"""",,5.0
1,XD-3479,FEATURE,Done,MEDIUM,"""Orchestrate job composition""","""As a XD user, I'd like to orchestrate composed jobs, so I can bring multiple jobs into single workflow and operationalize.""","""""""As a XD user, I'd like to orchestrate composed jobs, so I can bring multiple jobs into single workflow and operationalize.""""""",,5.0
1,XD-3478,FEATURE,Done,MEDIUM,"""Spike: Investigate options for composed jobs repository""","""As a XD developer, I'd like to explore repository options for """"composed jobs"""", so I have the leverage to read/write composed job definitions.""","""""""As a XD developer, I'd like to explore repository options for """"""""composed jobs"""""""", so I have the leverage to read/write composed job definitions.""""""",,5.0
1,XD-3471,FEATURE,Done,MEDIUM,"""Improve resilience of route creation/removal""","""The CF implementation requires that a route be created for each new app. This works fine on the happy path, but is brittle. For example, it will fail if the route required already exists.""","""""""The CF implementation requires that a route be created for each new app. This works fine on the happy path, but is brittle. For example, it will fail if the route required already exists.""""""",,2.0
1,XD-3470,IMPROVEMENT,Done,LOW,"""Add maxWait property to set in kafka message bus""","""What is the functional justification for enforcing this validation on modules? Is it really necessary to enforce that the short description must start with a capitol letter and end with a period? Seems a bit unnecessary and opinionated to me.  Caused by: org.springframework.validation.BindException: org.springframework.validation.BeanPropertyBindingResult: 1 errors Field error in object 'info' on field 'shortDescription': rejected value [...snip...]; codes [Pattern.info.shortDescription,Pattern.shortDescription,Pattern.java.lang.String,Pattern]; arguments [org.springframework.context.support.DefaultMessageSourceResolvable: codes [info.shortDescription,shortDescription]; arguments []; default message [shortDescription],[Ljavax.validation.constraints.Pattern$Flag;@3984374e,^\p{IsUppercase}.*\.$]; default message [Short description must start with a capital letter and end with a dot] ""","""""""What is the functional justification for enforcing this validation on modules? Is it really necessary to enforce that the short description must start with a capitol letter and end with a period? Seems a bit unnecessary and opinionated to me.  Caused by: org.springframework.validation.BindException: org.springframework.validation.BeanPropertyBindingResult: 1 errors Field error in object 'info' on field 'shortDescription': rejected value [...snip...]; codes [Pattern.info.shortDescription,Pattern.shortDescription,Pattern.java.lang.String,Pattern]; arguments [org.springframework.context.support.DefaultMessageSourceResolvable: codes [info.shortDescription,shortDescription]; arguments []; default message [shortDescription],[Ljavax.validation.constraints.Pattern$Flag;@3984374e,^\p{IsUppercase}.*\.$]; default message [Short description must start with a capital letter and end with a dot] """"""",,1.0
1,XD-3469,IMPROVEMENT,Done,MEDIUM,"""The new SCSM twitterstream module should produce same json as old XD source""","""The new SCSM twitterstream module uses a different format than XD 1.x source module. It should match what Twitter uses so existing processors etc. will continue to work.""","""""""The new SCSM twitterstream module uses a different format than XD 1.x source module. It should match what Twitter uses so existing processors etc. will continue to work.""""""",,3.0
1,XD-3468,BUG,Done,MEDIUM,"""Unable to set --closeTimeout on SCSM hdfs sink module""","""Creating a stream like this:    stream create --name myhdfsstream1 --definition """"time | hdfs --closeTimeout=5000"""" --deploy  causes:  java.lang.IllegalArgumentException: Task executor must be set         at org.springframework.util.Assert.notNull(Assert.java:115) ~[spring-core-4.2.1.RELEASE.jar!/:4.2.1.RELEASE]         at org.springframework.data.hadoop.store.support.PollingTaskSupport.init(PollingTaskSupport.java:105) ~[spring-data-hadoop-store-2.3.0.M2.jar!/:2.3.0.M2]         at org.springframework.data.hadoop.store.support.StoreObjectSupport.onInit(StoreObjectSupport.java:97) ~[spring-data-hadoop-store-2.3.0.M2.jar!/:2.3.0.M2]         at org.springframework.data.hadoop.store.support.OutputStoreObjectSupport.onInit(OutputStoreObjectSupport.java:81) ~[spring-data-hadoop-store-2.3.0.M2.jar!/:2.3.0.M2]         at org.springframework.data.hadoop.store.support.LifecycleObjectSupport.afterPropertiesSet(LifecycleObjectSupport.java:67) ~[spring-data-hadoop-store-2.3.0.M2.jar!/:2.3.0.M2]         at org.springframework.cloud.stream.module.hdfs.sink.DataStoreWriterFactoryBean.afterPropertiesSet(DataStoreWriterFactoryBean.java:175) ~[hdfs-sink-1.0.0.BUILD-SNAPSHOT-exec.jar!/:1.0.0.BUILD-SNAPSHOT]         at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1637) ~[spring-beans-4.2.1.RELEASE.jar!/:4.2.1.RELEASE]         at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1574) ~[spring-beans-4.2.1.RELEASE.jar!/:4.2.1.RELEASE]        ... 35 common frames omittedWrapped by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'dataStoreWriter' defined in class path resource [org/springframework/cloud/stream/module/hdfs/sink/HdfsSinkConfiguration.class]: Invocation of init method failed; nested exception is java.lang.IllegalArgumentException: Task executor must be set         at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1578) ~[spring-beans-4.2.1.RELEASE.jar!/:4.2.1.RELEASE]         at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:545) ~[spring-beans-4.2.1.RELEASE.jar!/:4.2.1.RELEASE]         at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:482) ~[spring-beans-4.2.1.RELEASE.jar!/:4.2.1.RELEASE]         at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:305) ~[spring-beans-4.2.1.RELEASE.jar!/:4.2.1.RELEASE]         at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:230) ~[spring-beans-4.2.1.RELEASE.jar!/:4.2.1.RELEASE]         at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:301) ~[spring-beans-4.2.1.RELEASE.jar!/:4.2.1.RELEASE]         at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBea""","""""""Creating a stream like this:    stream create --name myhdfsstream1 --definition """"""""time | hdfs --closeTimeout=5000"""""""" --deploy  causes:  java.lang.IllegalArgumentException: Task executor must be set         at org.springframework.util.Assert.notNull(Assert.java:115) ~[spring-core-4.2.1.RELEASE.jar!/:4.2.1.RELEASE]         at org.springframework.data.hadoop.store.support.PollingTaskSupport.init(PollingTaskSupport.java:105) ~[spring-data-hadoop-store-2.3.0.M2.jar!/:2.3.0.M2]         at org.springframework.data.hadoop.store.support.StoreObjectSupport.onInit(StoreObjectSupport.java:97) ~[spring-data-hadoop-store-2.3.0.M2.jar!/:2.3.0.M2]         at org.springframework.data.hadoop.store.support.OutputStoreObjectSupport.onInit(OutputStoreObjectSupport.java:81) ~[spring-data-hadoop-store-2.3.0.M2.jar!/:2.3.0.M2]         at org.springframework.data.hadoop.store.support.LifecycleObjectSupport.afterPropertiesSet(LifecycleObjectSupport.java:67) ~[spring-data-hadoop-store-2.3.0.M2.jar!/:2.3.0.M2]         at org.springframework.cloud.stream.module.hdfs.sink.DataStoreWriterFactoryBean.afterPropertiesSet(DataStoreWriterFactoryBean.java:175) ~[hdfs-sink-1.0.0.BUILD-SNAPSHOT-exec.jar!/:1.0.0.BUILD-SNAPSHOT]         at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1637) ~[spring-beans-4.2.1.RELEASE.jar!/:4.2.1.RELEASE]         at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1574) ~[spring-beans-4.2.1.RELEASE.jar!/:4.2.1.RELEASE]        ... 35 common frames omittedWrapped by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'dataStoreWriter' defined in class path resource [org/springframework/cloud/stream/module/hdfs/sink/HdfsSinkConfiguration.class]: Invocation of init method failed; nested exception is java.lang.IllegalArgumentException: Task executor must be set         at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1578) ~[spring-beans-4.2.1.RELEASE.jar!/:4.2.1.RELEASE]         at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:545) ~[spring-beans-4.2.1.RELEASE.jar!/:4.2.1.RELEASE]         at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:482) ~[spring-beans-4.2.1.RELEASE.jar!/:4.2.1.RELEASE]         at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:305) ~[spring-beans-4.2.1.RELEASE.jar!/:4.2.1.RELEASE]         at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:230) ~[spring-beans-4.2.1.RELEASE.jar!/:4.2.1.RELEASE]         at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:301) ~[spring-beans-4.2.1.RELEASE.jar!/:4.2.1.RELEASE]         at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBea""""""",,2.0
1,XD-3466,FEATURE,Done,MEDIUM,"""Add hdfs sink to module registry""","""As a s-c-d developer, I'd like to add _hdfs_ sink to module registry, so I can use this module to build streaming pipeline and write to Hadoop.""","""""""As a s-c-d developer, I'd like to add _hdfs_ sink to module registry, so I can use this module to build streaming pipeline and write to Hadoop.""""""",,1.0
1,XD-3464,BUG,Done,HIGH,"""Stream Destroy fails if stream deploy failed""","""(From Eric)  Deploying using the following stream fails (probably because of issues around quoting):  `stream create foo --definition """"time | filter --expression=payload.contains('0') | log"""" --deploy`  When you try to destroy the stream the destroy fails, which shouldn't happen whether the stream was valid or not.""","""""""(From Eric)  Deploying using the following stream fails (probably because of issues around quoting):  `stream create foo --definition """"""""time | filter --expression=payload.contains('0') | log"""""""" --deploy`  When you try to destroy the stream the destroy fails, which shouldn't happen whether the stream was valid or not.""""""",,2.0
1,XD-3463,FEATURE,Done,MEDIUM,"""Complete 'Running on Cloud Foundry' section in README""","""As a s-c-d developer, I'd like to document [Running on Cloud Foundry|https://github.com/spring-cloud/spring-cloud-dataflow#running-on-cloud-foundry] section in README, so it can be publicly available as deployment guideline.""","""""""As a s-c-d developer, I'd like to document [Running on Cloud Foundry|https://github.com/spring-cloud/spring-cloud-dataflow#running-on-cloud-foundry] section in README, so it can be publicly available as deployment guideline.""""""",,2.0
1,XD-3462,FEATURE,Done,MEDIUM,"""Create a new banner for spring-cloud-data-flow""","""As a s-c-d user, I'd like to create a new banner, so I can embed and display the banner when the shell server boots-up.   Perhaps use this [banner generator|http://patorjk.com/software/taag/#p=display&f=Standard&t=Spring%20Cloud%0AData%20Flow%20%20%3E%3E%3E%3E%3E%20]?""","""""""As a s-c-d user, I'd like to create a new banner, so I can embed and display the banner when the shell server boots-up.   Perhaps use this [banner generator|http://patorjk.com/software/taag/#p=display&f=Standard&t=Spring%20Cloud%0AData%20Flow%20%20%3E%3E%3E%3E%3E%20]?""""""",,1.0
1,XD-3461,FEATURE,Done,MEDIUM,"""Remove hardcoded yarn app version jars""","""We currently use fixed paths like `spring-cloud-data-yarn/spring-cloud-data-yarn-appmaster/target/spring-cloud-data-yarn-appmaster-1.0.0.BUILD-SNAPSHOT.jar` in yml files. Need to make define version during a build and allow to override location of those files.""","""""""We currently use fixed paths like `spring-cloud-data-yarn/spring-cloud-data-yarn-appmaster/target/spring-cloud-data-yarn-appmaster-1.0.0.BUILD-SNAPSHOT.jar` in yml files. Need to make define version during a build and allow to override location of those files.""""""",,1.0
1,XD-3460,BUG,Done,MEDIUM,"""Support underscore delimited module args for module launcher""","""If the module launcher's module arg is delimited by underscore (--args_0_fixedDelay=1), then boot ignores that property. It is important to support the underscore delimited property arguments as we set environment properties of these in CF and lattice environment.  The spring boot fix (https://github.com/spring-projects/spring-boot/commit/5a287455273270a20742f03e4546acde9e857bee) doesn't resolve the property if the value type of the Map is Map itself.""","""""""If the module launcher's module arg is delimited by underscore (--args_0_fixedDelay=1), then boot ignores that property. It is important to support the underscore delimited property arguments as we set environment properties of these in CF and lattice environment.  The spring boot fix (https://github.com/spring-projects/spring-boot/commit/5a287455273270a20742f03e4546acde9e857bee) doesn't resolve the property if the value type of the Map is Map itself.""""""",,2.0
1,XD-3458,FEATURE,Done,HIGH,"""Create CI Build for SCTM""","""Create CI Build for SCTM""","""Create CI Build for SCTM""",,3.0
1,XD-3456,FEATURE,Done,HIGH,"""Create infrastructure for Spring cloud task modules""","""Create Parent pom file for build Create .settings file Migrate Timestamp task from SCSM to SCTM. ""","""""""Create Parent pom file for build Create .settings file Migrate Timestamp task from SCSM to SCTM. """"""",,3.0
1,XD-3454,FEATURE,Done,MEDIUM,"""Add RxJava processor module""","""As a module author, I would like to apply RxJava processor module with spring cloud stream. ""","""""""As a module author, I would like to apply RxJava processor module with spring cloud stream. """"""",,3.0
1,XD-3453,FEATURE,Done,MEDIUM,"""Speed up upload of Module Launcher jar""","""Upload of module launcher bits is slow because we do not take into account the CC cache. To fix this we need to use an async upload, and to somehow generate the SHA, etc, for the Module Launcher so that CC can pre-empt uploading all the bits every time.""","""""""Upload of module launcher bits is slow because we do not take into account the CC cache. To fix this we need to use an async upload, and to somehow generate the SHA, etc, for the Module Launcher so that CC can pre-empt uploading all the bits every time.""""""",,3.0
1,XD-3452,FEATURE,Done,MEDIUM,"""Handle paginated responses""","""Currently we handle only a single page response from CC SPI list requests, but potentially there could be multiple ones.""","""""""Currently we handle only a single page response from CC SPI list requests, but potentially there could be multiple ones.""""""",,2.0
1,XD-3451,FEATURE,Done,MEDIUM,"""Correctly report state of module instances""","""Currently only the STARTED application (and application instance) status is recognised. This issue will look at the other possible states and report them as module instance states.  This would be trivial if we knew what the possible states might be and how we should interpret them for module instance state.""","""""""Currently only the STARTED application (and application instance) status is recognised. This issue will look at the other possible states and report them as module instance states.  This would be trivial if we knew what the possible states might be and how we should interpret them for module instance state.""""""",,2.0
1,XD-3450,FEATURE,Done,MEDIUM,"""Deploy multiple instances of a module""","""Currently we deploy a single instance, and ignore the ModuleDeploymentRequest instances setting.  It is easy to change this in the ModuleDeployer, but there is no guarantee this will work in the ModuleLauncher, so we hold off until that can be verified.""","""""""Currently we deploy a single instance, and ignore the ModuleDeploymentRequest instances setting.  It is easy to change this in the ModuleDeployer, but there is no guarantee this will work in the ModuleLauncher, so we hold off until that can be verified.""""""",,2.0
1,XD-3447,FEATURE,Done,MEDIUM,"""Document s-c-d architecture and deployment variants""","""As a s-c-d developer, I'd like to produce ref. documentation for s-c-d architecture, so I could define 1.x and 2.x deployment differences. ""","""""""As a s-c-d developer, I'd like to produce ref. documentation for s-c-d architecture, so I could define 1.x and 2.x deployment differences. """"""",,8.0
1,XD-3446,FEATURE,Done,HIGH,"""The tooltip for source displays incorrect information when using HDFS as sink""","""Deploying a Stream with HDFS sink and JDBC as source displays incorrect information on the tooltip for the JDBC source. The issue occurs when there are more than 1 containers deployed and the source is deployed on one container and the sink is deployed on another container. I have checked the REST endpoints and they seem to show the correct information. (http://localhost:9393/runtime/containers.json)  In my test case, say if there are 2 containers and source and sink are deployed on the same container, the tooltip's show correct information. The Stream I used for testing purposes is as follows - {noformat}  stream create swagataTestIssue --definition """"jdbc --query='select employee_id, employee_name, employer from EMPLOYEE' --url='jdbc:oracle:thin:@//localhost:1521/orcl'  --username=springxd --password=xdpwd --driverClassName=oracle.jdbc.OracleDriver --testOnBorrow=false | hdfs --inputType=application/json """" --deploy  {noformat}  I will attach the screenshots. This issue has also been reported when using Gemfire as source and HDFS as sink.  Thanks, Swagata""","""""""Deploying a Stream with HDFS sink and JDBC as source displays incorrect information on the tooltip for the JDBC source. The issue occurs when there are more than 1 containers deployed and the source is deployed on one container and the sink is deployed on another container. I have checked the REST endpoints and they seem to show the correct information. (http://localhost:9393/runtime/containers.json)  In my test case, say if there are 2 containers and source and sink are deployed on the same container, the tooltip's show correct information. The Stream I used for testing purposes is as follows -   I will attach the screenshots. This issue has also been reported when using Gemfire as source and HDFS as sink.  Thanks, Swagata""""""","""  stream create swagataTestIssue --definition """"""""jdbc --query='select employee_id, employee_name, employer from EMPLOYEE' --url='jdbc:oracle:thin:@//localhost:1521/orcl'  --username=springxd --password=xdpwd --driverClassName=oracle.jdbc.OracleDriver --testOnBorrow=false | hdfs --inputType=application/json """""""" --deploy  """,1.0
1,XD-3445,FEATURE,Done,MEDIUM,"""Fix Kafka Binder for s-c-s modules""","""As a s-c-s developer, I'd like to fix the {{Kafka}} binder, so I can create messaging microservices apps and successfully bind them to an operational Kafka broker. ""","""""""As a s-c-s developer, I'd like to fix the {{Kafka}} binder, so I can create messaging microservices apps and successfully bind them to an operational Kafka broker. """"""",,3.0
1,XD-3444,FEATURE,Done,MEDIUM,"""Create gh_pages for s-c-d and s-c-s-m repos""","""As a s-c-d developer, I'd like to setup {{gh_pages}} branch for s-c-d and s-c-s-m repos, so I can start pushing documentation with PR commits.""","""""""As a s-c-d developer, I'd like to setup {{gh_pages}} branch for s-c-d and s-c-s-m repos, so I can start pushing documentation with PR commits.""""""",,5.0
1,XD-3436,FEATURE,Done,MEDIUM,"""Create a spring cloud stream timestamp task module""","""Create a timestamp job that will be used a a sample for users to create their own spring boot based jobs.   ""","""""""Create a timestamp job that will be used a a sample for users to create their own spring boot based jobs.   """"""",,3.0
1,XD-3432,FEATURE,Done,MEDIUM,"""Update documentation for module launcher""","""The s-c-s-module-launcher document requires update for running it on standalone, docker, lattice. Also, the docker-compose yml requires fix so that modules in there are bound together.""","""""""The s-c-s-module-launcher document requires update for running it on standalone, docker, lattice. Also, the docker-compose yml requires fix so that modules in there are bound together.""""""",,1.0
1,XD-3431,FEATURE,Done,MEDIUM,"""Use mocks in shell tests""","""Instead of using real `moduleDeployer`, try using mocks so that the module deployer downloading the maven co-ordinates from repo can be avoided (for module deployment case).  Since module deployer and controllers are tested individually, it would be good to focus on shell functionality only for the shell tests.""","""""""Instead of using real `moduleDeployer`, try using mocks so that the module deployer downloading the maven co-ordinates from repo can be avoided (for module deployment case).  Since module deployer and controllers are tested individually, it would be good to focus on shell functionality only for the shell tests.""""""",,2.0
1,XD-3430,FEATURE,Done,MEDIUM,"""Add support for """"deployment properties""""""","""As a s-c-d developer, I'd like to provide optional key-value pairs as deployment properties, so I could leverage them at the runtime to instruct how the modules will be deployed.   _The scope of this story is to specifically support {{count}} to represent {{N}} instances of modules that share the same environment variables._""","""""""As a s-c-d developer, I'd like to provide optional key-value pairs as deployment properties, so I could leverage them at the runtime to instruct how the modules will be deployed.   _The scope of this story is to specifically support {{count}} to represent {{N}} instances of modules that share the same environment variables._""""""",,8.0
1,XD-3429,MAINTENANCE,Done,MEDIUM,"""Add """"module unregister"""" command""","""As a s-c-d developer, I'd like to have {{module unregister}} shell command, so I can unregister an existing module from the {{ModuleRegistry}}.""","""""""As a s-c-d developer, I'd like to have {{module unregister}} shell command, so I can unregister an existing module from the {{ModuleRegistry}}.""""""",,2.0
1,XD-3428,MAINTENANCE,Done,MEDIUM,"""Add """"module register"""" command""","""As a s-c-d developer, I'd like to have {{module register}} shell command, so I can register new modules in the {{ModuleRegistry}}.""","""""""As a s-c-d developer, I'd like to have {{module register}} shell command, so I can register new modules in the {{ModuleRegistry}}.""""""",,2.0
1,XD-3427,MAINTENANCE,Done,MEDIUM,"""Add """"module list"""" command""","""As a s-c-d developer, I'd like to have {{module list}} shell command, so I can query and list all the modules supported within the {{ModuleRegistry}}.""","""""""As a s-c-d developer, I'd like to have {{module list}} shell command, so I can query and list all the modules supported within the {{ModuleRegistry}}.""""""",,2.0
1,XD-3426,MAINTENANCE,Done,MEDIUM,"""Add """"module info"""" command""","""As a s-c-d developer, I'd like to have {{module info}} shell command, so I can query each of the module specifics such as description and support options. ""","""""""As a s-c-d developer, I'd like to have {{module info}} shell command, so I can query each of the module specifics such as description and support options. """"""",,2.0
1,XD-3425,FEATURE,Done,MEDIUM,"""Support shell commands to interact with module registry""","""As a s-c-d developer, I'd like to have {{module info}}, {{module list}}, {{module register}}, and {{module unregister}} commands, so I can interact with {{ModuleRegistry}}.""","""""""As a s-c-d developer, I'd like to have {{module info}}, {{module list}}, {{module register}}, and {{module unregister}} commands, so I can interact with {{ModuleRegistry}}.""""""",,8.0
1,XD-3424,BUG,Done,MEDIUM,"""Fix Cloud connector dependencies and service resolution""","""This JIRA addresses couple of issues: 1) When the modules are deployed into cloud environment there is an issue where local configuration beans collide with cloud service beans. We witnessed an issue where there are two `RedisConnectionFactory` beans registered in the same application context. We need to have a control the way in which the auto configuration gets invoked and service beans are created. 2) We need to align the cloud connector dependencies into a common place so that we don't have to specify them at various places like (SCS, SCS-Binder, SCS-modules) etc., It is a good idea to have these dependencies specified in SCS-modules so that it get used subsequently by SCS when the module is assembled at runtime.""","""""""This JIRA addresses couple of issues: 1) When the modules are deployed into cloud environment there is an issue where local configuration beans collide with cloud service beans. We witnessed an issue where there are two `RedisConnectionFactory` beans registered in the same application context. We need to have a control the way in which the auto configuration gets invoked and service beans are created. 2) We need to align the cloud connector dependencies into a common place so that we don't have to specify them at various places like (SCS, SCS-Binder, SCS-modules) etc., It is a good idea to have these dependencies specified in SCS-modules so that it get used subsequently by SCS when the module is assembled at runtime.""""""",,3.0
1,XD-3423,FEATURE,Done,HIGH,"""Update Shell to support tasks""","""h2. Narrative As a user, I need to be able to deploy a task (boot jar) via the CLI.  h2.  Back story Since the concept of jobs as an explicit primitive within Spring XD is going away in spring-cloud-data, the shell needs to be updated to reflect that.""","""""""h2. Narrative As a user, I need to be able to deploy a task (boot jar) via the CLI.  h2.  Back story Since the concept of jobs as an explicit primitive within Spring XD is going away in spring-cloud-data, the shell needs to be updated to reflect that.""""""",,5.0
1,XD-3421,FEATURE,Done,MEDIUM,"""Create a Rabbit|Kafka Available Rule in s-c-s-m""","""Can take from previous implementation in XD/SI/Boot. Should have a way to enforce not skipping tests based on an environment variable.  Consider moving this coverage to SI """"commons"""" or equivalent. ""","""""""Can take from previous implementation in XD/SI/Boot. Should have a way to enforce not skipping tests based on an environment variable.  Consider moving this coverage to SI """"""""commons"""""""" or equivalent. """"""",,3.0
1,XD-3419,FEATURE,Done,MEDIUM,"""Add registry to lookup module coordinates by name""","""As a s-c-d developer, I'd like to create {{ModuleRegistry}} implementation, so I can use this infrastructure to lookup module coordinates by name.""","""""""As a s-c-d developer, I'd like to create {{ModuleRegistry}} implementation, so I can use this infrastructure to lookup module coordinates by name.""""""",,8.0
1,XD-3418,FEATURE,Done,MEDIUM,"""Enable """"offline"""" mode for AetherModuleResolver""","""As a s-c-s developer, I'd like to enable {{offline}} mode for {{AetherModuleResolver}}, so I can pull the module artifacts from local instead of remote maven repo.""","""""""As a s-c-s developer, I'd like to enable {{offline}} mode for {{AetherModuleResolver}}, so I can pull the module artifacts from local instead of remote maven repo.""""""",,3.0
1,XD-3417,FEATURE,Done,MEDIUM,"""Add SmartLifecycle to ChannelBindingAdapter""","""Make ChannelBindingAdapter implement SmartLifecycle so that it gets started with the highest precedence and before any other message producing bean.""","""""""Make ChannelBindingAdapter implement SmartLifecycle so that it gets started with the highest precedence and before any other message producing bean.""""""",,1.0
1,XD-3416,FEATURE,Done,MEDIUM,"""Create foundation to support s-c-s 'processor' modules""","""As a s-c-d developer, I'd like to create foundation to support _processor_ as OOTB modules, so I can use the processor modules from {{s-c-s-m}} repo to build streaming pipeline.""","""""""As a s-c-d developer, I'd like to create foundation to support _processor_ as OOTB modules, so I can use the processor modules from {{s-c-s-m}} repo to build streaming pipeline.""""""",,5.0
1,XD-3414,FEATURE,Done,MEDIUM,"""Create a new project for @RedisRule""","""As a s-c-d developer, I'd like to create a new project to contain all the rules associated {{@RedisRule}} contract, so it is isolated from core functionalities and reusable by test coverage as needed.    Consider moving this coverage to SI """"commons"""" or equivalent. ""","""""""As a s-c-d developer, I'd like to create a new project to contain all the rules associated {{@RedisRule}} contract, so it is isolated from core functionalities and reusable by test coverage as needed.    Consider moving this coverage to SI """"""""commons"""""""" or equivalent. """"""",,5.0
1,XD-3412,FEATURE,Done,MEDIUM,"""Port SFTP as s-c-s source module""","""As a Spring XD developer, I'd like to port SFTP module from XD to s-c-s repo, so I can use it as source modules to build streaming pipeline. ""","""""""As a Spring XD developer, I'd like to port SFTP module from XD to s-c-s repo, so I can use it as source modules to build streaming pipeline. """"""",,5.0
1,XD-3411,FEATURE,Done,MEDIUM,"""Move external library resolver to its own project""","""As a s-c-d developer, I'd like to move the external library to its own project, so we have a clear separation of functionalities in s-c-d repo.""","""""""As a s-c-d developer, I'd like to move the external library to its own project, so we have a clear separation of functionalities in s-c-d repo.""""""",,3.0
1,XD-3406,FEATURE,Done,MEDIUM,"""Refactor to use Boot's JarLauncher""","""As a s-c-s developer, I'd like to refactor the current {{ModuleLauncher}} contract with Boot's {{JarLauncher}} API, so we don't have to maintain duplicate functionality.""","""""""As a s-c-s developer, I'd like to refactor the current {{ModuleLauncher}} contract with Boot's {{JarLauncher}} API, so we don't have to maintain duplicate functionality.""""""",,1.0
1,XD-3405,FEATURE,Done,MEDIUM,"""Spike: Study how to resolve and add JARs to Boot loader""","""As a s-c-d developer, I'd like to experiment how do we resolve and then add module dependent JAR's to Boot loader, so I have an approach to handle external libraries required by OOTB modules. ""","""""""As a s-c-d developer, I'd like to experiment how do we resolve and then add module dependent JAR's to Boot loader, so I have an approach to handle external libraries required by OOTB modules. """"""",,5.0
1,XD-3404,FEATURE,Done,MEDIUM,"""Refactor YARN deployer to deploy asycnhrounously""","""As a s-c-d developer, I'd like to make the deployer work asynchronously, so I can use the shell to return quickly and also queue deploy operations within YARN as tasks.""","""""""As a s-c-d developer, I'd like to make the deployer work asynchronously, so I can use the shell to return quickly and also queue deploy operations within YARN as tasks.""""""",,2.0
1,XD-3402,FEATURE,Done,MEDIUM,"""Add support to start Apps in YARN automatically by type""","""As an s-c-d developer, I'd like to add support to negotiate with the ResourceManager REST-APIs to deploy modules by groups. so I can build instrumentation to start the App instances automatically. Perhaps also take into account of the App specifics such as  {{appType=CLOUDDATA}} and {{appName=spring-cloud-data-yarn-app}}. ""","""""""As an s-c-d developer, I'd like to add support to negotiate with the ResourceManager REST-APIs to deploy modules by groups. so I can build instrumentation to start the App instances automatically. Perhaps also take into account of the App specifics such as  {{appType=CLOUDDATA}} and {{appName=spring-cloud-data-yarn-app}}. """"""",,3.0
1,XD-3401,FEATURE,Done,MEDIUM,"""Add support for deploying YARN app into HDFS""","""As a s-c-d developer, I'd like to add support to deploy YARN App into HDFS automatically, so I can have the {{xd-admin}} orchestrate overall deployment by leveraging the manifest to deploy where and with what assets.""","""""""As a s-c-d developer, I'd like to add support to deploy YARN App into HDFS automatically, so I can have the {{xd-admin}} orchestrate overall deployment by leveraging the manifest to deploy where and with what assets.""""""",,3.0
1,XD-3400,FEATURE,Done,MEDIUM,"""Add support for passing parameters to YARN container""","""As a s-c-d user, I'd like to have the option to support passing definition parameters into YARN container, so I can effectively use those _params_ within the module running inside the container.""","""""""As a s-c-d user, I'd like to have the option to support passing definition parameters into YARN container, so I can effectively use those _params_ within the module running inside the container.""""""",,3.0
1,XD-3399,FEATURE,Done,MEDIUM,"""Create CI Builds for SCD and Receptor Client""","""Build SCS and SCD projects upon change in github repo. Push docker image for SCD-Admin to docker hub""","""""""Build SCS and SCD projects upon change in github repo. Push docker image for SCD-Admin to docker hub""""""",,5.0
1,XD-3398,FEATURE,Done,MEDIUM,"""Create auto configuration/properties for Local Binder""","""As a s-c-s developer, I'd like to create auto configuration for {{singlenode}} binder configuration/properties, so I can automatically configure the Spring application based on the dependencies.""","""""""As a s-c-s developer, I'd like to create auto configuration for {{singlenode}} binder configuration/properties, so I can automatically configure the Spring application based on the dependencies.""""""",,1.0
1,XD-3397,FEATURE,Done,MEDIUM,"""Port admin web UI to Spring Cloud Data admin""","""As a user I should be able to use the existing admin UI client for spring-cloud-data admin with the appropriate server configurations.""","""""""As a user I should be able to use the existing admin UI client for spring-cloud-data admin with the appropriate server configurations.""""""",,2.0
1,XD-3396,FEATURE,Done,MEDIUM,"""Add cloud connector dependencies for spring-cloud-data admin""","""Spring-cloud-data admin requires lattice connector and `spring-cloud-spring-service-connector` dependencies so that the admin controllers get access to any services while running on lattice.  One example is, CounterContoller using `redis` service for MetricRepository.""","""""""Spring-cloud-data admin requires lattice connector and `spring-cloud-spring-service-connector` dependencies so that the admin controllers get access to any services while running on lattice.  One example is, CounterContoller using `redis` service for MetricRepository.""""""",,3.0
1,XD-3395,FEATURE,Done,MEDIUM,"""Module Launcher properties improvments""","""Improve Spring Cloud Stream module launcher/resolver properties:  1) Support comma separated remoteRepositories 2) Classify/group the properties""","""""""Improve Spring Cloud Stream module launcher/resolver properties:  1) Support comma separated remoteRepositories 2) Classify/group the properties""""""",,3.0
1,XD-3394,FEATURE,Done,MEDIUM,"""Known Hosts Configuration for SFTP Source""","""Spring Integration 4.2 changed the default SFTP session factory to *not* accept keys from unknown hosts by default. This is more secure.  You either have to provide a pre-populated {{known_hosts}} file or set {{allowUnknownKeys}} to true.  If you do both, the keys will be automatically added to the known hosts file.  When updating XD to 4.2.0.RC1 I simply set the boolean to true, to retain the previous behavior.  Add properties to the SFTP source to allow configuration of these properties at the stream level. ""","""""""Spring Integration 4.2 changed the default SFTP session factory to *not* accept keys from unknown hosts by default. This is more secure.  You either have to provide a pre-populated {{known_hosts}} file or set {{allowUnknownKeys}} to true.  If you do both, the keys will be automatically added to the known hosts file.  When updating XD to 4.2.0.RC1 I simply set the boolean to true, to retain the previous behavior.  Add properties to the SFTP source to allow configuration of these properties at the stream level. """"""",,1.0
1,XD-3393,FEATURE,Done,MEDIUM,"""Upgrade receptor-client to comply with latest Receptor APIs""","""As a s-c-d developer, I'd like to upgrade {{receptor-client}} to comply with latest {{Receptor}} API changes, so I can sync-up and take advantage of the recent improvements. ""","""""""As a s-c-d developer, I'd like to upgrade {{receptor-client}} to comply with latest {{Receptor}} API changes, so I can sync-up and take advantage of the recent improvements. """"""",,3.0
1,XD-3388,FEATURE,Done,MEDIUM,"""Port Trigger as s-c-s source""","""As a Spring XD developer, I'd like to move {{trigger}} module from XD to s-c-s repo, so I can use it as source to build streaming pipeline. ""","""""""As a Spring XD developer, I'd like to move {{trigger}} module from XD to s-c-s repo, so I can use it as source to build streaming pipeline. """"""",,2.0
1,XD-3387,FEATURE,Done,HIGH,"""Hide the passwords in custom modules from being displayed.""","""Hi, Passwords are visibly when using custom modules.  Attached is example custom module code and xd-shell script to reproduce the problem using stream module of type processor on Spring XD 1.2.1.RELEASE.   Compile with Maven (mvn clean install) and run xd-shell script (xd-shell --cmdfile ./runme.cmd). ""","""""""Hi, Passwords are visibly when using custom modules.  Attached is example custom module code and xd-shell script to reproduce the problem using stream module of type processor on Spring XD 1.2.1.RELEASE.   Compile with Maven (mvn clean install) and run xd-shell script (xd-shell --cmdfile ./runme.cmd). """"""",,2.0
1,XD-3385,BUG,Done,URGENT,"""Can't build and run singlenode spring-cloud-data-rest app on Ubuntu""","""Building and then running spring-cloud-data-rest app on Ubuntu fails when trying to create the first stream. The configuration ends up with a CloudFoundryConfig instead of LocalConfig for the moduleDeployer.  Env: Ubuntu 15.04 java version """"1.8.0_51"""" Java(TM) SE Runtime Environment (build 1.8.0_51-b16) Java HotSpot(TM) 64-Bit Server VM (build 25.51-b03, mixed mode)  Error: {code} 2015-08-10 11:43:47.199 ERROR 11062 --- [nio-9393-exec-1] o.s.c.d.r.c.RestControllerAdvice         : Caught exception while handling a request java.lang.UnsupportedOperationException: null  at org.springframework.cloud.data.module.deployer.cloudfoundry.CloudFoundryModuleDeployer.deploy(CloudFoundryModuleDeployer.java:30) ~[spring-cloud-data-module-deployer-cloudfoundry-1.0.0.BUILD-SNAPSHOT.jar!/:1.0.0.BUILD-SNAPSHOT]  at org.springframework.cloud.data.rest.controller.StreamController.deployStream(StreamController.java:213) ~[spring-cloud-data-rest-1.0.0.BUILD-SNAPSHOT.jar!/:1.0.0.BUILD-SNAPSHOT]  at org.springframework.cloud.data.rest.controller.StreamController.save(StreamController.java:140) ~[spring-cloud-data-rest-1.0.0.BUILD-SNAPSHOT.jar!/:1.0.0.BUILD-SNAPSHOT]  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.8.0_51]  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:1.8.0_51]  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_51]  at java.lang.reflect.Method.invoke(Method.java:497) ~[na:1.8.0_51]  at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:221) ~[spring-web-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]  at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:137) ~[spring-web-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]  at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:111) ~[spring-webmvc-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]  at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:806) ~[spring-webmvc-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]  at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:729) ~[spring-webmvc-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]  at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:85) ~[spring-webmvc-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]  at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:959) ~[spring-webmvc-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]  at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:893) ~[spring-webmvc-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]  at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:970) [spring-webmvc-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]  at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:872) [spring-webmvc-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]  at javax.servlet.http.HttpServlet.service(HttpServlet.java:648) [tomcat-embed-core-8.0.23.jar!/:8.0.23]  at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:846) [spring-webmvc-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]  at javax.servlet.http.HttpServlet.service(HttpServlet.java:729) [tomcat-embed-core-8.0.23.jar!/:8.0.23]  at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:291) [tomcat-embed-core-8.0.23.jar!/:8.0.23]  at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206) [tomcat-embed-core-8.0.23.jar!/:8.0.23]  at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:52) [tomcat-embed-websocket-8.0.23.jar!/:8.0.23]  at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:239) [tomcat-embed-core-8.0.23.jar!/:8.0.23]  at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206) [tomcat-embed-core-8.0.23.jar!/:8.0.23]  at org.springframework.boot.actuate.autoconfigure.EndpointWebMvcAutoConfiguration$ApplicationContextHeaderFilter.doFilterInternal(EndpointWebMvcAutoConfiguration.java:235) [spring-boot-actuator-1.3.0.BUILD-SNAPSHOT.jar!/:1.3.0.BUILD-SNAPSHOT]  at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]  at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:239) [tomcat-embed-core-8.0.23.jar!/:8.0.23]  at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206) [tomcat-embed-core-8.0.23.jar!/:8.0.23]  at org.springframework.boot.actuate.trace.WebRequestTraceFilter.doFilterInternal(WebRequestTraceFilter.java:102) [spring-boot-actuator-1.3.0.BUILD-SNAPSHOT.jar!/:1.3.0.BUILD-SNAPSHOT]  at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]  at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:239) [tomcat-embed-core-8.0.23.jar!/:8.0.23]  at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206) [tomcat-embed-core-8.0.23.jar!/:8.0.23]  at org.springframework.web.filter.HttpPutFormContentFilter.doFilterInternal(HttpPutFormContentFilter.java:87) [spring-web-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]  at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]  at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:239) [tomcat-embed-core-8.0.23.jar!/:8.0.23]  at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206) [tomcat-embed-core-8.0.23.jar!/:8.0.23]  at org.springframework.web.filter.HiddenHttpMethodFilter.doFilterInternal(HiddenHttpMethodFilter.java:77) [spring-web-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]  at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]  at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:239) [tomcat-embed-core-8.0.23.jar!/:8.0.23]  at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206) [tomcat-embed-core-8.0.23.jar!/:8.0.23]  at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:85) [spring-web-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]  at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]  at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:239) [tomcat-embed-core-8.0.23.jar!/:8.0.23]  at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206) [tomcat-embed-core-8.0.23.jar!/:8.0.23]  at org.springframework.boot.actuate.autoconfigure.MetricsFilter.doFilterInternal(MetricsFilter.java:69) [spring-boot-actuator-1.3.0.BUILD-SNAPSHOT.jar!/:1.3.0.BUILD-SNAPSHOT]  at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]  at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:239) [tomcat-embed-core-8.0.23.jar!/:8.0.23]  at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206) [tomcat-embed-core-8.0.23.jar!/:8.0.23]  at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:219) [tomcat-embed-core-8.0.23.jar!/:8.0.23]  at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:106) [tomcat-embed-core-8.0.23.jar!/:8.0.23]  at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:502) [tomcat-embed-core-8.0.23.jar!/:8.0.23]  at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:142) [tomcat-embed-core-8.0.23.jar!/:8.0.23]  at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:79) [tomcat-embed-core-8.0.23.jar!/:8.0.23]  at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:88) [tomcat-embed-core-8.0.23.jar!/:8.0.23]  at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:518) [tomcat-embed-core-8.0.23.jar!/:8.0.23]  at org.apache.coyote.http11.AbstractHttp11Processor.process(AbstractHttp11Processor.java:1091) [tomcat-embed-core-8.0.23.jar!/:8.0.23]  at org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process(AbstractProtocol.java:668) [tomcat-embed-core-8.0.23.jar!/:8.0.23]  at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1521) [tomcat-embed-core-8.0.23.jar!/:8.0.23]  at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.run(NioEndpoint.java:1478) [tomcat-embed-core-8.0.23.jar!/:8.0.23]  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_51]  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_51]  at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61) [tomcat-embed-core-8.0.23.jar!/:8.0.23]  at java.lang.Thread.run(Thread.java:745) [na:1.8.0_51] 2015-08-10 11:43:47.284  WARN 11062 --- [nio-9393-exec-1] .m.m.a.ExceptionHandlerExceptionResolver : Handler execution resulted in exception: null {code}""","""""""Building and then running spring-cloud-data-rest app on Ubuntu fails when trying to create the first stream. The configuration ends up with a CloudFoundryConfig instead of LocalConfig for the moduleDeployer.  Env: Ubuntu 15.04 java version """"""""1.8.0_51"""""""" Java(TM) SE Runtime Environment (build 1.8.0_51-b16) Java HotSpot(TM) 64-Bit Server VM (build 25.51-b03, mixed mode)  Error: """"""",""" 2015-08-10 11:43:47.199 ERROR 11062 --- [nio-9393-exec-1] o.s.c.d.r.c.RestControllerAdvice         : Caught exception while handling a request java.lang.UnsupportedOperationException: null  at org.springframework.cloud.data.module.deployer.cloudfoundry.CloudFoundryModuleDeployer.deploy(CloudFoundryModuleDeployer.java:30) ~[spring-cloud-data-module-deployer-cloudfoundry-1.0.0.BUILD-SNAPSHOT.jar!/:1.0.0.BUILD-SNAPSHOT]  at org.springframework.cloud.data.rest.controller.StreamController.deployStream(StreamController.java:213) ~[spring-cloud-data-rest-1.0.0.BUILD-SNAPSHOT.jar!/:1.0.0.BUILD-SNAPSHOT]  at org.springframework.cloud.data.rest.controller.StreamController.save(StreamController.java:140) ~[spring-cloud-data-rest-1.0.0.BUILD-SNAPSHOT.jar!/:1.0.0.BUILD-SNAPSHOT]  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.8.0_51]  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:1.8.0_51]  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_51]  at java.lang.reflect.Method.invoke(Method.java:497) ~[na:1.8.0_51]  at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:221) ~[spring-web-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]  at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:137) ~[spring-web-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]  at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:111) ~[spring-webmvc-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]  at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:806) ~[spring-webmvc-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]  at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:729) ~[spring-webmvc-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]  at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:85) ~[spring-webmvc-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]  at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:959) ~[spring-webmvc-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]  at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:893) ~[spring-webmvc-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]  at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:970) [spring-webmvc-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]  at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:872) [spring-webmvc-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]  at javax.servlet.http.HttpServlet.service(HttpServlet.java:648) [tomcat-embed-core-8.0.23.jar!/:8.0.23]  at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:846) [spring-webmvc-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]  at javax.servlet.http.HttpServlet.service(HttpServlet.java:729) [tomcat-embed-core-8.0.23.jar!/:8.0.23]  at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:291) [tomcat-embed-core-8.0.23.jar!/:8.0.23]  at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206) [tomcat-embed-core-8.0.23.jar!/:8.0.23]  at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:52) [tomcat-embed-websocket-8.0.23.jar!/:8.0.23]  at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:239) [tomcat-embed-core-8.0.23.jar!/:8.0.23]  at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206) [tomcat-embed-core-8.0.23.jar!/:8.0.23]  at org.springframework.boot.actuate.autoconfigure.EndpointWebMvcAutoConfiguration$ApplicationContextHeaderFilter.doFilterInternal(EndpointWebMvcAutoConfiguration.java:235) [spring-boot-actuator-1.3.0.BUILD-SNAPSHOT.jar!/:1.3.0.BUILD-SNAPSHOT]  at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]  at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:239) [tomcat-embed-core-8.0.23.jar!/:8.0.23]  at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206) [tomcat-embed-core-8.0.23.jar!/:8.0.23]  at org.springframework.boot.actuate.trace.WebRequestTraceFilter.doFilterInternal(WebRequestTraceFilter.java:102) [spring-boot-actuator-1.3.0.BUILD-SNAPSHOT.jar!/:1.3.0.BUILD-SNAPSHOT]  at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]  at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:239) [tomcat-embed-core-8.0.23.jar!/:8.0.23]  at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206) [tomcat-embed-core-8.0.23.jar!/:8.0.23]  at org.springframework.web.filter.HttpPutFormContentFilter.doFilterInternal(HttpPutFormContentFilter.java:87) [spring-web-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]  at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]  at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:239) [tomcat-embed-core-8.0.23.jar!/:8.0.23]  at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206) [tomcat-embed-core-8.0.23.jar!/:8.0.23]  at org.springframework.web.filter.HiddenHttpMethodFilter.doFilterInternal(HiddenHttpMethodFilter.java:77) [spring-web-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]  at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]  at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:239) [tomcat-embed-core-8.0.23.jar!/:8.0.23]  at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206) [tomcat-embed-core-8.0.23.jar!/:8.0.23]  at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:85) [spring-web-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]  at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]  at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:239) [tomcat-embed-core-8.0.23.jar!/:8.0.23]  at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206) [tomcat-embed-core-8.0.23.jar!/:8.0.23]  at org.springframework.boot.actuate.autoconfigure.MetricsFilter.doFilterInternal(MetricsFilter.java:69) [spring-boot-actuator-1.3.0.BUILD-SNAPSHOT.jar!/:1.3.0.BUILD-SNAPSHOT]  at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]  at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:239) [tomcat-embed-core-8.0.23.jar!/:8.0.23]  at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206) [tomcat-embed-core-8.0.23.jar!/:8.0.23]  at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:219) [tomcat-embed-core-8.0.23.jar!/:8.0.23]  at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:106) [tomcat-embed-core-8.0.23.jar!/:8.0.23]  at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:502) [tomcat-embed-core-8.0.23.jar!/:8.0.23]  at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:142) [tomcat-embed-core-8.0.23.jar!/:8.0.23]  at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:79) [tomcat-embed-core-8.0.23.jar!/:8.0.23]  at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:88) [tomcat-embed-core-8.0.23.jar!/:8.0.23]  at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:518) [tomcat-embed-core-8.0.23.jar!/:8.0.23]  at org.apache.coyote.http11.AbstractHttp11Processor.process(AbstractHttp11Processor.java:1091) [tomcat-embed-core-8.0.23.jar!/:8.0.23]  at org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process(AbstractProtocol.java:668) [tomcat-embed-core-8.0.23.jar!/:8.0.23]  at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1521) [tomcat-embed-core-8.0.23.jar!/:8.0.23]  at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.run(NioEndpoint.java:1478) [tomcat-embed-core-8.0.23.jar!/:8.0.23]  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_51]  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_51]  at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61) [tomcat-embed-core-8.0.23.jar!/:8.0.23]  at java.lang.Thread.run(Thread.java:745) [na:1.8.0_51] 2015-08-10 11:43:47.284  WARN 11062 --- [nio-9393-exec-1] .m.m.a.ExceptionHandlerExceptionResolver : Handler execution resulted in exception: null """,3.0
1,XD-3384,FEATURE,Done,MEDIUM,"""Spike: Investigate distributed deployment of s-c-s modules via YARN SPI""","""As an s-c-d developer, I'd like to investigate the distributed deployment of s-c-s modules on YARN, so I can experiment the implementation of YARN SPI and derive the strategy for {{YARNModuleDeployer}}.""","""""""As an s-c-d developer, I'd like to investigate the distributed deployment of s-c-s modules on YARN, so I can experiment the implementation of YARN SPI and derive the strategy for {{YARNModuleDeployer}}.""""""",,8.0
1,XD-3381,FEATURE,Done,MEDIUM,"""Provide test infrastructure for module authors""","""As a module author, I want to be able to test my code in """"next to real world"""" conditions (ie Integration Testing, but not really): - I want all my module wiring to be testable - I want all my module configuration (@ConfigurationProperties) to be in effect, and I want to be able to test various combination of props - I want to be able to send data to my module and assert what is coming at the other end - I want an idiomatic way of asserting the above (eg integration with Hamcrest, etc) - I DONT want to have to send data to an actual bus (redis, rabbit, etc)""","""""""As a module author, I want to be able to test my code in """"""""next to real world"""""""" conditions (ie Integration Testing, but not really): - I want all my module wiring to be testable - I want all my module configuration (@ConfigurationProperties) to be in effect, and I want to be able to test various combination of props - I want to be able to send data to my module and assert what is coming at the other end - I want an idiomatic way of asserting the above (eg integration with Hamcrest, etc) - I DONT want to have to send data to an actual bus (redis, rabbit, etc)""""""",,5.0
1,XD-3380,FEATURE,Done,MEDIUM,"""Refactor Binder @Configuration to use AutoConfiguration""","""Currently, @EnableModule hardcodes references to both the redis and rabbit configuration classes, which trigger or don't trigger based on the presence of another jar (which itself has the meat of the configuration). This is typically what boot AutoConfiguration is for.  Moreover, adding a new binding (eg Kafka or a stub for module testing) would require to crack open @EnableModule""","""""""Currently, @EnableModule hardcodes references to both the redis and rabbit configuration classes, which trigger or don't trigger based on the presence of another jar (which itself has the meat of the configuration). This is typically what boot AutoConfiguration is for.  Moreover, adding a new binding (eg Kafka or a stub for module testing) would require to crack open @EnableModule""""""",,5.0
1,XD-3378,FEATURE,Done,MEDIUM,"""Upgrade HDP/PHD distrubutions""","""As a Spring XD user, I'd like to use the latest releases of {{HDP}}/{{PHD}} distros, so I can leverage the latest features to create pipelines involving {{HDFS}}.""","""""""As a Spring XD user, I'd like to use the latest releases of {{HDP}}/{{PHD}} distros, so I can leverage the latest features to create pipelines involving {{HDFS}}.""""""",,5.0
1,XD-3377,FEATURE,Done,MEDIUM,"""Refactor Task parsing ""","""Currently the DSL parsing for tasks is a copy and paste of what it is for streams (minus the ability to parse multiple modules).  This results in a lot of duplication.  This should be refactored to remove duplication and remove explicit references to either streams or tasks in common code.""","""""""Currently the DSL parsing for tasks is a copy and paste of what it is for streams (minus the ability to parse multiple modules).  This results in a lot of duplication.  This should be refactored to remove duplication and remove explicit references to either streams or tasks in common code.""""""",,8.0
1,XD-3376,FEATURE,Done,MEDIUM,"""Port gemfire-server sink as s-c-s module""","""As a Spring XD developer, I'd like to move {{gemfire}} module from XD to s-c-s repo, so I can use it as {{sink}} to build streaming pipeline.""","""""""As a Spring XD developer, I'd like to move {{gemfire}} module from XD to s-c-s repo, so I can use it as {{sink}} to build streaming pipeline.""""""",,2.0
1,XD-3375,FEATURE,Done,MEDIUM,"""Port Rabbit as s-c-s source""","""As a s-c-d developer, I'd like to move {{rabbit}} module from XD to s-c-s repo, so I can use it as {{source}} to build streaming pipeline.""","""""""As a s-c-d developer, I'd like to move {{rabbit}} module from XD to s-c-s repo, so I can use it as {{source}} to build streaming pipeline.""""""",,2.0
1,XD-3374,FEATURE,Done,MEDIUM,"""Port Redis as s-c-s sink""","""As a Spring XD developer, I'd like to move {{redis}} module from XD to s-c-s repo, so I can use it as {{sink}} to build streaming pipeline.""","""""""As a Spring XD developer, I'd like to move {{redis}} module from XD to s-c-s repo, so I can use it as {{sink}} to build streaming pipeline.""""""",,2.0
1,XD-3373,BUG,Done,HIGH,"""First deploy/launch of Pig job that includes yarn-site.xml file fails""","""Deploying and launching a Pig job that contains a yarn-site.xml config file fails on the first deploy after XD starts up. This happens consistently.  The error is:    Error: Could not find or load main class org.apache.hadoop.mapreduce.v2.app.MRAppMaster  which indicates that the yarn-site.xml file never made it to the classpath.  Un-deploying and re-deploying the job seems to fix the problem.""","""""""Deploying and launching a Pig job that contains a yarn-site.xml config file fails on the first deploy after XD starts up. This happens consistently.  The error is:    Error: Could not find or load main class org.apache.hadoop.mapreduce.v2.app.MRAppMaster  which indicates that the yarn-site.xml file never made it to the classpath.  Un-deploying and re-deploying the job seems to fix the problem.""""""",,5.0
1,XD-3370,FEATURE,Done,MEDIUM,"""Port FTP as s-c-s sink module""","""As a Spring XD developer, I'd like to port {{FTP}} module from XD to s-c-s repo, so I can use it as {{sink}} modules to build streaming pipeline.""","""""""As a Spring XD developer, I'd like to port {{FTP}} module from XD to s-c-s repo, so I can use it as {{sink}} modules to build streaming pipeline.""""""",,5.0
1,XD-3369,FEATURE,Done,MEDIUM,"""Port File as s-c-s sink""","""As a Spring XD developer, I'd like to port {{file}} module from XD to s-c-s repo, so I can use it as {{sink}} module to build streaming pipeline.""","""""""As a Spring XD developer, I'd like to port {{file}} module from XD to s-c-s repo, so I can use it as {{sink}} module to build streaming pipeline.""""""",,3.0
1,XD-3368,FEATURE,Done,MEDIUM,"""Port Router as s-c-s sink""","""As a Spring XD developer, I'd like to port {{router}} module from XD to s-c-s repo, so I can use it as {{sink}} module to build streaming pipeline.""","""""""As a Spring XD developer, I'd like to port {{router}} module from XD to s-c-s repo, so I can use it as {{sink}} module to build streaming pipeline.""""""",,2.0
1,XD-3367,FEATURE,Done,MEDIUM,"""Port Transform as s-c-s module""","""As a Spring XD developer, I'd like to port {{transform}} module from XD to s-c-s repo, so I can use it as {{processor}} module to build streaming pipeline.""","""""""As a Spring XD developer, I'd like to port {{transform}} module from XD to s-c-s repo, so I can use it as {{processor}} module to build streaming pipeline.""""""",,2.0
1,XD-3366,FEATURE,Done,MEDIUM,"""Port Filter as s-c-s module""","""As a Spring XD developer, I'd like to port {{filter}} module from XD to s-c-s repo, so I can use it as {{processor}} module to build streaming pipeline.""","""""""As a Spring XD developer, I'd like to port {{filter}} module from XD to s-c-s repo, so I can use it as {{processor}} module to build streaming pipeline.""""""",,2.0
1,XD-3365,FEATURE,Done,MEDIUM,"""Port Twittersearch as s-c-s module""","""As a Spring XD developer, I'd like to move {{twittersearch}} module from XD to s-c-s repo, so I can use it as source modules to build streaming pipeline. ""","""""""As a Spring XD developer, I'd like to move {{twittersearch}} module from XD to s-c-s repo, so I can use it as source modules to build streaming pipeline. """"""",,2.0
1,XD-3364,FEATURE,Done,MEDIUM,"""Port Twitterstream as s-c-s module""","""As a Spring XD developer, I'd like to move {{twitterstream}} module from XD to s-c-s repo, so I can use it as source modules to build streaming pipeline. ""","""""""As a Spring XD developer, I'd like to move {{twitterstream}} module from XD to s-c-s repo, so I can use it as source modules to build streaming pipeline. """"""",,3.0
1,XD-3363,FEATURE,Done,MEDIUM,"""Port TCP as s-c-s module""","""As a Spring XD developer, I'd like to port {{tcp}} module from XD to s-c-s repo, so I can use it as {{source}} module to build streaming pipeline. ""","""""""As a Spring XD developer, I'd like to port {{tcp}} module from XD to s-c-s repo, so I can use it as {{source}} module to build streaming pipeline. """"""",,2.0
1,XD-3362,FEATURE,Done,MEDIUM,"""Port HTTP as s-c-s module""","""As a Spring XD developer, I'd like to port {{http}} module from XD to s-c-s repo, so I can use it as {{source}} module in streaming pipeline. ""","""""""As a Spring XD developer, I'd like to port {{http}} module from XD to s-c-s repo, so I can use it as {{source}} module in streaming pipeline. """"""",,2.0
1,XD-3360,FEATURE,Done,MEDIUM,"""Add Spring Cloud Config to SPI Module Parent""","""Enable spring cloud config for all modules  * Add spring cloud config client to pom dependencies.  * Add bootstrap.yml to scs project ""","""""""Enable spring cloud config for all modules  * Add spring cloud config client to pom dependencies.  * Add bootstrap.yml to scs project """"""",,2.0
1,XD-3359,FEATURE,Done,MEDIUM,"""Standardize Spring Cloud Data configuration""","""User can configure spring cloud data via  via Spring Cloud Config, data-admin.yml or Spring Cloud Connector * Add bootstrap.yml to spring cloud data * create a default data-admin.yml and configure spring data to look for this vs application.yml. * Spring Cloud Data will have Spring Cloud Config enabled by default ** User has the ability to disable it via the bootstrap.yml""","""""""User can configure spring cloud data via  via Spring Cloud Config, data-admin.yml or Spring Cloud Connector * Add bootstrap.yml to spring cloud data * create a default data-admin.yml and configure spring data to look for this vs application.yml. * Spring Cloud Data will have Spring Cloud Config enabled by default ** User has the ability to disable it via the bootstrap.yml""""""",,5.0
1,XD-3358,BUG,Done,MEDIUM,"""Admin UI deploys job with wrong module count""","""When deploying a job through admin UI with a count of 0 the module is actually deployed with count 1.  More info here: [http://stackoverflow.com/questions/31858631/how-to-define-named-channel-consumer-module-deployment-properties]""","""""""When deploying a job through admin UI with a count of 0 the module is actually deployed with count 1.  More info here: [http://stackoverflow.com/questions/31858631/how-to-define-named-channel-consumer-module-deployment-properties]""""""",,2.0
1,XD-3355,FEATURE,Done,MEDIUM,"""Add support for 'module info' to list module properties""","""As a s-c-d developer, I'd like to derive a strategy for module metadata via {{@ConfigurationProperties}}, so I can implement {{module info}} command in shell to list all the module properties.  ""","""""""As a s-c-d developer, I'd like to derive a strategy for module metadata via {{@ConfigurationProperties}}, so I can implement {{module info}} command in shell to list all the module properties.  """"""",,0.0
1,XD-3354,FEATURE,Done,MEDIUM,"""Move shell integration tests to spring-cloud-data shell ""","""This could focus only on the subset (Stream operations)""","""""""This could focus only on the subset (Stream operations)""""""",,3.0
1,XD-3353,FEATURE,Done,MEDIUM,"""Add shell as a rest client to the spring-cloud-data REST API""","""As a user I would like to have shell interface to the spring-cloud-data rest API. The scope for this JIRA could be limited to stream commands.""","""""""As a user I would like to have shell interface to the spring-cloud-data rest API. The scope for this JIRA could be limited to stream commands.""""""",,5.0
1,XD-3352,FEATURE,Done,MEDIUM,"""[SCS] - Replace Binder XML config with @Configuration""","""Create Confguration and ConfigurationProperties. Configuration must support replacing the default Kryo Codec implementation with something else.""","""""""Create Confguration and ConfigurationProperties. Configuration must support replacing the default Kryo Codec implementation with something else.""""""",,3.0
1,XD-3351,FEATURE,Done,MEDIUM,"""[SCS]- Replace codec impl in spring-cloud-stream-codec with SI-Codec""","""[SCS]- Replace codec impl in spring-cloud-stream-codec with SI-Codec""","""[SCS]- Replace codec impl in spring-cloud-stream-codec with SI-Codec""",,2.0
1,XD-3350,FEATURE,Done,MEDIUM,"""Add support to expose counter metrics for dashboarding""","""As a s-c-d developer, I'd like to add support to expose counter (metrics) endpoints, so I can consume to feed the dashboards to demonstrate {{firehose | counter}} pipe.""","""""""As a s-c-d developer, I'd like to add support to expose counter (metrics) endpoints, so I can consume to feed the dashboards to demonstrate {{firehose | counter}} pipe.""""""",,3.0
1,XD-3349,FEATURE,Done,MEDIUM,"""Design the foundation to port XD modules to s-c-s""","""As an s-c-s developer, I'd like to brainstorm and design the foundation to port XD modules as s-c-s modules, so I can use it as the base and start migrating the modules.""","""""""As an s-c-s developer, I'd like to brainstorm and design the foundation to port XD modules as s-c-s modules, so I can use it as the base and start migrating the modules.""""""",,5.0
1,XD-3348,FEATURE,Done,MEDIUM,"""Add profile support for stream repositories""","""As a s-c-d developer, I'd like to add support for _profiles_ to the core {{Admin}} application, so I can back the stream repository with respective backend strategy. For example: {{local}} profile would use in-memory strategy to store the metadata.""","""""""As a s-c-d developer, I'd like to add support for _profiles_ to the core {{Admin}} application, so I can back the stream repository with respective backend strategy. For example: {{local}} profile would use in-memory strategy to store the metadata.""""""",,3.0
1,XD-3347,FEATURE,Done,MEDIUM,"""Run all shell integration tests also with enabled security""","""Apply the same strategy for the Module Command Tests also to all other Shell integration tests.""","""""""Apply the same strategy for the Module Command Tests also to all other Shell integration tests.""""""",,5.0
1,XD-3346,BUG,Done,MEDIUM,"""Accessing step progress via REST fails with 403""","""As a XD user, I'm trying to access URI (- GET /jobs/executions//steps//progress => hasRole('ROLE_VIEW')), but it fails with 403 forbidden error for the role with view access. More details [here|https://issuetracker.springsource.com/browse/VESC-475].  Another URL with the same error: http://<HOST>:9393/streams/definitions.json?page=0&size=10""","""""""As a XD user, I'm trying to access URI (- GET /jobs/executions//steps//progress => hasRole('ROLE_VIEW')), but it fails with 403 forbidden error for the role with view access. More details [here|https://issuetracker.springsource.com/browse/VESC-475].  Another URL with the same error: http://<HOST>:9393/streams/definitions.json?page=0&size=10""""""",,1.0
1,XD-3344,FEATURE,Done,MEDIUM,"""Implement undeploy operation for singlenode SPI ""","""As a s-c-d developer, I'd like to implement _undeploy_ operation for {{singlenode}} (single JVM), so I can use this target to undeploy a running stream. More details in [this PR|https://github.com/spring-cloud/spring-cloud-data/pull/19].  *Note:* Its a prerequisite to determine consistent _undeploy_ strategy for both {{jobs}} and {{streams}}. ""","""""""As a s-c-d developer, I'd like to implement _undeploy_ operation for {{singlenode}} (single JVM), so I can use this target to undeploy a running stream. More details in [this PR|https://github.com/spring-cloud/spring-cloud-data/pull/19].  *Note:* Its a prerequisite to determine consistent _undeploy_ strategy for both {{jobs}} and {{streams}}. """"""",,8.0
1,XD-3341,FEATURE,Done,MEDIUM,"""Publish s-c-d image to DockerHub""","""As a s-c-d developer, I'd like to publish the s-c-d image to DockerHub, so I can incrementally push the latest commits to the remote location.""","""""""As a s-c-d developer, I'd like to publish the s-c-d image to DockerHub, so I can incrementally push the latest commits to the remote location.""""""",,1.0
1,XD-3339,FEATURE,Done,MEDIUM,"""Dependency resolution support for modules with different versions""","""As a s-c-d developer, I'd like to add support for dependency resolution, so when two or more modules use different version of jars (ex: direct binding of two modules that include different versions of spring data), I have the capability to resolve and include the right bits at runtime.""","""""""As a s-c-d developer, I'd like to add support for dependency resolution, so when two or more modules use different version of jars (ex: direct binding of two modules that include different versions of spring data), I have the capability to resolve and include the right bits at runtime.""""""",,2.0
1,XD-3338,BUG,Done,MEDIUM,"""Do not include optional dependencies automatically via 'includes'""","""As a s-c-d developer, I'd like the 'includes' feature of the module launcher not to include optional dependencies, so that I can have better control over what gets added to the class path.""","""""""As a s-c-d developer, I'd like the 'includes' feature of the module launcher not to include optional dependencies, so that I can have better control over what gets added to the class path.""""""",,2.0
1,XD-3337,FEATURE,Done,MEDIUM,"""Spike: Investigate the inclusion of message bus binding libaries""","""As a s-c-d developer, I'd like to investigate how to include/exclude msg bus/binding jars, so I can decide the binding selection and fallback mechanism when there is none setup.""","""""""As a s-c-d developer, I'd like to investigate how to include/exclude msg bus/binding jars, so I can decide the binding selection and fallback mechanism when there is none setup.""""""",,5.0
1,XD-3335,BUG,Done,HIGH,"""Kafka Source must set autoStartup=false on KafkaMessageDrivenChannelAdapter""","""If the value is not set, the source may start before being bound to the bus, throwing a """"Dispatcher has no subscribers"""" error""","""""""If the value is not set, the source may start before being bound to the bus, throwing a """"""""Dispatcher has no subscribers"""""""" error""""""",,3.0
1,XD-3334,FEATURE,Done,MEDIUM,"""Refactor to use RestOperations""","""The current implementation makes use of cf-java-client, which is relatively heavy for our needs. It should be removed in favour of a bespoke RestOperations wrapper. See https://github.com/Zteve/test-cc-oauth for sample code.""","""""""The current implementation makes use of cf-java-client, which is relatively heavy for our needs. It should be removed in favour of a bespoke RestOperations wrapper. See https://github.com/Zteve/test-cc-oauth for sample code.""""""",,5.0
1,XD-3333,FEATURE,Done,MEDIUM,"""Refactor CloudFoundryApplicationFactory""","""This class should not know what the test app is. This means changing the constructors on CloudFoundryApplication.""","""""""This class should not know what the test app is. This means changing the constructors on CloudFoundryApplication.""""""",,1.0
1,XD-3332,FEATURE,Done,MEDIUM,"""Obtain username and password credentials for CloudFoundry""","""As part of moving to a bespoke RestOperations application we will need credentials to access CloudFoundry. These will need to be supplied from the new XD Admin app at runtime.""","""""""As part of moving to a bespoke RestOperations application we will need credentials to access CloudFoundry. These will need to be supplied from the new XD Admin app at runtime.""""""",,2.0
1,XD-3331,FEATURE,Done,MEDIUM,"""Add real ModuleRunner application""","""The current ModuleRunner is test app used for validation. This should be replaced by a real app.""","""""""The current ModuleRunner is test app used for validation. This should be replaced by a real app.""""""",,2.0
1,XD-3330,FEATURE,Done,MEDIUM,"""Implement undeploy operation for CC SPI""","""Currently undeploy is a no-op.""","""""""Currently undeploy is a no-op.""""""",,2.0
1,XD-3329,FEATURE,Done,MEDIUM,"""Return full ModuleInstanceStatus information""","""Currently there is no ModuleInstanceStatus returned. This issue will fill in the details.""","""""""Currently there is no ModuleInstanceStatus returned. This issue will fill in the details.""""""",,1.0
1,XD-3328,FEATURE,Done,MEDIUM,"""Return full ModuleStatus information""","""Remove all stubs and check all required information is returned accurately.""","""""""Remove all stubs and check all required information is returned accurately.""""""",,1.0
1,XD-3326,FEATURE,Done,MEDIUM,"""Add parameter information to application definition""","""A ModuleDefinition contains parameters, which need to be passed to the CloudFoundry application. Currently these are put directly into the application's environment. This issue will verify they are correctly named.""","""""""A ModuleDefinition contains parameters, which need to be passed to the CloudFoundry application. Currently these are put directly into the application's environment. This issue will verify they are correctly named.""""""",,1.0
1,XD-3325,FEATURE,Done,MEDIUM,"""Add binding information to application definition""","""ModuleDefinition contains bindings that need to be passed to the ModuleRunner app. It appears these can be included in the application's environment.""","""""""ModuleDefinition contains bindings that need to be passed to the ModuleRunner app. It appears these can be included in the application's environment.""""""",,2.0
1,XD-3322,FEATURE,Done,MEDIUM,"""Create CI infrastructure for s-c-s-m repo""","""As a s-c-s developer, I'd like to setup CI infrastructure for {{spring-cloud-stream-modules}} (s-c-s-m) repo, so I can build the project continuously on every commits. ""","""""""As a s-c-s developer, I'd like to setup CI infrastructure for {{spring-cloud-stream-modules}} (s-c-s-m) repo, so I can build the project continuously on every commits. """"""",,3.0
1,XD-3321,BUG,Done,MEDIUM,"""Make requirement for MD5 hash files configurable for the custom module registry ""","""Post 1.2 upgrade, the custom modules no longer show up by just copying the jars to the {{xd.customModule.home}} directory. Instead I have to use the 'module upload' command to install the modules. This is because an MD5 file is required. More details in [SO thread|http://stackoverflow.com/questions/31792220/spring-xd-1-2-0-custom-module-deployment]. ""","""""""Post 1.2 upgrade, the custom modules no longer show up by just copying the jars to the {{xd.customModule.home}} directory. Instead I have to use the 'module upload' command to install the modules. This is because an MD5 file is required. More details in [SO thread|http://stackoverflow.com/questions/31792220/spring-xd-1-2-0-custom-module-deployment]. """"""",,3.0
1,XD-3320,FEATURE,Done,MEDIUM,"""Migrate StreamController from XD 1.0""","""As a s-c-d user, I'd like to add REST support for stream commands, so I can maneuver streaming pipeline backed by StreamController.""","""""""As a s-c-d user, I'd like to add REST support for stream commands, so I can maneuver streaming pipeline backed by StreamController.""""""",,5.0
1,XD-3319,FEATURE,Done,MEDIUM,"""Add PHD HDFS as s-c-s module""","""As a s-c-s developer, I'd like to investigate the right approach to port {{PHD}} as the provider to support {{HDFS}} module from XD, so I can decide better handling of HDFS dependencies, which needs loaded and available in root CP at the runtime. ""","""""""As a s-c-s developer, I'd like to investigate the right approach to port {{PHD}} as the provider to support {{HDFS}} module from XD, so I can decide better handling of HDFS dependencies, which needs loaded and available in root CP at the runtime. """"""",,5.0
1,XD-3318,FEATURE,Done,MEDIUM,"""Bootify ModuleLauncher""","""As a s-c-s developer, I'd like to _bootify_ {{ModuleLauncher}}, so I can use Spring Boot's support for property, setting, as well as adding options and new functionality in the future, such as CP augmentation. ""","""""""As a s-c-s developer, I'd like to _bootify_ {{ModuleLauncher}}, so I can use Spring Boot's support for property, setting, as well as adding options and new functionality in the future, such as CP augmentation. """"""",,5.0
1,XD-3317,FEATURE,Done,MEDIUM,"""Add support to resolve and add JARs to Boot loader""","""As a s-c-d developer, I'd like to resolve and then add module dependent JAR's to Boot loader, so I have an approach to handle external libraries (ex: database drivers) required by OOTB modules. ""","""""""As a s-c-d developer, I'd like to resolve and then add module dependent JAR's to Boot loader, so I have an approach to handle external libraries (ex: database drivers) required by OOTB modules. """"""",,8.0
1,XD-3316,FEATURE,Done,MEDIUM,"""Create CI infrastructure for s-c-d repo""","""As a s-c-d developer, I'd like to setup CI infrastructure for [s-c-d repo|https://github.com/spring-cloud/spring-cloud-data], so I can build the project continuously on every commits. ""","""""""As a s-c-d developer, I'd like to setup CI infrastructure for [s-c-d repo|https://github.com/spring-cloud/spring-cloud-data], so I can build the project continuously on every commits. """"""",,5.0
1,XD-3315,FEATURE,Done,MEDIUM,"""Port Redis counter as s-c-s sink""","""As a s-c-s developer, I'd like to adapt redis {{counter}} from XD to s-c-s, so I can build streaming pipes using s-c-s modules with simple counters to feed dashboards. ""","""""""As a s-c-s developer, I'd like to adapt redis {{counter}} from XD to s-c-s, so I can build streaming pipes using s-c-s modules with simple counters to feed dashboards. """"""",,3.0
1,XD-3314,FEATURE,Done,MEDIUM,"""Validate stream commands from shell""","""As a s-c-d developer, I'd like to invoke REST APIs via shell, so I can validate {{StreamController}} operations.""","""""""As a s-c-d developer, I'd like to invoke REST APIs via shell, so I can validate {{StreamController}} operations.""""""",,8.0
1,XD-3313,FEATURE,Done,MEDIUM,"""Add in-memory stream definition repository""","""As a spring-cloud-data developer, I'd like to use an in-memory stream definition repository, so I don't have to spin up a store; obviously, this will not persist between application executions, but it will be useful for a simplified development experience. ""","""""""As a spring-cloud-data developer, I'd like to use an in-memory stream definition repository, so I don't have to spin up a store; obviously, this will not persist between application executions, but it will be useful for a simplified development experience. """"""",,5.0
1,XD-3312,FEATURE,Done,MEDIUM,"""Move spring-cloud-stream-modules to spring-cloud repo""","""As a s-c-s developer, I'd like to move {{spring-cloud-stream-modules}} from s-c-s to s-c repo, so I can cleanup s-c-s project and at the same time make these modules visible outside of s-c-s.""","""""""As a s-c-s developer, I'd like to move {{spring-cloud-stream-modules}} from s-c-s to s-c repo, so I can cleanup s-c-s project and at the same time make these modules visible outside of s-c-s.""""""",,3.0
1,XD-3311,FEATURE,Done,MEDIUM,"""Create ModuleRegistry stubs""","""As a s-c-d developer, I'd like to create {[ModuleRegistry}} stubs, so I can create mock streams by interacting with the registry APIs.""","""""""As a s-c-d developer, I'd like to create {[ModuleRegistry}} stubs, so I can create mock streams by interacting with the registry APIs.""""""",,3.0
1,XD-3310,FEATURE,Done,MEDIUM,"""Add REST support for spring-cloud-data""","""As a s-c-d developer, I'd like to establish the foundation to expose REST-APIs to interact with the {{xd-admin}} and likewise perform CRUD operations to maneuver streaming and batch pipelines. ""","""""""As a s-c-d developer, I'd like to establish the foundation to expose REST-APIs to interact with the {{xd-admin}} and likewise perform CRUD operations to maneuver streaming and batch pipelines. """"""",,5.0
1,XD-3309,FEATURE,Done,MEDIUM,"""Add direct binding option for s-c-s modules""","""As a s-c-s user, I'd like to have the option to direct bind _modules_, so I don't have to use messaging middleware and I can eliminate latency between them. This is important for high throughput and low latency use cases.   ""","""""""As a s-c-s user, I'd like to have the option to direct bind _modules_, so I don't have to use messaging middleware and I can eliminate latency between them. This is important for high throughput and low latency use cases.   """"""",,5.0
1,XD-3308,BUG,Done,MEDIUM,"""With Security - Unable to upload module""","""Once security is enabled, one cannot upload modules using the shell any longer.""","""""""Once security is enabled, one cannot upload modules using the shell any longer.""""""",,2.0
1,XD-3307,IMPROVEMENT,Done,HIGH,"""Add support for offline module resolution""","""h2.  Narrarive As a developer, I need to be able to test modules without pushing them to a remote maven repository.  I should be able to do {{$ mvn install}} in my module project locally (which will install the artifact into my local repository) and have it resolvable by spring-cloud-streams.""","""""""h2.  Narrarive As a developer, I need to be able to test modules without pushing them to a remote maven repository.  I should be able to do {{$ mvn install}} in my module project locally (which will install the artifact into my local repository) and have it resolvable by spring-cloud-streams.""""""",,5.0
1,XD-3306,BUG,Done,HIGH,"""[Flo] Some streams can't be created using FLO""","""Trying to create streams from the flo UI may end up in weird exceptions, whereas doing the same thing (copying/pasting the stream) directly from XD shell works smoothly.  This simple stream is an example, but this situation happens in multiple scenarios (for example using the same module several times with labels).   {code:java} trigger --cron='0 05 14 ? * MON-FRI' | mail --from='''<EMAIL>''' --to='''<EMAIL>''' --bcc='''<EMAIL>''' {code} ""","""""""Trying to create streams from the flo UI may end up in weird exceptions, whereas doing the same thing (copying/pasting the stream) directly from XD shell works smoothly.  This simple stream is an example, but this situation happens in multiple scenarios (for example using the same module several times with labels).    """"""",""" trigger --cron='0 05 14 ? * MON-FRI' | mail --from='''xd@mycompany.com''' --to='''a-wise-guy@mycompany.com''' --bcc='''me@mycompany.com''' """,0.0
1,XD-3303,IMPROVEMENT,Done,MEDIUM,"""Update 1.3 installation instructions""","""As a user, I'd like to refer to documentation while migrating to 1.3 release.""","""""""As a user, I'd like to refer to documentation while migrating to 1.3 release.""""""",,3.0
1,XD-3300,FEATURE,Done,HIGH,"""Spike: Determine best way to centrally configure the job repository for batch jobs.""","""h2. Narrative As a developer, I need to be able to run batch jobs that use the centrally configured job repository to store job state.  h2. Back story The XD containers each used a {{BatchConfigurer}} implementation ({{RuntimeBatchConfigurer}}) to add a consistent configuration for the job repository.  This functionality needs to be replicated in some way in just a regular Spring Boot application.""","""""""h2. Narrative As a developer, I need to be able to run batch jobs that use the centrally configured job repository to store job state.  h2. Back story The XD containers each used a {{BatchConfigurer}} implementation ({{RuntimeBatchConfigurer}}) to add a consistent configuration for the job repository.  This functionality needs to be replicated in some way in just a regular Spring Boot application.""""""",,5.0
1,XD-3298,FEATURE,Done,HIGH,"""Create basic TaskLauncher""","""h2. Narrative As Spring XD, I will be able to launch Spring Boot jar files as Diego Tasks.  h2. Back story The {{TaskLauncher}} will be responsible for listening for launch requests, looking up the definition in the {{TaskDescriptorRepository}}, and launching it.  The first implementation of this would be a Receptor based implementation. The scope here is to produce a _basic_ version of {{TaskLauncher}} and incrementally evolve into comprehensive launch capabilities.  *See:* https://docs.google.com/document/d/1q964adRCA-kJke_i0GBToJHLXJJTV_7TaTpQT0ymsbc/edit""","""""""h2. Narrative As Spring XD, I will be able to launch Spring Boot jar files as Diego Tasks.  h2. Back story The {{TaskLauncher}} will be responsible for listening for launch requests, looking up the definition in the {{TaskDescriptorRepository}}, and launching it.  The first implementation of this would be a Receptor based implementation. The scope here is to produce a _basic_ version of {{TaskLauncher}} and incrementally evolve into comprehensive launch capabilities.  *See:* https://docs.google.com/document/d/1q964adRCA-kJke_i0GBToJHLXJJTV_7TaTpQT0ymsbc/edit""""""",,5.0
1,XD-3296,FEATURE,Done,HIGH,"""Spike: Design a tasks repository""","""h2. Narrative As a developer, I'd like to be able to run a boot jar as a task on CF and obtain the result reliably.  h2. Back story Currently Lattice/Diego's tasks implementation provides the ability to run things as short lived tasks.  However, obtaining the result of said task can be an issue.  There are two ways to do so:  # Poll for the result. # Register a callback URL to be called once the task completes.  Since a task is only available for a short time after its completion before it is deleted, polling can run the risk of missing the result completely.  When you consider the fact that the provided GUIDs that identify tasks can be re-used polling becomes a precarious option.  Registering a callback URL would be a better option, however there are no good guarantees that the message will be delivered.  The service will try to execute the callback until it's successful or the task is cleaned up.  """"Successful"""" is defined in this case as anything other than a 502 or a 503 return code.  In order for Spring XD to be able to support Diego tasks, a more durable option for maintaining the result of tasks will need to be developed.  *Note:* The outcome of this spike may be feature requests for the CF/Diego team.""","""""""h2. Narrative As a developer, I'd like to be able to run a boot jar as a task on CF and obtain the result reliably.  h2. Back story Currently Lattice/Diego's tasks implementation provides the ability to run things as short lived tasks.  However, obtaining the result of said task can be an issue.  There are two ways to do so:  # Poll for the result. # Register a callback URL to be called once the task completes.  Since a task is only available for a short time after its completion before it is deleted, polling can run the risk of missing the result completely.  When you consider the fact that the provided GUIDs that identify tasks can be re-used polling becomes a precarious option.  Registering a callback URL would be a better option, however there are no good guarantees that the message will be delivered.  The service will try to execute the callback until it's successful or the task is cleaned up.  """"""""Successful"""""""" is defined in this case as anything other than a 502 or a 503 return code.  In order for Spring XD to be able to support Diego tasks, a more durable option for maintaining the result of tasks will need to be developed.  *Note:* The outcome of this spike may be feature requests for the CF/Diego team.""""""",,8.0
1,XD-3295,FEATURE,Done,HIGH,"""Spike: Determine options for configuring shared module dependencies""","""h2. Narrative As a developer, I'd like to be able to configure common dependencies for the entire environment.  An example could be that I use MySql for my databases.  I want to be able to configure the MySql driver once and have all modules use it.  h2. Back story Spring Batch uses a database to store job state (the job repository).  This is a shared resource across all jobs (both custom developed and OOTB).  In order to support OOTB jobs, we'll need to have a way for users to provide the db driver to each module.  Ideally this would be possible without requiring that each of our OOTB modules be repackaged. ""","""""""h2. Narrative As a developer, I'd like to be able to configure common dependencies for the entire environment.  An example could be that I use MySql for my databases.  I want to be able to configure the MySql driver once and have all modules use it.  h2. Back story Spring Batch uses a database to store job state (the job repository).  This is a shared resource across all jobs (both custom developed and OOTB).  In order to support OOTB jobs, we'll need to have a way for users to provide the db driver to each module.  Ideally this would be possible without requiring that each of our OOTB modules be repackaged. """"""",,8.0
1,XD-3288,FEATURE,Done,MEDIUM,"""Add CI workflow to build, bundle and upload module-launcher image to DockerHub""","""As a s-c-s developer, I'd like to setup a CI workflow to build, bundle and upload the {{module-launcher}} image to DockerHub, so I don't have to worry about having a local-private docker registry for development/testing.  It could be nice to have the image uploaded to existing [spring-cloud|https://registry.hub.docker.com/repos/springcloud/] DockerHub location. ""","""""""As a s-c-s developer, I'd like to setup a CI workflow to build, bundle and upload the {{module-launcher}} image to DockerHub, so I don't have to worry about having a local-private docker registry for development/testing.  It could be nice to have the image uploaded to existing [spring-cloud|https://registry.hub.docker.com/repos/springcloud/] DockerHub location. """"""",,5.0
1,XD-3287,BUG,Done,HIGH,"""Add HA support for NameNode when installed using Ambari""","""As a user, I'm trying to setup HA cluster using Ambari installed Spring XD; however, I'm running into issues with the overrides. More details [here|https://github.com/spring-projects/spring-xd-ambari/issues/6].""","""""""As a user, I'm trying to setup HA cluster using Ambari installed Spring XD; however, I'm running into issues with the overrides. More details [here|https://github.com/spring-projects/spring-xd-ambari/issues/6].""""""",,5.0
1,XD-3283,FEATURE,Done,MEDIUM,"""Port FTP as s-c-s source module""","""As a Spring XD developer, I'd like to port {{FTP}} modules from XD to s-c-s repo, so I can use them as {{source}} modules to build streaming pipeline.""","""""""As a Spring XD developer, I'd like to port {{FTP}} modules from XD to s-c-s repo, so I can use them as {{source}} modules to build streaming pipeline.""""""",,1.0
1,XD-3282,FEATURE,Done,MEDIUM,"""Create CI infrastructure for s-c-s""","""As a s-c-s developer, I'd like to setup CI builds for s-c-s builds, so I can incrementally build and test code commits automatically.""","""""""As a s-c-s developer, I'd like to setup CI builds for s-c-s builds, so I can incrementally build and test code commits automatically.""""""",,3.0
1,XD-3281,FEATURE,Done,MEDIUM,"""Self-register xd-admin server with Eureka""","""As a Spring XD developer, I'd like to self-register {{xd-admin}} server with {{Eureka}}, so I could have admin server exposed as discoverable endpoint. ""","""""""As a Spring XD developer, I'd like to self-register {{xd-admin}} server with {{Eureka}}, so I could have admin server exposed as discoverable endpoint. """"""",,3.0
1,XD-3277,FEATURE,Done,MEDIUM,"""Replace controller calls with respective SPI implementation""","""As a Spring XD developer, I'd like to refactor current controller with SPI calls, so I can invoke the respective Admin SPI implementation based on the deployment.   *Controllers to Refactor* * ContainersController * StreamsController * ModulesController * JobsController ""","""""""As a Spring XD developer, I'd like to refactor current controller with SPI calls, so I can invoke the respective Admin SPI implementation based on the deployment.   *Controllers to Refactor* * ContainersController * StreamsController * ModulesController * JobsController """"""",,5.0
1,XD-3276,FEATURE,Done,MEDIUM,"""Add state to Eureka when deploying s-c-s modules""","""As a s-c-s user, I'd like to have my modules add/update it's current state to Eureka, so I can use the repository to discover the current sate of the module as needed.  ""","""""""As a s-c-s user, I'd like to have my modules add/update it's current state to Eureka, so I can use the repository to discover the current sate of the module as needed.  """"""",,2.0
1,XD-3275,FEATURE,Done,MEDIUM,"""Add support to store metadata in Eureka""","""As a s-c-s user, I'd like to store module metadata in {{Eureka}}, so I can use the repository to determine the current state.""","""""""As a s-c-s user, I'd like to store module metadata in {{Eureka}}, so I can use the repository to determine the current state.""""""",,2.0
1,XD-3274,FEATURE,Done,MEDIUM,"""Make s-c-s modules searchable by it's name""","""As a s-c-s user, I'd like to search the modules by it's name aside from the default {{spring.application.name}} offered by boot, so I can also fetch modules by it's name.""","""""""As a s-c-s user, I'd like to search the modules by it's name aside from the default {{spring.application.name}} offered by boot, so I can also fetch modules by it's name.""""""",,2.0
1,XD-3273,FEATURE,Done,MEDIUM,"""Add support for modules to register itself to Eureka""","""As a s-c-s user, I'd like to have the modules self-register itself with {{Eureka}} whenever they're installed, so I can also discover the same modules using Spring XD Admin SPI and reuse them to create data pipelines. ""","""""""As a s-c-s user, I'd like to have the modules self-register itself with {{Eureka}} whenever they're installed, so I can also discover the same modules using Spring XD Admin SPI and reuse them to create data pipelines. """"""",,5.0
1,XD-3271,FEATURE,Done,MEDIUM,"""Add automatic wiring of profile-driven SPI implementations""","""As a Spring XD user, I'd like to make SPI implementation profile aware, so I can run {{java -jar admin}} or {{cf push}} admin or {{ltc create admin}} and the corresponding implementation gets wired-in automatically.""","""""""As a Spring XD user, I'd like to make SPI implementation profile aware, so I can run {{java -jar admin}} or {{cf push}} admin or {{ltc create admin}} and the corresponding implementation gets wired-in automatically.""""""",,3.0
1,XD-3270,FEATURE,Done,MEDIUM,"""Create module registry abstraction""","""As a Spring XD developer, I'd like to create initial version of the new module registry abstraction, so we could leverage the foundation to make progress and test the respective SPI ({{receptor}} or {{cloudcontroller}}) implementations.""","""""""As a Spring XD developer, I'd like to create initial version of the new module registry abstraction, so we could leverage the foundation to make progress and test the respective SPI ({{receptor}} or {{cloudcontroller}}) implementations.""""""",,5.0
1,XD-3269,FEATURE,Done,MEDIUM,"""Find a permanent home for SPI""","""As a Spring XD developer, I'd like to have a permanent location of SPI implementations, so I could use the common repo every time I contribute or enhance the test coverage. ""","""""""As a Spring XD developer, I'd like to have a permanent location of SPI implementations, so I could use the common repo every time I contribute or enhance the test coverage. """"""",,3.0
1,XD-3267,FEATURE,Done,MEDIUM,"""Add support for Receptor SPI to query module status""","""As a Spring XD on CF user, I'd like to use Receptor implementation of Admin SPI every time I deploy Spring XD modules, so I can leverage the SPI to query for module status and health metrics.  *Possible APIs:* {code}  ModuleStatus getStatus(ModuleDescriptor descriptor);  Collection<ModuleDescriptor> listModules();  Map<ModuleDescriptor.Key, ModuleStatus>  {code}""","""""""As a Spring XD on CF user, I'd like to use Receptor implementation of Admin SPI every time I deploy Spring XD modules, so I can leverage the SPI to query for module status and health metrics.  *Possible APIs:* """"""","""  ModuleStatus getStatus(ModuleDescriptor descriptor);  Collection<ModuleDescriptor> listModules();  Map<ModuleDescriptor.Key, ModuleStatus>  """,5.0
1,XD-3266,BUG,Done,MEDIUM,"""No pagination for Jobs / Deployments page in Admin UI""","""After successfully deploying 12 jobs the Jobs / Deployments page still shows only 10 results.  It looks like {{http://localhost:9393/jobs/configurations.json?page=0&size=10}} always returns {{content.page.totalPages}} of 1 regardless of the {{size}} parameter.""","""""""After successfully deploying 12 jobs the Jobs / Deployments page still shows only 10 results.  It looks like {{http://localhost:9393/jobs/configurations.json?page=0&size=10}} always returns {{content.page.totalPages}} of 1 regardless of the {{size}} parameter.""""""",,2.0
1,XD-3265,FEATURE,Done,MEDIUM,"""Spike: Investigate distributed implementation of CloudController Admin SPI""","""As a Spring XD user, I'd like to use {{CloudController}} based implementation of XD Admin SPI (based on ModuleLauncher), so I can run data pipeline use-cases running on CF.  Relevant repos: [spring-cloud-data|https://github.com/spring-cloud/spring-cloud-data/tree/master/spring-cloud-data-module-deployers] | [spring-cloud-stream|https://github.com/spring-cloud/spring-cloud-stream]  Please refer to XD-3194 or XD-3229 as sample spike-deliverables (_google doc_) that were completed in the last sprint. ""","""""""As a Spring XD user, I'd like to use {{CloudController}} based implementation of XD Admin SPI (based on ModuleLauncher), so I can run data pipeline use-cases running on CF.  Relevant repos: [spring-cloud-data|https://github.com/spring-cloud/spring-cloud-data/tree/master/spring-cloud-data-module-deployers] | [spring-cloud-stream|https://github.com/spring-cloud/spring-cloud-stream]  Please refer to XD-3194 or XD-3229 as sample spike-deliverables (_google doc_) that were completed in the last sprint. """"""",,8.0
1,XD-3263,BUG,Done,MEDIUM,"""Pagination for containers, it is limited to only 20""","""Hi ,  Customer has 48 containers, but it only shows 20 containers. We need pagination to browse all containers.""","""""""Hi ,  Customer has 48 containers, but it only shows 20 containers. We need pagination to browse all containers.""""""",,2.0
1,XD-3262,FEATURE,Done,MEDIUM,"""UI: Add Pagination to Containers Page ""","""Add Pagination to Containers Page""","""""""Add Pagination to Containers Page""""""",,2.0
1,XD-3261,FEATURE,Done,URGENT,"""Update Groovy to 2.4.4""","""There is a vulnerability in Groovy that is fixed in 2.4.4:  CVE-2015-3253: Remote execution of untrusted code  See:  http://groovy-lang.org/security.html  http://mail-archives.apache.org/mod_mbox/incubator-groovy-users/201507.mbox/%3CCADQzvmmYC7RbZnsQ8O63XN4HCMYh9RGRdMiuWupVt=u=pjH8+<EMAIL>%3E   ""","""""""There is a vulnerability in Groovy that is fixed in 2.4.4:  CVE-2015-3253: Remote execution of untrusted code  See:  http://groovy-lang.org/security.html  http://mail-archives.apache.org/mod_mbox/incubator-groovy-users/201507.mbox/%3CCADQzvmmYC7RbZnsQ8O63XN4HCMYh9RGRdMiuWupVt=u=pjH8+<EMAIL>%3E   """"""",,1.0
1,XD-3260,FEATURE,Done,MEDIUM,"""Move serialization codec from XD to Spring Integration""","""As a developer, I'd like to move 'serialization codec' from Spring XD repo into SI, so I can update Spring XD to inherit the features/functionalities via maven dependency.""","""""""As a developer, I'd like to move 'serialization codec' from Spring XD repo into SI, so I can update Spring XD to inherit the features/functionalities via maven dependency.""""""",,2.0
1,XD-3259,FEATURE,Done,MEDIUM,"""[SCS] Rename xd.messagebus binder properties ""","""replace with xd.messagebus prefix with spring.cloud.stream.binder""","""""""replace with xd.messagebus prefix with spring.cloud.stream.binder""""""",,3.0
1,XD-3258,FEATURE,Done,MEDIUM,"""Add jars for Avro and Snappy compression to Sqoop job submission""","""We need to have some jars as part of the Sqoop job submission to YARN:  for Avro we need:   avro-1.7.6.jar   avro-mapred-1.7.6.jar  for Snappy we need:   snappy-java-1.0.5.jar (note: the 1.1.0.1 version from xd/lib doesn't work)   commons-compress-1.4.1.jar  We can either have these included using the --libjars option or automatically include them. ""","""""""We need to have some jars as part of the Sqoop job submission to YARN:  for Avro we need:   avro-1.7.6.jar   avro-mapred-1.7.6.jar  for Snappy we need:   snappy-java-1.0.5.jar (note: the 1.1.0.1 version from xd/lib doesn't work)   commons-compress-1.4.1.jar  We can either have these included using the --libjars option or automatically include them. """"""",,3.0
1,XD-3257,BUG,Done,HIGH,"""xd-shell doesn't work with SSL proxy""","""After connecting to admin endpoint over HTTPS xd-shell will use plain HTTP for all further calls, which fails if HTTPS is mandatory:  {code} server-unknown:>admin config server https://my-host/ Successfully targeted https://my-host/ xd:>job list Command failed org.springframework.web.client.ResourceAccessException: I/O error on GET request for """"http://my-host/jobs/definitions?size=10000&deployments=true"""":Connection refused; nested exception is java.net.ConnectException: Connection refused {code}  In above example {{my-host}} is a proxy that provides load balancing and enforces HTTPS for users. It forwards the traffic to to spring-xd-admin service REST API over regular HTTP and port 9393. Admin service is returning {{_links}} with plain HTTP because it's unaware of the network proxy.  Can this be solved without employing server side HTTP->HTTPS url rewrite? Is enabling SSL in spring-xd-admin the only way?""","""""""After connecting to admin endpoint over HTTPS xd-shell will use plain HTTP for all further calls, which fails if HTTPS is mandatory:    In above example {{my-host}} is a proxy that provides load balancing and enforces HTTPS for users. It forwards the traffic to to spring-xd-admin service REST API over regular HTTP and port 9393. Admin service is returning {{_links}} with plain HTTP because it's unaware of the network proxy.  Can this be solved without employing server side HTTP->HTTPS url rewrite? Is enabling SSL in spring-xd-admin the only way?""""""",""" server-unknown:>admin config server https://my-host/ Successfully targeted https://my-host/ xd:>job list Command failed org.springframework.web.client.ResourceAccessException: I/O error on GET request for """"""""http://my-host/jobs/definitions?size=10000&deployments=true"""""""":Connection refused; nested exception is java.net.ConnectException: Connection refused """,2.0
1,XD-3256,FEATURE,Done,MEDIUM,"""Spike: Investigate installation of XD modules from maven repo""","""As a developer, I'd like to brainstorm and investigate various techniques around installation of XD modules from a maven repo, so I could define the module {{artifactId}} from CLI to have the module downloaded from the repo and installed to a running Spring XD runtime.""","""""""As a developer, I'd like to brainstorm and investigate various techniques around installation of XD modules from a maven repo, so I could define the module {{artifactId}} from CLI to have the module downloaded from the repo and installed to a running Spring XD runtime.""""""",,8.0
1,XD-3255,MAINTENANCE,Done,MEDIUM,"""SCS - split kryo implementation to spring-cloud-streams-codec-kryo""","""SCS - split kryo implementation to spring-cloud-streams-codec-kryo""","""SCS - split kryo implementation to spring-cloud-streams-codec-kryo""",,1.0
1,XD-3254,MAINTENANCE,Done,MEDIUM,"""SCS-  Rename codec packages""","""SCS-  Rename codec packages""","""SCS-  Rename codec packages""",,1.0
1,XD-3250,FEATURE,Done,MEDIUM,"""Refactor s-c-s samples to use @ConfigurationProperties""","""The spring-cloud-streams samples have module options classes copied over from XD. They should use a pure @ConfigurationProperties approach, making sure metadata is generated/hand written as appropriate.  @Mixins are still referenced there but obviously can't work, so provide an equivalent""","""""""The spring-cloud-streams samples have module options classes copied over from XD. They should use a pure @ConfigurationProperties approach, making sure metadata is generated/hand written as appropriate.  @Mixins are still referenced there but obviously can't work, so provide an equivalent""""""",,3.0
1,XD-3249,FEATURE,Done,MEDIUM,"""Change module option type from Class to String""","""This better aligns with boot. Moreover, using Class was a bad design choice (one can always get a Class from a String [modulo knowing which CL to use], while to converse is not always easy [CL not being available]) ""","""""""This better aligns with boot. Moreover, using Class was a bad design choice (one can always get a Class from a String [modulo knowing which CL to use], while to converse is not always easy [CL not being available]) """"""",,3.0
1,XD-3245,MAINTENANCE,Done,MEDIUM,"""Replace spring-xd-messagebus-* dependencies with SCS""","""XD 2.0 will not have direct dependency on the s-c-s Binder (as MB has been renamed).  The message bus code is obsolete/orphaned in XD 2.0 but some is required to support current integration tests. We can look at pruning it some more but complete removal likely depends on integrating the s-c-s enabled Admin SPI.  MB will remain in XD 1.x.""","""""""XD 2.0 will not have direct dependency on the s-c-s Binder (as MB has been renamed).  The message bus code is obsolete/orphaned in XD 2.0 but some is required to support current integration tests. We can look at pruning it some more but complete removal likely depends on integrating the s-c-s enabled Admin SPI.  MB will remain in XD 1.x.""""""",,3.0
1,XD-3244,MAINTENANCE,Done,MEDIUM,"""Copy spring-xd-messagebus-* to SCS as spring-cloud-binding-*""","""Copy spring-xd-messagebus-* to SCS as spring-cloud-binding-*""","""Copy spring-xd-messagebus-* to SCS as spring-cloud-binding-*""",,2.0
1,XD-3242,MAINTENANCE,Done,MEDIUM,"""Copy spring-xd-codec to SCS as spring-cloud-streams-codec""","""Create the equivalent library in spring-cloud-streams""","""""""Create the equivalent library in spring-cloud-streams""""""",,2.0
1,XD-3241,FEATURE,Done,MEDIUM,"""Add support for update in gpfdist sink""","""Currently we can only do plain inserts, should follow same logic from native gpfdist sink and add upserts.""","""""""Currently we can only do plain inserts, should follow same logic from native gpfdist sink and add upserts.""""""",,1.0
1,XD-3240,FEATURE,Done,MEDIUM,"""Add better support for using control file with gpfdist""","""Currently only database connection info can be read from a control file yml format. Should add rest of the missing options to align how native format works.""","""""""Currently only database connection info can be read from a control file yml format. Should add rest of the missing options to align how native format works.""""""",,2.0
1,XD-3239,FEATURE,Done,MEDIUM,"""Move 1.2.x branch to EC2 CI infrastructure""","""As a developer, I'd like to move 1.2.x branch to EC2 infrastructure, so I can reliably run CI test suites.""","""""""As a developer, I'd like to move 1.2.x branch to EC2 infrastructure, so I can reliably run CI test suites.""""""",,2.0
1,XD-3238,FEATURE,Done,MEDIUM,"""Complete remaining Kryo optimization changes""","""As a developer, I'd like to complete the remaining Kryo optimization changes, so I can polish and get the guidelines documented appropriately. ""","""""""As a developer, I'd like to complete the remaining Kryo optimization changes, so I can polish and get the guidelines documented appropriately. """"""",,1.0
1,XD-3235,BUG,Done,MEDIUM,"""FileJdbc Job throws exception during Acceptance Tests ""","""Currently the testFileJdbcJobMultipleInvocations fails on line 156 stating data is different in table that what is expected.  Currently this is failing on the single admin/container deployment using redis as a transport.    Also seeing the following exception in the attached log: {noformat} 2.0.0.SNAP ERROR xdbus.job:ec2Job3.0.requests-1 step.AbstractStep - Encountered an error executing step step1 in job ec2Job3 org.springframework.batch.item.ItemStreamException: Failed to initialize the reader ... Caused by: java.lang.IllegalStateException: Input resource must exist (reader is in 'strict' mode): URL [file:/tmp/xd/output/filejdbctest/filejdbctest1.out] {noformat} The file is should be present and data present for the test.  At least according to the checker on EC2 and local deployments.    ""","""""""Currently the testFileJdbcJobMultipleInvocations fails on line 156 stating data is different in table that what is expected.  Currently this is failing on the single admin/container deployment using redis as a transport.    Also seeing the following exception in the attached log:  The file is should be present and data present for the test.  At least according to the checker on EC2 and local deployments.    """"""",""" 2.0.0.SNAP ERROR xdbus.job:ec2Job3.0.requests-1 step.AbstractStep - Encountered an error executing step step1 in job ec2Job3 org.springframework.batch.item.ItemStreamException: Failed to initialize the reader ... Caused by: java.lang.IllegalStateException: Input resource must exist (reader is in 'strict' mode): URL [file:/tmp/xd/output/filejdbctest/filejdbctest1.out] """,3.0
1,XD-3234,FEATURE,Done,MEDIUM,"""Remove XML REST Endpoints""","""The XML REST endpoints:  * are not working correctly * interfere with security * are not used   ""","""""""The XML REST endpoints:  * are not working correctly * interfere with security * are not used   """"""",,3.0
1,XD-3233,FEATURE,Done,MEDIUM,"""Enable component model for spring-cloud-streams""","""As a developer, I'd like to create an annotation ({{@EnableModule}}) driven programming model for modules, so instead of explicitly defining I/O channels as beans on the module, for classes annotated with {{@EnableModule}}, the application would be responsible for creating the actual channel beans and channel adapters vs. the developer creating concrete channel instance types.  The {{@Input}} and {{@Output}} annotations will be used to indicate the input and output channels of the module. ""","""""""As a developer, I'd like to create an annotation ({{@EnableModule}}) driven programming model for modules, so instead of explicitly defining I/O channels as beans on the module, for classes annotated with {{@EnableModule}}, the application would be responsible for creating the actual channel beans and channel adapters vs. the developer creating concrete channel instance types.  The {{@Input}} and {{@Output}} annotations will be used to indicate the input and output channels of the module. """"""",,8.0
1,XD-3232,FEATURE,Done,MEDIUM,"""Update Spring Integration to 4.2.0.M2 (4.1.6 on 1.2.x)""","""Also Spring Framework 4.2.0.RC2, Spring AMQP 1.5.0.M1  Also Batch 3.0.4""","""""""Also Spring Framework 4.2.0.RC2, Spring AMQP 1.5.0.M1  Also Batch 3.0.4""""""",,1.0
1,XD-3231,FEATURE,Done,MEDIUM,"""Add support to Ambari install multiple XD Admin's ""","""As a developer, I'd like to update Ambari installed Spring XD cluster to spin-up multiple instances of XD-Admin servers, so it is setup for HA. ""","""""""As a developer, I'd like to update Ambari installed Spring XD cluster to spin-up multiple instances of XD-Admin servers, so it is setup for HA. """"""",,5.0
1,XD-3230,FEATURE,Done,MEDIUM,"""Update to Reactor 2.0.4""","""As a developer, I'd like to upgrade to Reactor 2.0.4 release, so I could leverage the latest improvements and bug-fixes.""","""""""As a developer, I'd like to upgrade to Reactor 2.0.4 release, so I could leverage the latest improvements and bug-fixes.""""""",,1.0
1,XD-3229,FEATURE,Done,MEDIUM,"""Spike: XD Admin SPI to discover s-c-s modules""","""As a s-c-s user, I'd like to investigate the possibility of s-c-s modules self-registering themselves to service discovery, so I could use Spring XD runtime (running on CF) to discover and orchestrate such modules through streams.""","""""""As a s-c-s user, I'd like to investigate the possibility of s-c-s modules self-registering themselves to service discovery, so I could use Spring XD runtime (running on CF) to discover and orchestrate such modules through streams.""""""",,8.0
1,XD-3228,FEATURE,Done,MEDIUM,"""Spike: Kickoff distributed Receptor implementation of Admin SPI""","""As a Spring XD user, I'd like to use Diego based Receptor implementation of XD Admin SPI (based on ModuleLauncher), so I can run data pipeline use-cases running on CF Lattice/Diego.  ""","""""""As a Spring XD user, I'd like to use Diego based Receptor implementation of XD Admin SPI (based on ModuleLauncher), so I can run data pipeline use-cases running on CF Lattice/Diego.  """"""",,8.0
1,XD-3227,FEATURE,Done,MEDIUM,"""Spike: Kickoff singlenode implementation of Admin SPI""","""As a developer, I'd like to develop a singlenode (in a single JVM) implementation of XD Admin SPI (based on Module Launcher), so I can run data pipeline use-cases locally. ""","""""""As a developer, I'd like to develop a singlenode (in a single JVM) implementation of XD Admin SPI (based on Module Launcher), so I can run data pipeline use-cases locally. """"""",,8.0
1,XD-3226,FEATURE,Done,MEDIUM,"""Move serialization codec from XD to spring-cloud-stream [Phase #1]""","""As a developer, I'd like to move 'serialization codec' from Spring XD repo into spring-bus, so I can update Spring XD to inherit the features/functionalities via maven dependency.""","""""""As a developer, I'd like to move 'serialization codec' from Spring XD repo into spring-bus, so I can update Spring XD to inherit the features/functionalities via maven dependency.""""""",,8.0
1,XD-3225,FEATURE,Done,MEDIUM,"""Move input/output type-conversion from XD to spring-cloud-stream""","""As a developer, I'd like to move input/output type conversion from Spring XD repo to spring-cloud-dataflow, so I can implement a custom module which produces or consumes a custom domain object.""","""""""As a developer, I'd like to move input/output type conversion from Spring XD repo to spring-cloud-dataflow, so I can implement a custom module which produces or consumes a custom domain object.""""""",,8.0
1,XD-3224,FEATURE,Done,MEDIUM,"""Move message-bus implemenation from XD to spring-cloud-streams [Phase #1]""","""As a developer, I'd like to move message-bus from Spring XD repo into spring-bus, so I can update Spring XD to inherit the features/functionalities via maven dependency.  ""","""""""As a developer, I'd like to move message-bus from Spring XD repo into spring-bus, so I can update Spring XD to inherit the features/functionalities via maven dependency.  """"""",,8.0
1,XD-3222,FEATURE,Done,MEDIUM,"""Find a way to connect Sqoop job to Teradata""","""As a user I would like to connect the Sqoop batch job to Teradata for import jobs.   I have tried the Teradata JDBC driver directly using:  {code}job create tdTest --definition """"sqoop --command=import --args='--table Frequent_Flyers --connect jdbc:teradata://tdexpress/DATABASE=transportation --driver com.teradata.jdbc.TeraDriver --username dbc --password dbc --target-dir=/xd/teradata --num-mappers 1'"""" {code}  but that results in an NPE.  The only way so far is to use the Hortonworks Connector for Teradata - http://public-repo-1.hortonworks.com/HDP/tools/2.2.4.2/hdp-connector-for-teradata-1.3.4.2.2.4.2-2-distro.tar.gz  That one allows me to use the following:  {code}job create tdTest --definition """"sqoop --command=import --args='--table Frequent_Flyers --connect jdbc:teradata://tdexpress/DATABASE=transportation --connection-manager org.apache.sqoop.teradata.TeradataConnManager --username dbc --password dbc --target-dir=/xd/teradata --num-mappers 1'"""" {code}""","""""""As a user I would like to connect the Sqoop batch job to Teradata for import jobs.   I have tried the Teradata JDBC driver directly using:    but that results in an NPE.  The only way so far is to use the Hortonworks Connector for Teradata - http://public-repo-1.hortonworks.com/HDP/tools/2.2.4.2/hdp-connector-for-teradata-1.3.4.2.2.4.2-2-distro.tar.gz  That one allows me to use the following:  """"""","""job create tdTest --definition """"""""sqoop --command=import --args='--table Frequent_Flyers --connect jdbc:teradata://tdexpress/DATABASE=transportation --driver com.teradata.jdbc.TeraDriver --username dbc --password dbc --target-dir=/xd/teradata --num-mappers 1'"""""""" job create tdTest --definition """"""""sqoop --command=import --args='--table Frequent_Flyers --connect jdbc:teradata://tdexpress/DATABASE=transportation --connection-manager org.apache.sqoop.teradata.TeradataConnManager --username dbc --password dbc --target-dir=/xd/teradata --num-mappers 1'"""""""" """,3.0
1,XD-3221,BUG,Done,URGENT,"""Enabling security breaks xd-shell""","""After enabling security in {{XD_HOME/config/servers.yml}}  {code} spring:   profiles: admin security:   basic:     enabled: true # false to disable security settings (default)     realm: SpringXD   user: # valid only if security.basic.enabled=true     name: johndoe     password: johndoe     role: ADMIN, VIEW, CREATE {code}  It's no longer possible to connect to admin server through xd-shell  {code} server-unknown:>admin config server --username johndoe --password johndoe Unable to contact XD Admin Server at 'http://localhost:9393/'. server-unknown:>admin config info   -------------  --------------------------------------------------------------   Credentials    [username='johndoe, password=****']   Result         Unable to contact XD Admin Server at 'http://localhost:9393/'.   Target         http://localhost:9393/   Timezone used  Greenwich Mean Time (UTC 0:00)   -------------  -------------------------------------------------------------- ------------------------------------------------------------------------------- An exception ocurred during targeting: org.springframework.web.client.HttpClientErrorException: 403 Forbidden     at org.springframework.web.client.DefaultResponseErrorHandler.handleError(DefaultResponseErrorHandler.java:91)     at org.springframework.xd.rest.client.impl.VndErrorResponseErrorHandler.handleError(VndErrorResponseErrorHandler.java:50)     at org.springframework.web.client.RestTemplate.handleResponse(RestTemplate.java:614)     at org.springframework.web.client.RestTemplate.doExecute(RestTemplate.java:570)     at org.springframework.web.client.RestTemplate.execute(RestTemplate.java:545)     at org.springframework.web.client.RestTemplate.getForObject(RestTemplate.java:253)     at org.springframework.xd.rest.client.impl.SpringXDTemplate.<init>(SpringXDTemplate.java:114)     at org.springframework.xd.rest.client.impl.SpringXDTemplate.<init>(SpringXDTemplate.java:102)     at org.springframework.xd.shell.command.ConfigCommands.target(ConfigCommands.java:112)     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)     at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)     at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)     at java.lang.reflect.Method.invoke(Method.java:606)     at org.springframework.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:202)     at org.springframework.shell.core.SimpleExecutionStrategy.invoke(SimpleExecutionStrategy.java:64)     at org.springframework.shell.core.SimpleExecutionStrategy.execute(SimpleExecutionStrategy.java:57)     at org.springframework.shell.core.AbstractShell.executeCommand(AbstractShell.java:127)     at org.springframework.shell.core.JLineShell.promptLoop(JLineShell.java:533)     at org.springframework.shell.core.JLineShell.run(JLineShell.java:179)     at java.lang.Thread.run(Thread.java:745) {code}  It looks like admin base URL (http://localhost:9393/) is not defined in security section of {{application.yml}} so {{SpringXDTemplate}} can't be initialized.""","""""""After enabling security in {{XD_HOME/config/servers.yml}}    It's no longer possible to connect to admin server through xd-shell    It looks like admin base URL (http://localhost:9393/) is not defined in security section of {{application.yml}} so {{SpringXDTemplate}} can't be initialized.""""""",""" spring:   profiles: admin security:   basic:     enabled: true # false to disable security settings (default)     realm: SpringXD   user: # valid only if security.basic.enabled=true     name: johndoe     password: johndoe     role: ADMIN, VIEW, CREATE  server-unknown:>admin config server --username johndoe --password johndoe Unable to contact XD Admin Server at 'http://localhost:9393/'. server-unknown:>admin config info   -------------  --------------------------------------------------------------   Credentials    [username='johndoe, password=****']   Result         Unable to contact XD Admin Server at 'http://localhost:9393/'.   Target         http://localhost:9393/   Timezone used  Greenwich Mean Time (UTC 0:00)   -------------  -------------------------------------------------------------- ------------------------------------------------------------------------------- An exception ocurred during targeting: org.springframework.web.client.HttpClientErrorException: 403 Forbidden     at org.springframework.web.client.DefaultResponseErrorHandler.handleError(DefaultResponseErrorHandler.java:91)     at org.springframework.xd.rest.client.impl.VndErrorResponseErrorHandler.handleError(VndErrorResponseErrorHandler.java:50)     at org.springframework.web.client.RestTemplate.handleResponse(RestTemplate.java:614)     at org.springframework.web.client.RestTemplate.doExecute(RestTemplate.java:570)     at org.springframework.web.client.RestTemplate.execute(RestTemplate.java:545)     at org.springframework.web.client.RestTemplate.getForObject(RestTemplate.java:253)     at org.springframework.xd.rest.client.impl.SpringXDTemplate.<init>(SpringXDTemplate.java:114)     at org.springframework.xd.rest.client.impl.SpringXDTemplate.<init>(SpringXDTemplate.java:102)     at org.springframework.xd.shell.command.ConfigCommands.target(ConfigCommands.java:112)     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)     at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)     at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)     at java.lang.reflect.Method.invoke(Method.java:606)     at org.springframework.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:202)     at org.springframework.shell.core.SimpleExecutionStrategy.invoke(SimpleExecutionStrategy.java:64)     at org.springframework.shell.core.SimpleExecutionStrategy.execute(SimpleExecutionStrategy.java:57)     at org.springframework.shell.core.AbstractShell.executeCommand(AbstractShell.java:127)     at org.springframework.shell.core.JLineShell.promptLoop(JLineShell.java:533)     at org.springframework.shell.core.JLineShell.run(JLineShell.java:179)     at java.lang.Thread.run(Thread.java:745) """,1.0
1,XD-3220,BUG,Done,URGENT,"""Enabling security breaks job launching from Admin UI""","""After enabling security (see XD-3214) and granting user {{ROLE_VIEW, ROLE_CREATE, ROLE_ADMIN}} privileges it's not possible to launch jobs from Admin UI.   For {{bdl-sqoop-combo-lukasz-MONGO-DEV}} job, 403 error is returned when Admin UI attempts to access following URL after """"Launch Job"""" button is pressed: {code} http://ilabphd12.isus.emc.com:9393/jobs/executions?jobParameters=%7B%7D&jobname=bdl-sqoop-combo-lukasz-MONGO-DEV {code}  Please see attached screenshot.""","""""""After enabling security (see XD-3214) and granting user {{ROLE_VIEW, ROLE_CREATE, ROLE_ADMIN}} privileges it's not possible to launch jobs from Admin UI.   For {{bdl-sqoop-combo-lukasz-MONGO-DEV}} job, 403 error is returned when Admin UI attempts to access following URL after """"""""Launch Job"""""""" button is pressed:   Please see attached screenshot.""""""",""" http://ilabphd12.isus.emc.com:9393/jobs/executions?jobParameters=%7B%7D&jobname=bdl-sqoop-combo-lukasz-MONGO-DEV """,2.0
1,XD-3219,BUG,Done,MEDIUM,"""Fix random configuration in SecuredShellTests""","""Since the SecuredShellTests initialize singlenode app in a static way, the random configuration needs to be setup statically as well.""","""""""Since the SecuredShellTests initialize singlenode app in a static way, the random configuration needs to be setup statically as well.""""""",,1.0
1,XD-3218,IMPROVEMENT,Done,MEDIUM,"""[Backport] Handle stream/job deployment status recalculation failures""","""The admin leader election fails when the stream/job module definitions doesn't exist in `module-registry` but still some of the references still exist in ZK (via some of the previous deployments that had this module in module registry).   Though this is expected, this behavior will make *all* the subsequent deployments in *deploying* state because the admin leader isn't elected.  ""","""""""The admin leader election fails when the stream/job module definitions doesn't exist in `module-registry` but still some of the references still exist in ZK (via some of the previous deployments that had this module in module registry).   Though this is expected, this behavior will make *all* the subsequent deployments in *deploying* state because the admin leader isn't elected.  """"""",,1.0
1,XD-3217,BUG,Done,URGENT,"""Cannot connect to admin server with basic security enabled""","""As a user, I'm trying to connect to {{xd-admin}} server with basic security enabled; however, I'm unable to successfully connect to the server and I get the following error message.   {code:java} server-unknown:>admin config server --uri http://localhost:9393 --username bob --password bobspwd Unable to contact XD Admin Server at 'http://localhost:9393'. {code}""","""""""As a user, I'm trying to connect to {{xd-admin}} server with basic security enabled; however, I'm unable to successfully connect to the server and I get the following error message.   """"""",""" server-unknown:>admin config server --uri http://localhost:9393 --username bob --password bobspwd Unable to contact XD Admin Server at 'http://localhost:9393'. """,5.0
1,XD-3216,BUG,Done,MEDIUM,"""On specific shutdown scenarios, the stream resumes from the start of the bus topic""","""https://github.com/spring-projects/spring-xd/issues/1727""","""""""https://github.com/spring-projects/spring-xd/issues/1727""""""",,2.0
1,XD-3214,BUG,Done,URGENT,"""Enabling security breaks Jobs page in Admin UI""","""After enabling Spring XD security in {{XD_HOME/config/servers.yml}}:  {code} spring:   profiles: admin security:   basic:     enabled: true     realm: SpringXD xd:   security:     authentication:       file:         enabled: true         users:           user: password, ROLE_VIEW           admin: password, ROLE_VIEW, ROLE_CREATE, ROLE_ADMIN {code}  after logging in as {{user}} with only {{ROLE_VIEW}} privilege, Jobs admin page is broken and is not displaying data. 403 error code is returned for following URLs:  {code} http://localhost:9393/jobs/configurations.json?page=0&size=10 http://localhost:9393/jobs/definitions.json?page=0&size=10 {code}  Looks like {{/jobs/configurations.\*}} and {{/jobs/definitions.\*}} URLs are not covered in security section of applications.yml file.""","""""""After enabling Spring XD security in {{XD_HOME/config/servers.yml}}:    after logging in as {{user}} with only {{ROLE_VIEW}} privilege, Jobs admin page is broken and is not displaying data. 403 error code is returned for following URLs:    Looks like {{/jobs/configurations.\*}} and {{/jobs/definitions.\*}} URLs are not covered in security section of applications.yml file.""""""",""" spring:   profiles: admin security:   basic:     enabled: true     realm: SpringXD xd:   security:     authentication:       file:         enabled: true         users:           user: password, ROLE_VIEW           admin: password, ROLE_VIEW, ROLE_CREATE, ROLE_ADMIN  http://localhost:9393/jobs/configurations.json?page=0&size=10 http://localhost:9393/jobs/definitions.json?page=0&size=10 """,2.0
1,XD-3213,IMPROVEMENT,Done,MEDIUM,"""Use Deque instead of LinkedList when gathering metrics""","""When profiling metrics we noticed a small improvement when using Deque instead of <USER>in ExponentialMovingAverageRatio, ExponentialMovingAverageRate...  ""","""""""When profiling metrics we noticed a small improvement when using Deque instead of <USER>in ExponentialMovingAverageRatio, ExponentialMovingAverageRate...  """"""",,3.0
1,XD-3208,BUG,Done,MEDIUM,"""Change in file source breaks backward compatibility ""","""With version 1.2.0 the option ref of the file source was removed and a new option mode was introduced.  see XD-2850 and PR  https://github.com/spring-projects/spring-xd/pull/1624.  This means you have to destroy all streams using the ref option before you do an upgrade.  It would have been much better to leave the ref option in the code and emit a deprecation warning if it is still used. This way an upgrade would be possible without interruption.     ""","""""""With version 1.2.0 the option ref of the file source was removed and a new option mode was introduced.  see XD-2850 and PR  https://github.com/spring-projects/spring-xd/pull/1624.  This means you have to destroy all streams using the ref option before you do an upgrade.  It would have been much better to leave the ref option in the code and emit a deprecation warning if it is still used. This way an upgrade would be possible without interruption.     """"""",,1.0
1,XD-3206,BUG,Done,HIGH,"""An error message occurs about the shortDescription (header-enricher)""","""Here is an error I got using the header-enricher from spring-xd-modules :   {code:java} Field error in object 'info' on field 'shortDescription': rejected value [A Header Enricher to set message headers in a stream]; codes [Pattern.info.shortDescription,Pattern.shortDescription,Pattern.java.lang.String,Pattern]; arguments [org.springframework.context.support.DefaultMessageSourceResolvable: codes [info.shortDescription,shortDescription]; arguments []; default message [shortDescription],[Ljavax.validation.constraints.Pattern$Flag;@11eeec65,^\p{IsUppercase}.*\.$]; default message [Short description must start with a capital letter and end with a dot] {code}  And if I look the config properties, indeed, short description doesn't end with a dot. {code:java} info.shortDescription = A Header Enricher to set message headers in a stream {code}""","""""""Here is an error I got using the header-enricher from spring-xd-modules :     And if I look the config properties, indeed, short description doesn't end with a dot. """"""",""" Field error in object 'info' on field 'shortDescription': rejected value [A Header Enricher to set message headers in a stream]; codes [Pattern.info.shortDescription,Pattern.shortDescription,Pattern.java.lang.String,Pattern]; arguments [org.springframework.context.support.DefaultMessageSourceResolvable: codes [info.shortDescription,shortDescription]; arguments []; default message [shortDescription],[Ljavax.validation.constraints.Pattern$Flag;@11eeec65,^\p{IsUppercase}.*\.$]; default message [Short description must start with a capital letter and end with a dot]  info.shortDescription = A Header Enricher to set message headers in a stream """,1.0
1,XD-3205,FEATURE,Done,MEDIUM,"""Investigate the steps to Ambari upgrade Spring XD""","""As a user, I'd like to upgrade Spring XD from 1.2 RC to 1.2 GA using the Ambari plugin, so I can work on the latest release bits. I'd like to refer to the documentation to do so.""","""""""As a user, I'd like to upgrade Spring XD from 1.2 RC to 1.2 GA using the Ambari plugin, so I can work on the latest release bits. I'd like to refer to the documentation to do so.""""""",,1.0
1,XD-3204,FEATURE,Done,MEDIUM,"""Review spring-bus design specs""","""As a spring-bus lead, I'd like to review the current spring-bus architecture and the design specs, so I can address any foundation level gaps.""","""""""As a spring-bus lead, I'd like to review the current spring-bus architecture and the design specs, so I can address any foundation level gaps.""""""",,5.0
1,XD-3202,FEATURE,Done,MEDIUM,"""Investigate performance of channel metrics in SI 4.2 ""","""As a developer, I'd like to investigate channel performance issues in SI 4.2, so I can determine the bottlenecks and take corrective actions to improve overall channel performance. ""","""""""As a developer, I'd like to investigate channel performance issues in SI 4.2, so I can determine the bottlenecks and take corrective actions to improve overall channel performance. """"""",,8.0
1,XD-3198,FEATURE,Done,MEDIUM,"""Spike: Investigate the use of config server for spring-cloud-stream modules ""","""As a developer, I'd like to use spring-cloud-config server for spring-bus modules, so I can centrally manage external properties.""","""""""As a developer, I'd like to use spring-cloud-config server for spring-bus modules, so I can centrally manage external properties.""""""",,8.0
1,XD-3196,FEATURE,Done,MEDIUM,"""Move MASTER branch CI builds to EC2 based infrastructure""","""As a developer, I'd like to migrate the current MASTER branch CI builds to EC2 instances, so I can manage them all in one-place reliably.""","""""""As a developer, I'd like to migrate the current MASTER branch CI builds to EC2 instances, so I can manage them all in one-place reliably.""""""",,8.0
1,XD-3194,FEATURE,Done,MEDIUM,"""Spike: Investigate spring-cloud-config and the XD fit""","""As a developer, I'd like to have a central place to manage external properties for applications across all the environments, so I can provide server and client-side support for externalized configuration for XD-Admin and XD-Container servers. ""","""""""As a developer, I'd like to have a central place to manage external properties for applications across all the environments, so I can provide server and client-side support for externalized configuration for XD-Admin and XD-Container servers. """"""",,8.0
1,XD-3193,FEATURE,Done,MEDIUM,"""Spike: Investigate bootification of module options""","""As a developer, I'd like to handle module options via pure boot property source management, so I can leverage Boot's module [METADATA|http://docs.spring.io/spring-boot/docs/current-SNAPSHOT/reference/htmlsingle/#configuration-metadata] option to inject module options as opposed to maintaining them in core Spring XD runtime CP. ""","""""""As a developer, I'd like to handle module options via pure boot property source management, so I can leverage Boot's module [METADATA|http://docs.spring.io/spring-boot/docs/current-SNAPSHOT/reference/htmlsingle/#configuration-metadata] option to inject module options as opposed to maintaining them in core Spring XD runtime CP. """"""",,8.0
1,XD-3192,FEATURE,Done,MEDIUM,"""Spike: Investigate Boot export metrics and the XD fit""","""As a user, I'd like to have the module/app specific metrics consumed directly from Boot actuator [export()|https://github.com/spring-projects/spring-boot/blob/master/spring-boot-actuator/src/main/java/org/springframework/boot/actuate/autoconfigure/MetricRepositoryAutoConfiguration.java] API, so I can have insight on how it is performing, being used and that it works etc.  ""","""""""As a user, I'd like to have the module/app specific metrics consumed directly from Boot actuator [export()|https://github.com/spring-projects/spring-boot/blob/master/spring-boot-actuator/src/main/java/org/springframework/boot/actuate/autoconfigure/MetricRepositoryAutoConfiguration.java] API, so I can have insight on how it is performing, being used and that it works etc.  """"""",,8.0
1,XD-3189,FEATURE,Done,MEDIUM,"""Testers need ability to wait for a file to be created in XD directory""","""User's need ability to wait for user specified time in millis for a file to be created in the XD directory.  If file is not created in allotted time then return false else return true.  Also check to see if a file exists in the XD directory.  ""","""""""User's need ability to wait for user specified time in millis for a file to be created in the XD directory.  If file is not created in allotted time then return false else return true.  Also check to see if a file exists in the XD directory.  """"""",,3.0
1,XD-3188,BUG,Done,HIGH,"""FileDeletionListener resolves resources once""","""In the {{filejdbc}} job, there is the option to delete the imported files.  This functionality is created using a listener called the {{FileDeletionStepExecutionListener}}.  When you run the job the first time with the {{--deleteFiles=true}}, everything works as expected.  The second time you run the job, the files are not deleted.  I believe the issue here is that since the {{FileDeletionStepExecutionListener}} is a singleton, the resources are resolved only once (the first time the job runs) and so it works the first time, but if the job is run again later and new files match the expression, they are not picked up.  I believe the fix is to make the {{FileDeletionStepExecutionListener}} used in this job step scoped.""","""""""In the {{filejdbc}} job, there is the option to delete the imported files.  This functionality is created using a listener called the {{FileDeletionStepExecutionListener}}.  When you run the job the first time with the {{--deleteFiles=true}}, everything works as expected.  The second time you run the job, the files are not deleted.  I believe the issue here is that since the {{FileDeletionStepExecutionListener}} is a singleton, the resources are resolved only once (the first time the job runs) and so it works the first time, but if the job is run again later and new files match the expression, they are not picked up.  I believe the fix is to make the {{FileDeletionStepExecutionListener}} used in this job step scoped.""""""",,1.0
1,XD-3187,BUG,Done,MEDIUM,"""XD admin leader should cleanup deployment after initializing the container path cache""","""The admin leader starts cleaning up the deployments for the container(s) that is/are no longer connected to the ZK. This clean up needs to happen after the container path cache is started by the admin leader. ""","""""""The admin leader starts cleaning up the deployments for the container(s) that is/are no longer connected to the ZK. This clean up needs to happen after the container path cache is started by the admin leader. """"""",,1.0
1,XD-3184,FEATURE,Done,MEDIUM,"""Update spring-xd-yarn servers.yml with settings for HDP 2.2.6.0""","""We need to add the settings needed to run XD on YARN when using Hortonworks HDP 2.2.6.0 which is the version you now get when installing with Ambari.""","""""""We need to add the settings needed to run XD on YARN when using Hortonworks HDP 2.2.6.0 which is the version you now get when installing with Ambari.""""""",,1.0
1,XD-3183,FEATURE,Done,MEDIUM,"""Upgrade to Spring Boot 1.2.5""","""Spring Boot 1.2.4 (and earlier) does not allow for the retrieval of boolean values from the vcap environment. This means that when XD Admin tries to retrieve the value of (for example) {{vcap.services.rabbitmq.credentials.protocols.amqp.ssl}} it will fail, as that value returns a boolean.  Spring Boot 1.2.5 (as yet unreleased) contains a fix for this issue (https://github.com/spring-projects/spring-boot/pull/3237)""","""""""Spring Boot 1.2.4 (and earlier) does not allow for the retrieval of boolean values from the vcap environment. This means that when XD Admin tries to retrieve the value of (for example) {{vcap.services.rabbitmq.credentials.protocols.amqp.ssl}} it will fail, as that value returns a boolean.  Spring Boot 1.2.5 (as yet unreleased) contains a fix for this issue (https://github.com/spring-projects/spring-boot/pull/3237)""""""",,1.0
1,XD-3180,FEATURE,Done,HIGH,"""Spring xd, to configure the stream drag and graphic ""","""spring xd Can drag way to configure flow in the form of figure?  Similar to configure a workflow graphical interface   Looking forward to reply""","""""""spring xd Can drag way to configure flow in the form of figure?  Similar to configure a workflow graphical interface   Looking forward to reply""""""",,1.0
1,XD-3178,BUG,Done,MEDIUM,"""Hadoop Distro log message shows wrong version when set via env var""","""If we export HADOOP_DISTRO env var instead of using --hadoopDistro parameter then the logging message is wrong, it always says  Hadoop Distro: hadoop26  even if we set HADOOP_DISTRO to something else  The classpath is built correctly. Maybe we should just remove this logging message since we log the actual version used in the next log message. ""","""""""If we export HADOOP_DISTRO env var instead of using --hadoopDistro parameter then the logging message is wrong, it always says  Hadoop Distro: hadoop26  even if we set HADOOP_DISTRO to something else  The classpath is built correctly. Maybe we should just remove this logging message since we log the actual version used in the next log message. """"""",,1.0
1,XD-3177,FEATURE,Done,MEDIUM,"""Make RabbitMessageBus RabbitMQ Config Properties Optional""","""When the bus is used outside of the XD container (e.g. spring-bus), the inheritance from Spring Boot configuration is broken (no application.yml or servers.yml on the cp).  Make the bus properties optional (Add """":"""")""","""""""When the bus is used outside of the XD container (e.g. spring-bus), the inheritance from Spring Boot configuration is broken (no application.yml or servers.yml on the cp).  Make the bus properties optional (Add """""""":"""""""")""""""",,1.0
1,XD-3176,BUG,Done,URGENT,"""Using HDFS for custom module home doesn't work with Kerberized Hadoop cluster""","""I tried setting the xd.customModule.home property to point to a Kerberized Hadoop cluster with all usual security config settings provided. It failed with the following exception:  {code} org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'moduleRegistry' defined in class path resource [META-INF/spring-xd/internal/repositories.xml]: Cannot create inner bean 'org.springframework.xd.dirt.module.CustomModuleRegistryFactoryBean#19f459aa' of type [org.springframework.xd.dirt.module.CustomModuleRegistryFactoryBean] while setting constructor argument with key [1]; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'org.springframework.xd.dirt.module.CustomModuleRegistryFactoryBean#19f459aa' defined in class path resource [META-INF/spring-xd/internal/repositories.xml]: Invocation of init method failed; nested exception is org.apache.hadoop.security.AccessControlException: SIMPLE authentication is not enabled.  Available:[TOKEN, KERBEROS]  at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveInnerBean(BeanDefinitionValueResolver.java:313) ~[spring-beans-4.1.6.RELEASE.jar:4.1.6.RELEASE]  at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveValueIfNecessary(BeanDefinitionValueResolver.java:122) ~[spring-beans-4.1.6.RELEASE.jar:4.1.6.RELEASE]  at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveManagedList(BeanDefinitionValueResolver.java:382) ~[spring-beans-4.1.6.RELEASE.jar:4.1.6.RELEASE]  at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveValueIfNecessary(BeanDefinitionValueResolver.java:157) ~[spring-beans-4.1.6.RELEASE.jar:4.1.6.RELEASE]  at org.springframework.beans.factory.support.ConstructorResolver.resolveConstructorArguments(ConstructorResolver.java:648) ~[spring-beans-4.1.6.RELEASE.jar:4.1.6.RELEASE]  at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:140) ~[spring-beans-4.1.6.RELEASE.jar:4.1.6.RELEASE]  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1139) ~[spring-beans-4.1.6.RELEASE.jar:4.1.6.RELEASE]  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1042) ~[spring-beans-4.1.6.RELEASE.jar:4.1.6.RELEASE]  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:504) ~[spring-beans-4.1.6.RELEASE.jar:4.1.6.RELEASE]  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:476) ~[spring-beans-4.1.6.RELEASE.jar:4.1.6.RELEASE]  at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:303) ~[spring-beans-4.1.6.RELEASE.jar:4.1.6.RELEASE]  at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:230) ~[spring-beans-4.1.6.RELEASE.jar:4.1.6.RELEASE]  at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:299) ~[spring-beans-4.1.6.RELEASE.jar:4.1.6.RELEASE]  at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:194) ~[spring-beans-4.1.6.RELEASE.jar:4.1.6.RELEASE]  at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:755) ~[spring-beans-4.1.6.RELEASE.jar:4.1.6.RELEASE]  at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:757) ~[spring-context-4.1.6.RELEASE.jar:4.1.6.RELEASE]  at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:480) ~[spring-context-4.1.6.RELEASE.jar:4.1.6.RELEASE]  at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:686) [spring-boot-1.2.3.RELEASE.jar:1.2.3.RELEASE]  at org.springframework.boot.SpringApplication.run(SpringApplication.java:320) [spring-boot-1.2.3.RELEASE.jar:1.2.3.RELEASE]  at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:139) [spring-boot-1.2.3.RELEASE.jar:1.2.3.RELEASE]  at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:129) [spring-boot-1.2.3.RELEASE.jar:1.2.3.RELEASE]  at org.springframework.xd.dirt.server.admin.AdminServerApplication.run(AdminServerApplication.java:95) [spring-xd-dirt-1.2.0.BUILD-SNAPSHOT.jar:1.2.0.BUILD-SNAPSHOT]  at org.springframework.xd.dirt.server.admin.AdminServerApplication.main(AdminServerApplication.java:79) [spring-xd-dirt-1.2.0.BUILD-SNAPSHOT.jar:1.2.0.BUILD-SNAPSHOT] Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'org.springframework.xd.dirt.module.CustomModuleRegistryFactoryBean#19f459aa' defined in class path resource [META-INF/spring-xd/internal/repositories.xml]: Invocation of init method failed; nested exception is org.apache.hadoop.security.AccessControlException: SIMPLE authentication is not enabled.  Available:[TOKEN, KERBEROS]  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1574) ~[spring-beans-4.1.6.RELEASE.jar:4.1.6.RELEASE]  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:539) ~[spring-beans-4.1.6.RELEASE.jar:4.1.6.RELEASE]  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:476) ~[spring-beans-4.1.6.RELEASE.jar:4.1.6.RELEASE]  at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveInnerBean(BeanDefinitionValueResolver.java:299) ~[spring-beans-4.1.6.RELEASE.jar:4.1.6.RELEASE]  ... 22 common frames omitted Caused by: org.apache.hadoop.security.AccessControlException: SIMPLE authentication is not enabled.  Available:[TOKEN, KERBEROS]  at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[na:1.7.0_67]  at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) ~[na:1.7.0_67]  at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[na:1.7.0_67]  at java.lang.reflect.Constructor.newInstance(Constructor.java:526) ~[na:1.7.0_67]  at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106) ~[hadoop-common-2.6.0.jar:na]  at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:73) ~[hadoop-common-2.6.0.jar:na]  at org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2755) ~[hadoop-hdfs-2.6.0.jar:na]  at org.apache.hadoop.hdfs.DFSClient.mkdirs(DFSClient.java:2724) ~[hadoop-hdfs-2.6.0.jar:na]  at org.apache.hadoop.hdfs.DistributedFileSystem$17.doCall(DistributedFileSystem.java:870) ~[hadoop-hdfs-2.6.0.jar:na]  at org.apache.hadoop.hdfs.DistributedFileSystem$17.doCall(DistributedFileSystem.java:866) ~[hadoop-hdfs-2.6.0.jar:na]  at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81) ~[hadoop-common-2.6.0.jar:na]  at org.apache.hadoop.hdfs.DistributedFileSystem.mkdirsInternal(DistributedFileSystem.java:866) ~[hadoop-hdfs-2.6.0.jar:na]  at org.apache.hadoop.hdfs.DistributedFileSystem.mkdirs(DistributedFileSystem.java:859) ~[hadoop-hdfs-2.6.0.jar:na]  at org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:1817) ~[hadoop-common-2.6.0.jar:na]  at org.springframework.xd.dirt.module.ExtendedResource$HdfsExtendedResource.mkdirs(ExtendedResource.java:127) ~[spring-xd-dirt-1.2.0.BUILD-SNAPSHOT.jar:1.2.0.BUILD-SNAPSHOT]  at org.springframework.xd.dirt.module.WritableResourceModuleRegistry.afterPropertiesSet(WritableResourceModuleRegistry.java:123) ~[spring-xd-dirt-1.2.0.BUILD-SNAPSHOT.jar:1.2.0.BUILD-SNAPSHOT]  at org.springframework.xd.dirt.module.CustomModuleRegistryFactoryBean.afterPropertiesSet(CustomModuleRegistryFactoryBean.java:79) ~[spring-xd-dirt-1.2.0.BUILD-SNAPSHOT.jar:1.2.0.BUILD-SNAPSHOT]  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1633) ~[spring-beans-4.1.6.RELEASE.jar:4.1.6.RELEASE]  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1570) ~[spring-beans-4.1.6.RELEASE.jar:4.1.6.RELEASE]  ... 25 common frames omitted Caused by: org.apache.hadoop.ipc.RemoteException: SIMPLE authentication is not enabled.  Available:[TOKEN, KERBEROS]  at org.apache.hadoop.ipc.Client.call(Client.java:1468) ~[hadoop-common-2.6.0.jar:na]  at org.apache.hadoop.ipc.Client.call(Client.java:1399) ~[hadoop-common-2.6.0.jar:na]  at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232) ~[hadoop-common-2.6.0.jar:na]  at com.sun.proxy.$Proxy79.mkdirs(Unknown Source) ~[na:na]  at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.mkdirs(ClientNamenodeProtocolTranslatorPB.java:539) ~[hadoop-hdfs-2.6.0.jar:na]  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.7.0_67]  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) ~[na:1.7.0_67]  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.7.0_67]  at java.lang.reflect.Method.invoke(Method.java:606) ~[na:1.7.0_67]  at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187) ~[hadoop-common-2.6.0.jar:na]  at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102) ~[hadoop-common-2.6.0.jar:na]  at com.sun.proxy.$Proxy80.mkdirs(Unknown Source) ~[na:na]  at org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2753) ~[hadoop-hdfs-2.6.0.jar:na]  ... 37 common frames omitted 2015-06-10T14:49:20-0400 1.2.0.SNAP ERROR main boot.SpringApplication - Application startup failed {code}""","""""""I tried setting the xd.customModule.home property to point to a Kerberized Hadoop cluster with all usual security config settings provided. It failed with the following exception:  """"""",""" org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'moduleRegistry' defined in class path resource [META-INF/spring-xd/internal/repositories.xml]: Cannot create inner bean 'org.springframework.xd.dirt.module.CustomModuleRegistryFactoryBean#19f459aa' of type [org.springframework.xd.dirt.module.CustomModuleRegistryFactoryBean] while setting constructor argument with key [1]; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'org.springframework.xd.dirt.module.CustomModuleRegistryFactoryBean#19f459aa' defined in class path resource [META-INF/spring-xd/internal/repositories.xml]: Invocation of init method failed; nested exception is org.apache.hadoop.security.AccessControlException: SIMPLE authentication is not enabled.  Available:[TOKEN, KERBEROS]  at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveInnerBean(BeanDefinitionValueResolver.java:313) ~[spring-beans-4.1.6.RELEASE.jar:4.1.6.RELEASE]  at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveValueIfNecessary(BeanDefinitionValueResolver.java:122) ~[spring-beans-4.1.6.RELEASE.jar:4.1.6.RELEASE]  at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveManagedList(BeanDefinitionValueResolver.java:382) ~[spring-beans-4.1.6.RELEASE.jar:4.1.6.RELEASE]  at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveValueIfNecessary(BeanDefinitionValueResolver.java:157) ~[spring-beans-4.1.6.RELEASE.jar:4.1.6.RELEASE]  at org.springframework.beans.factory.support.ConstructorResolver.resolveConstructorArguments(ConstructorResolver.java:648) ~[spring-beans-4.1.6.RELEASE.jar:4.1.6.RELEASE]  at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:140) ~[spring-beans-4.1.6.RELEASE.jar:4.1.6.RELEASE]  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1139) ~[spring-beans-4.1.6.RELEASE.jar:4.1.6.RELEASE]  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1042) ~[spring-beans-4.1.6.RELEASE.jar:4.1.6.RELEASE]  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:504) ~[spring-beans-4.1.6.RELEASE.jar:4.1.6.RELEASE]  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:476) ~[spring-beans-4.1.6.RELEASE.jar:4.1.6.RELEASE]  at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:303) ~[spring-beans-4.1.6.RELEASE.jar:4.1.6.RELEASE]  at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:230) ~[spring-beans-4.1.6.RELEASE.jar:4.1.6.RELEASE]  at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:299) ~[spring-beans-4.1.6.RELEASE.jar:4.1.6.RELEASE]  at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:194) ~[spring-beans-4.1.6.RELEASE.jar:4.1.6.RELEASE]  at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:755) ~[spring-beans-4.1.6.RELEASE.jar:4.1.6.RELEASE]  at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:757) ~[spring-context-4.1.6.RELEASE.jar:4.1.6.RELEASE]  at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:480) ~[spring-context-4.1.6.RELEASE.jar:4.1.6.RELEASE]  at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:686) [spring-boot-1.2.3.RELEASE.jar:1.2.3.RELEASE]  at org.springframework.boot.SpringApplication.run(SpringApplication.java:320) [spring-boot-1.2.3.RELEASE.jar:1.2.3.RELEASE]  at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:139) [spring-boot-1.2.3.RELEASE.jar:1.2.3.RELEASE]  at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:129) [spring-boot-1.2.3.RELEASE.jar:1.2.3.RELEASE]  at org.springframework.xd.dirt.server.admin.AdminServerApplication.run(AdminServerApplication.java:95) [spring-xd-dirt-1.2.0.BUILD-SNAPSHOT.jar:1.2.0.BUILD-SNAPSHOT]  at org.springframework.xd.dirt.server.admin.AdminServerApplication.main(AdminServerApplication.java:79) [spring-xd-dirt-1.2.0.BUILD-SNAPSHOT.jar:1.2.0.BUILD-SNAPSHOT] Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'org.springframework.xd.dirt.module.CustomModuleRegistryFactoryBean#19f459aa' defined in class path resource [META-INF/spring-xd/internal/repositories.xml]: Invocation of init method failed; nested exception is org.apache.hadoop.security.AccessControlException: SIMPLE authentication is not enabled.  Available:[TOKEN, KERBEROS]  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1574) ~[spring-beans-4.1.6.RELEASE.jar:4.1.6.RELEASE]  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:539) ~[spring-beans-4.1.6.RELEASE.jar:4.1.6.RELEASE]  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:476) ~[spring-beans-4.1.6.RELEASE.jar:4.1.6.RELEASE]  at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveInnerBean(BeanDefinitionValueResolver.java:299) ~[spring-beans-4.1.6.RELEASE.jar:4.1.6.RELEASE]  ... 22 common frames omitted Caused by: org.apache.hadoop.security.AccessControlException: SIMPLE authentication is not enabled.  Available:[TOKEN, KERBEROS]  at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[na:1.7.0_67]  at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) ~[na:1.7.0_67]  at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[na:1.7.0_67]  at java.lang.reflect.Constructor.newInstance(Constructor.java:526) ~[na:1.7.0_67]  at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106) ~[hadoop-common-2.6.0.jar:na]  at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:73) ~[hadoop-common-2.6.0.jar:na]  at org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2755) ~[hadoop-hdfs-2.6.0.jar:na]  at org.apache.hadoop.hdfs.DFSClient.mkdirs(DFSClient.java:2724) ~[hadoop-hdfs-2.6.0.jar:na]  at org.apache.hadoop.hdfs.DistributedFileSystem$17.doCall(DistributedFileSystem.java:870) ~[hadoop-hdfs-2.6.0.jar:na]  at org.apache.hadoop.hdfs.DistributedFileSystem$17.doCall(DistributedFileSystem.java:866) ~[hadoop-hdfs-2.6.0.jar:na]  at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81) ~[hadoop-common-2.6.0.jar:na]  at org.apache.hadoop.hdfs.DistributedFileSystem.mkdirsInternal(DistributedFileSystem.java:866) ~[hadoop-hdfs-2.6.0.jar:na]  at org.apache.hadoop.hdfs.DistributedFileSystem.mkdirs(DistributedFileSystem.java:859) ~[hadoop-hdfs-2.6.0.jar:na]  at org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:1817) ~[hadoop-common-2.6.0.jar:na]  at org.springframework.xd.dirt.module.ExtendedResource$HdfsExtendedResource.mkdirs(ExtendedResource.java:127) ~[spring-xd-dirt-1.2.0.BUILD-SNAPSHOT.jar:1.2.0.BUILD-SNAPSHOT]  at org.springframework.xd.dirt.module.WritableResourceModuleRegistry.afterPropertiesSet(WritableResourceModuleRegistry.java:123) ~[spring-xd-dirt-1.2.0.BUILD-SNAPSHOT.jar:1.2.0.BUILD-SNAPSHOT]  at org.springframework.xd.dirt.module.CustomModuleRegistryFactoryBean.afterPropertiesSet(CustomModuleRegistryFactoryBean.java:79) ~[spring-xd-dirt-1.2.0.BUILD-SNAPSHOT.jar:1.2.0.BUILD-SNAPSHOT]  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1633) ~[spring-beans-4.1.6.RELEASE.jar:4.1.6.RELEASE]  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1570) ~[spring-beans-4.1.6.RELEASE.jar:4.1.6.RELEASE]  ... 25 common frames omitted Caused by: org.apache.hadoop.ipc.RemoteException: SIMPLE authentication is not enabled.  Available:[TOKEN, KERBEROS]  at org.apache.hadoop.ipc.Client.call(Client.java:1468) ~[hadoop-common-2.6.0.jar:na]  at org.apache.hadoop.ipc.Client.call(Client.java:1399) ~[hadoop-common-2.6.0.jar:na]  at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232) ~[hadoop-common-2.6.0.jar:na]  at com.sun.proxy.$Proxy79.mkdirs(Unknown Source) ~[na:na]  at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.mkdirs(ClientNamenodeProtocolTranslatorPB.java:539) ~[hadoop-hdfs-2.6.0.jar:na]  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.7.0_67]  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) ~[na:1.7.0_67]  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.7.0_67]  at java.lang.reflect.Method.invoke(Method.java:606) ~[na:1.7.0_67]  at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187) ~[hadoop-common-2.6.0.jar:na]  at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102) ~[hadoop-common-2.6.0.jar:na]  at com.sun.proxy.$Proxy80.mkdirs(Unknown Source) ~[na:na]  at org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2753) ~[hadoop-hdfs-2.6.0.jar:na]  ... 37 common frames omitted 2015-06-10T14:49:20-0400 1.2.0.SNAP ERROR main boot.SpringApplication - Application startup failed """,3.0
1,XD-3172,FEATURE,Done,MEDIUM,"""Provide a source option to enable the SOF/EOF markers when splitting a file into lines""","""Depends on INT-3727""","""""""Depends on INT-3727""""""",,2.0
1,XD-3171,BUG,Done,HIGH,"""Composed modules not working on YARN""","""From https://github.com/spring-projects/spring-xd/issues/1704  I am trying to use composed modules when running on YARN.    In ZK, each child module definition of the composed module gets serialized as follows:  ``` {""""@class"""":""""org.springframework.xd.module.SimpleModuleDefinition"""",""""name"""":""""transform"""",""""type"""":""""processor"""",""""location"""":""""file:/tmp/hadoop-hduser/nm-local-dir/usercache/hduser/appcache/application_1433789137218_0001/filecache/17/spring-xd-yarn-1.1.2.RELEASE.zip/modules/processor/transform/""""} ```  When I try to use the composed module on YARN, it may be deployed to a different container where the """"location"""" file path is not valid.  In this case I get the following exception:  ``` java.lang.IllegalArgumentException: File must exist  at org.springframework.boot.loader.data.RandomAccessDataFile.<init>(RandomAccessDataFile.java:67)  at org.springframework.boot.loader.data.RandomAccessDataFile.<init>(RandomAccessDataFile.java:51)  at org.springframework.boot.loader.jar.JarFile.<init>(JarFile.java:95)  at org.springframework.boot.loader.archive.JarFileArchive.<init>(JarFileArchive.java:61)  at org.springframework.boot.loader.archive.JarFileArchive.<init>(JarFileArchive.java:57)  at org.springframework.xd.module.support.ModuleUtils.createModuleClassLoader(ModuleUtils.java:54)  at org.springframework.xd.module.support.ModuleUtils.createModuleClassLoader(ModuleUtils.java:47)  at org.springframework.xd.module.options.DefaultModuleOptionsMetadataResolver.resolveNormalMetadata(DefaultModuleOptionsMetadataResolver.java:186)  at org.springframework.xd.module.options.DefaultModuleOptionsMetadataResolver.resolve(DefaultModuleOptionsMetadataResolver.java:164)  at org.springframework.xd.module.options.DelegatingModuleOptionsMetadataResolver.resolve(DelegatingModuleOptionsMetadataResolver.java:44)  at org.springframework.xd.module.options.EnvironmentAwareModuleOptionsMetadataResolver.resolve(EnvironmentAwareModuleOptionsMetadataResolver.java:127)  at org.springframework.xd.module.options.DefaultModuleOptionsMetadataResolver.resolveComposedModuleMetadata(DefaultModuleOptionsMetadataResolver.java:175)  at org.springframework.xd.module.options.DefaultModuleOptionsMetadataResolver.resolve(DefaultModuleOptionsMetadataResolver.java:167)  at org.springframework.xd.module.options.DelegatingModuleOptionsMetadataResolver.resolve(DelegatingModuleOptionsMetadataResolver.java:44)  at org.springframework.xd.module.options.EnvironmentAwareModuleOptionsMetadataResolver.resolve(EnvironmentAwareModuleOptionsMetadataResolver.java:127)  at org.springframework.xd.dirt.stream.XDStreamParser.parse(XDStreamParser.java:174) ```  I think the issue may be related to the following line in ArchiveModuleRegistry:  ``` String filename = resource.getFile().getCanonicalFile().getName(); ```  ""","""""""From https://github.com/spring-projects/spring-xd/issues/1704  I am trying to use composed modules when running on YARN.    In ZK, each child module definition of the composed module gets serialized as follows:  ``` {""""""""@class"""""""":""""""""org.springframework.xd.module.SimpleModuleDefinition"""""""",""""""""name"""""""":""""""""transform"""""""",""""""""type"""""""":""""""""processor"""""""",""""""""location"""""""":""""""""file:/tmp/hadoop-hduser/nm-local-dir/usercache/hduser/appcache/application_1433789137218_0001/filecache/17/spring-xd-yarn-1.1.2.RELEASE.zip/modules/processor/transform/""""""""} ```  When I try to use the composed module on YARN, it may be deployed to a different container where the """"""""location"""""""" file path is not valid.  In this case I get the following exception:  ``` java.lang.IllegalArgumentException: File must exist  at org.springframework.boot.loader.data.RandomAccessDataFile.<init>(RandomAccessDataFile.java:67)  at org.springframework.boot.loader.data.RandomAccessDataFile.<init>(RandomAccessDataFile.java:51)  at org.springframework.boot.loader.jar.JarFile.<init>(JarFile.java:95)  at org.springframework.boot.loader.archive.JarFileArchive.<init>(JarFileArchive.java:61)  at org.springframework.boot.loader.archive.JarFileArchive.<init>(JarFileArchive.java:57)  at org.springframework.xd.module.support.ModuleUtils.createModuleClassLoader(ModuleUtils.java:54)  at org.springframework.xd.module.support.ModuleUtils.createModuleClassLoader(ModuleUtils.java:47)  at org.springframework.xd.module.options.DefaultModuleOptionsMetadataResolver.resolveNormalMetadata(DefaultModuleOptionsMetadataResolver.java:186)  at org.springframework.xd.module.options.DefaultModuleOptionsMetadataResolver.resolve(DefaultModuleOptionsMetadataResolver.java:164)  at org.springframework.xd.module.options.DelegatingModuleOptionsMetadataResolver.resolve(DelegatingModuleOptionsMetadataResolver.java:44)  at org.springframework.xd.module.options.EnvironmentAwareModuleOptionsMetadataResolver.resolve(EnvironmentAwareModuleOptionsMetadataResolver.java:127)  at org.springframework.xd.module.options.DefaultModuleOptionsMetadataResolver.resolveComposedModuleMetadata(DefaultModuleOptionsMetadataResolver.java:175)  at org.springframework.xd.module.options.DefaultModuleOptionsMetadataResolver.resolve(DefaultModuleOptionsMetadataResolver.java:167)  at org.springframework.xd.module.options.DelegatingModuleOptionsMetadataResolver.resolve(DelegatingModuleOptionsMetadataResolver.java:44)  at org.springframework.xd.module.options.EnvironmentAwareModuleOptionsMetadataResolver.resolve(EnvironmentAwareModuleOptionsMetadataResolver.java:127)  at org.springframework.xd.dirt.stream.XDStreamParser.parse(XDStreamParser.java:174) ```  I think the issue may be related to the following line in ArchiveModuleRegistry:  ``` String filename = resource.getFile().getCanonicalFile().getName(); ```  """"""",,5.0
1,XD-3170,FEATURE,Done,MEDIUM,"""Update Spark streaming documentation""","""As a user I need to know the Spark streaming features like adding tap at the spark module output and the examples need to be updated. The documentation also needs some more information on `Reliable` receiver.""","""""""As a user I need to know the Spark streaming features like adding tap at the spark module output and the examples need to be updated. The documentation also needs some more information on `Reliable` receiver.""""""",,1.0
1,XD-3169,FEATURE,Done,MEDIUM,"""Spike: Explore options for batch modules to be short lived""","""As a developer, I'd like a job module to be bootstrapped when the job is launched and shut down once it is complete instead of the current behavior of bootstrapping the context when the module is deployed regardless of if it's being used so that I can achieve better resource utilization.""","""""""As a developer, I'd like a job module to be bootstrapped when the job is launched and shut down once it is complete instead of the current behavior of bootstrapping the context when the module is deployed regardless of if it's being used so that I can achieve better resource utilization.""""""",,8.0
1,XD-3166,FEATURE,Done,MEDIUM,"""Publish performance benchmarks""","""As a developer, I'd like to publish performance benchmarks along with the infrastructure specifics, so the users can use it as a reference while setting up Spring XD cluster. ""","""""""As a developer, I'd like to publish performance benchmarks along with the infrastructure specifics, so the users can use it as a reference while setting up Spring XD cluster. """"""",,8.0
1,XD-3165,FEATURE,Done,MEDIUM,"""Synchronize XD and XD + Ambari RPMs into a single repo""","""As a PM, I'd like to have XD and XD + Ambari RPM scripts into a single public repo, so that users can go to a single location to use the respective build scripts.""","""""""As a PM, I'd like to have XD and XD + Ambari RPM scripts into a single public repo, so that users can go to a single location to use the respective build scripts.""""""",,3.0
1,XD-3164,FEATURE,Done,MEDIUM,"""Kafka bus defaults configurable at producer/consumer level""","""As a developer, I want to be able to override Kafka bus defaults for module consumers and producers, so that I can finely tune performance and behaviour.   Such properties should include - autoCommitEnabled,queueSize,maxWait,fetchSize for consumers - batchSize,batchTimeout for producers""","""""""As a developer, I want to be able to override Kafka bus defaults for module consumers and producers, so that I can finely tune performance and behaviour.   Such properties should include - autoCommitEnabled,queueSize,maxWait,fetchSize for consumers - batchSize,batchTimeout for producers""""""",,3.0
1,XD-3162,FEATURE,Done,MEDIUM,"""Update Master Environment for 2.0 CI Acceptance Tests""","""Update Master Environment for 2.0 CI Acceptance Tests""","""Update Master Environment for 2.0 CI Acceptance Tests""",,3.0
1,XD-3161,FEATURE,Done,MEDIUM,"""Add CI Acceptance Test for 1.2.x""","""Need acceptance tests to run on the 1.2.X branch.  Needs to be setup as a child of the Publish 1.2.x""","""""""Need acceptance tests to run on the 1.2.X branch.  Needs to be setup as a child of the Publish 1.2.x""""""",,3.0
1,XD-3160,FEATURE,Done,MEDIUM,"""Reorganize OOTB module list in docs""","""Sort alphabetically, nest """"Available modules"""" section appropriately. Optionally, move to a whole different """"PART"""" in reference doc""","""""""Sort alphabetically, nest """"""""Available modules"""""""" section appropriately. Optionally, move to a whole different """"""""PART"""""""" in reference doc""""""",,2.0
1,XD-3157,FEATURE,Done,HIGH,"""How to get in the module's container ID, and module id""","""As a developer, I in the new development of component (source/processor/sink), how to get the module id and container id  Because components need to generate log, log information must include the unique identifier   xd:>runtime modules ""","""""""As a developer, I in the new development of component (source/processor/sink), how to get the module id and container id  Because components need to generate log, log information must include the unique identifier   xd:>runtime modules """"""",,1.0
1,XD-3155,BUG,Done,MEDIUM,"""MessageBusSupport loads classes using the wrong ClassLoader""","""Use Spring XD modules with Spring Boot devtools the following SpEL errors occur:  {noformat} Caused by: org.springframework.expression.spel.SpelEvaluationException: EL1004E:(pos 8): Method call: Method accept(demo.Vote) cannot be found on demo.CounterApplication$$EnhancerBySpringCGLIB$$8b6c5177 type     at org.springframework.expression.spel.ast.MethodReference.findAccessorForMethod(MethodReference.java:211)     at org.springframework.expression.spel.ast.MethodReference.getValueInternal(MethodReference.java:125)     at org.springframework.expression.spel.ast.MethodReference.access$000(MethodReference.java:49)     at org.springframework.expression.spel.ast.MethodReference$MethodValueRef.getValue(MethodReference.java:342) ... {noformat}  The root cause of the error is that SpEL is using a different classloader to compare argument times to the one that loaded the object.  The actual incoming object is loaded via  {{MessageBusSupport}} which is calling {{Class.forName}}. If {{ClassUtils.forName}} is used instead then the context classloader is used and everything appears to work.""","""""""Use Spring XD modules with Spring Boot devtools the following SpEL errors occur:    The root cause of the error is that SpEL is using a different classloader to compare argument times to the one that loaded the object.  The actual incoming object is loaded via  {{MessageBusSupport}} which is calling {{Class.forName}}. If {{ClassUtils.forName}} is used instead then the context classloader is used and everything appears to work.""""""",""" Caused by: org.springframework.expression.spel.SpelEvaluationException: EL1004E:(pos 8): Method call: Method accept(demo.Vote) cannot be found on demo.CounterApplication$$EnhancerBySpringCGLIB$$8b6c5177 type     at org.springframework.expression.spel.ast.MethodReference.findAccessorForMethod(MethodReference.java:211)     at org.springframework.expression.spel.ast.MethodReference.getValueInternal(MethodReference.java:125)     at org.springframework.expression.spel.ast.MethodReference.access$000(MethodReference.java:49)     at org.springframework.expression.spel.ast.MethodReference$MethodValueRef.getValue(MethodReference.java:342) ... """,0.0
1,XD-3154,FEATURE,Done,MEDIUM,"""Update to Spring Hadoop 2.2.0 GA ""","""As a developer, I'd like to update to Spring Hadoop 2.2.0 GA release, so I can leverage the latest improvements. ""","""""""As a developer, I'd like to update to Spring Hadoop 2.2.0 GA release, so I can leverage the latest improvements. """"""",,1.0
1,XD-3153,FEATURE,Done,MEDIUM,"""Update to Spring Integration Kafka 1.2.0 GA""","""As a developer, I'd like to update to SI Kafka extension 1.2.0, so I can leverage the latest performance improvements.""","""""""As a developer, I'd like to update to SI Kafka extension 1.2.0, so I can leverage the latest performance improvements.""""""",,1.0
1,XD-3152,FEATURE,Done,MEDIUM,"""Update to Spring Integration 4.1.5 ""","""As a developer, I'd like to update to the 4.1.5 SI release, so I can pickup the latest improvements to message channels.""","""""""As a developer, I'd like to update to the 4.1.5 SI release, so I can pickup the latest improvements to message channels.""""""",,1.0
1,XD-3151,FEATURE,Done,MEDIUM,"""Update Modules - Build and Package sections""","""Some info is obsolete and add more content re. dependency management""","""""""Some info is obsolete and add more content re. dependency management""""""",,1.0
1,XD-3150,BUG,Done,URGENT,"""the 'filepollhdfs' job fails on second submission""","""Definitions:  >job create pollHdfs --definition """"filepollhdfs --names=name,age"""" --deploy true  >stream create csvStream --definition """"file --mode=ref --dir=/Users/trisberg/Test/files --pattern=*.csv > queue:job:pollHdfs"""" --deploy  Here is the exception:  {code} org.springframework.data.hadoop.store.StoreException: Error while flushing stream; nested exception is java.nio.channels.ClosedChannelException  at org.springframework.xd.batch.item.hadoop.HdfsTextItemWriter.update(HdfsTextItemWriter.java:135)  at org.springframework.batch.item.support.CompositeItemStream.update(CompositeItemStream.java:74)  at org.springframework.batch.core.step.tasklet.TaskletStep.doExecute(TaskletStep.java:250)  at org.springframework.batch.core.step.AbstractStep.execute(AbstractStep.java:198)  at org.springframework.batch.core.job.SimpleStepHandler.handleStep(SimpleStepHandler.java:148)  at org.springframework.batch.core.job.flow.JobFlowExecutor.executeStep(JobFlowExecutor.java:64)  at org.springframework.batch.core.job.flow.support.state.StepState.handle(StepState.java:67)  at org.springframework.batch.core.job.flow.support.SimpleFlow.resume(SimpleFlow.java:165)  at org.springframework.batch.core.job.flow.support.SimpleFlow.start(SimpleFlow.java:144)  at org.springframework.batch.core.job.flow.FlowJob.doExecute(FlowJob.java:134)  at org.springframework.batch.core.job.AbstractJob.execute(AbstractJob.java:304)  at org.springframework.batch.core.launch.support.SimpleJobLauncher$1.run(SimpleJobLauncher.java:135)  at org.springframework.core.task.SyncTaskExecutor.execute(SyncTaskExecutor.java:50)  at org.springframework.batch.core.launch.support.SimpleJobLauncher.run(SimpleJobLauncher.java:128)  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)  at java.lang.reflect.Method.invoke(Method.java:606)  at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)  at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)  at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)  at org.springframework.batch.core.configuration.annotation.SimpleBatchConfiguration$PassthruAdvice.invoke(SimpleBatchConfiguration.java:127)  at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)  at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)  at com.sun.proxy.$Proxy54.run(Unknown Source)  at org.springframework.batch.integration.launch.JobLaunching {code}""","""""""Definitions:  >job create pollHdfs --definition """"""""filepollhdfs --names=name,age"""""""" --deploy true  >stream create csvStream --definition """"""""file --mode=ref --dir=/Users/trisberg/Test/files --pattern=*.csv > queue:job:pollHdfs"""""""" --deploy  Here is the exception:  """"""",""" org.springframework.data.hadoop.store.StoreException: Error while flushing stream; nested exception is java.nio.channels.ClosedChannelException  at org.springframework.xd.batch.item.hadoop.HdfsTextItemWriter.update(HdfsTextItemWriter.java:135)  at org.springframework.batch.item.support.CompositeItemStream.update(CompositeItemStream.java:74)  at org.springframework.batch.core.step.tasklet.TaskletStep.doExecute(TaskletStep.java:250)  at org.springframework.batch.core.step.AbstractStep.execute(AbstractStep.java:198)  at org.springframework.batch.core.job.SimpleStepHandler.handleStep(SimpleStepHandler.java:148)  at org.springframework.batch.core.job.flow.JobFlowExecutor.executeStep(JobFlowExecutor.java:64)  at org.springframework.batch.core.job.flow.support.state.StepState.handle(StepState.java:67)  at org.springframework.batch.core.job.flow.support.SimpleFlow.resume(SimpleFlow.java:165)  at org.springframework.batch.core.job.flow.support.SimpleFlow.start(SimpleFlow.java:144)  at org.springframework.batch.core.job.flow.FlowJob.doExecute(FlowJob.java:134)  at org.springframework.batch.core.job.AbstractJob.execute(AbstractJob.java:304)  at org.springframework.batch.core.launch.support.SimpleJobLauncher$1.run(SimpleJobLauncher.java:135)  at org.springframework.core.task.SyncTaskExecutor.execute(SyncTaskExecutor.java:50)  at org.springframework.batch.core.launch.support.SimpleJobLauncher.run(SimpleJobLauncher.java:128)  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)  at java.lang.reflect.Method.invoke(Method.java:606)  at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)  at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)  at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)  at org.springframework.batch.core.configuration.annotation.SimpleBatchConfiguration$PassthruAdvice.invoke(SimpleBatchConfiguration.java:127)  at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)  at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)  at com.sun.proxy.$Proxy54.run(Unknown Source)  at org.springframework.batch.integration.launch.JobLaunching """,3.0
1,XD-3149,FEATURE,Done,MEDIUM,"""Batch job filepollhdfs docs are outdated""","""The stream definition example uses old style syntax, should be --mode=ref instead of --ref=true ""","""""""The stream definition example uses old style syntax, should be --mode=ref instead of --ref=true """"""",,1.0
1,XD-3148,FEATURE,Done,URGENT,"""Remove mr1 jar from cdh5 hadoop build""","""There is an hadoop-core-2.5.0-mr1-cdh5.3.3.jar in the lib/cdh5 directory - we need to remove that from the dist""","""""""There is an hadoop-core-2.5.0-mr1-cdh5.3.3.jar in the lib/cdh5 directory - we need to remove that from the dist""""""",,3.0
1,XD-3147,FEATURE,Done,HIGH,"""Composing Jobs via the DSL""","""h2. Narrative As a developer, I want to be able to construct jobs using a DSL similar to the current syntax for streams.  h2.  Back story Streams currently provide a DSL for assembling modules into flows (streams) that consist of a source, n processors, and a sink.  While constructing the steps of jobs themselves would be difficult in this manor, creating flows of jobs (essentially a job that consists only of job steps) would be very useful.  It would allow a developer to create something like the following:  {code} filejdbc | mycustomjob | jdbchdfs {code}  This approach also allows the existing packaging/module registry/etc to work out of the box.  This gets us closer to what Oozie provides out of the box without the need to create custom jobs to do the orchestration.""","""""""h2. Narrative As a developer, I want to be able to construct jobs using a DSL similar to the current syntax for streams.  h2.  Back story Streams currently provide a DSL for assembling modules into flows (streams) that consist of a source, n processors, and a sink.  While constructing the steps of jobs themselves would be difficult in this manor, creating flows of jobs (essentially a job that consists only of job steps) would be very useful.  It would allow a developer to create something like the following:    This approach also allows the existing packaging/module registry/etc to work out of the box.  This gets us closer to what Oozie provides out of the box without the need to create custom jobs to do the orchestration.""""""",""" filejdbc | mycustomjob | jdbchdfs """,8.0
1,XD-3142,BUG,Done,MEDIUM,"""Enable/Disable Boot and Integration MBeans when JMX is enabled/disabled""","""Spring Integration MBeans are enabled by default even though XD_JMX_ENABLED is set to false. We need to disable JMX on these MBeans as well as Spring Boot MBeans.""","""""""Spring Integration MBeans are enabled by default even though XD_JMX_ENABLED is set to false. We need to disable JMX on these MBeans as well as Spring Boot MBeans.""""""",,1.0
1,XD-3141,BUG,Done,URGENT,"""Uploaded custom module requires restart to get in effect""","""When a custom module is uploaded to module registry, though the module registry is updated with the changed module after deleting the existing one, the module changes aren't loaded when deployed.""","""""""When a custom module is uploaded to module registry, though the module registry is updated with the changed module after deleting the existing one, the module changes aren't loaded when deployed.""""""",,5.0
1,XD-3140,FEATURE,Done,MEDIUM,"""Create a landing page with links for all OOTB modules""","""As a user, I'd like to have a landing page with higher-order links for sources, processors, sinks and jobs, so I can jump to right section from one place. ""","""""""As a user, I'd like to have a landing page with higher-order links for sources, processors, sinks and jobs, so I can jump to right section from one place. """"""",,1.0
1,XD-3139,FEATURE,Done,MEDIUM,"""Document the new analytics tab features""","""As a user, I'd like to refer to the analytics tab docs, so I can understand how to use various widgets from streaming pipeline.  ""","""""""As a user, I'd like to refer to the analytics tab docs, so I can understand how to use various widgets from streaming pipeline.  """"""",,2.0
1,XD-3138,FEATURE,Done,MEDIUM,"""Better classloader strategy for XD admin/container""","""This is in reference to the investigation done as part of XD-2548""","""""""This is in reference to the investigation done as part of XD-2548""""""",,8.0
1,XD-3137,FEATURE,Done,MEDIUM,"""Upgrade to 3.1.1 of the Gradle Artifactory Plugin ""","""This addresses The plugin issue https://www.jfrog.com/jira/browse/GAP-172 to disable spring-xd/pom.xml""","""""""This addresses The plugin issue https://www.jfrog.com/jira/browse/GAP-172 to disable spring-xd/pom.xml""""""",,1.0
1,XD-3136,BUG,Done,MEDIUM,"""Example hashtag-count MR job fails when running XD on YARN with PHD 3.0""","""Running XD on YARN on PHD 3.0 Ambari install.  Uploading and submitting a custom job fails with the following:  {code} 2015-06-02 16:54:15,580 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: Diagnostics report from attempt_1433273561345_0009_m_000000_0: Error: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.springframework.xd.examples.hadoop.HashtagCount$TokenizerMapper not found  at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2076)  at org.apache.hadoop.mapreduce.task.JobContextImpl.getMapperClass(JobContextImpl.java:186)  at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:742)  at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)  at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)  at java.security.AccessController.doPrivileged(Native Method)  at javax.security.auth.Subject.doAs(Subject.java:415)  at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)  at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158) Caused by: java.lang.ClassNotFoundException: Class org.springframework.xd.examples.hadoop.HashtagCount$TokenizerMapper not found  at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:1982)  at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2074)  ... 8 more {code}  Same example jar works fine when submitted from XD cluster.""","""""""Running XD on YARN on PHD 3.0 Ambari install.  Uploading and submitting a custom job fails with the following:    Same example jar works fine when submitted from XD cluster.""""""",""" 2015-06-02 16:54:15,580 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: Diagnostics report from attempt_1433273561345_0009_m_000000_0: Error: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.springframework.xd.examples.hadoop.HashtagCount$TokenizerMapper not found  at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2076)  at org.apache.hadoop.mapreduce.task.JobContextImpl.getMapperClass(JobContextImpl.java:186)  at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:742)  at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)  at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)  at java.security.AccessController.doPrivileged(Native Method)  at javax.security.auth.Subject.doAs(Subject.java:415)  at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)  at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158) Caused by: java.lang.ClassNotFoundException: Class org.springframework.xd.examples.hadoop.HashtagCount$TokenizerMapper not found  at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:1982)  at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2074)  ... 8 more """,5.0
1,XD-3135,BUG,Done,MEDIUM,"""Spark streaming module includes logback jar when using dist zip""","""When running spark streaming module on spark standalone cluster from XD distribution, I see the following error:  [Stage 3:=============================>                             (1 + 1) / 2]2015-06-02T10:05:53-0700 1.2.0.SNAP WARN task-result-getter-3 scheduler.TaskSetManager - Lost task 0.0 in stage 3.0 (TID 50, 192.168.2.8): java.lang.IllegalArgumentException: LoggerFactory is not a Logback LoggerContext but Logback is on the classpath. Either remove Logback or the competing implementation (class org.slf4j.impl.Log4jLoggerFactory loaded from file:/Users/igopinatha/workspace/git/ilayaperumalg/spark/assembly/target/scala-2.10/spark-assembly-1.2.1-hadoop2.2.0.jar). If you are using Weblogic you will need to add 'org.slf4j' to prefer-application-packages in WEB-INF/weblogic.xml Object of class [org.slf4j.impl.Log4jLoggerFactory] must be an instance of class ch.qos.logback.classic.LoggerContext      at org.springframework.util.Assert.isInstanceOf(Assert.java:339)      at org.springframework.boot.logging.logback.LogbackLoggingSystem.getLoggerContext(LogbackLoggingSystem.java:151)      at org.springframework.boot.logging.logback.LogbackLoggingSystem.getLogger(LogbackLoggingSystem.java:143)      at org.springframework.boot.logging.logback.LogbackLoggingSystem.beforeInitialize(LogbackLoggingSystem.java:89)      at org.springframework.boot.logging.LoggingApplicationListener.onApplicationStartedEvent(LoggingApplicationListener.java:152)      at org.springframework.boot.logging.LoggingApplicationListener.onApplicationEvent(LoggingApplicationListener.java:139)      at org.springframework.context.event.SimpleApplicationEventMulticaster.invokeListener(SimpleApplicationEventMulticaster.java:151)      at org.springframework.context.event.SimpleApplicationEventMulticaster.multicastEvent(SimpleApplicationEventMulticaster.java:128)      at org.springframework.boot.context.event.EventPublishingRunListener.publishEvent(EventPublishingRunListener.java:100)      at org.springframework.boot.context.event.EventPublishingRunListener.started(EventPublishingRunListener.java:54)      at org.springframework.boot.SpringApplication.run(SpringApplication.java:277)      at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:139)      at org.springframework.xd.dirt.plugins.spark.streaming.MessageBusConfiguration.createApplicationContext(MessageBusConfiguration.java:82)      at org.springframework.xd.dirt.plugins.spark.streaming.MessageBusSender.start(MessageBusSender.java:105)      at org.springframework.xd.spark.streaming.java.ModuleExecutor$1$1.call(ModuleExecutor.java:58)      at org.springframework.xd.spark.streaming.java.ModuleExecutor$1$1.call(ModuleExecutor.java:53) ""","""""""When running spark streaming module on spark standalone cluster from XD distribution, I see the following error:  [Stage 3:=============================>                             (1 + 1) / 2]2015-06-02T10:05:53-0700 1.2.0.SNAP WARN task-result-getter-3 scheduler.TaskSetManager - Lost task 0.0 in stage 3.0 (TID 50, 192.168.2.8): java.lang.IllegalArgumentException: LoggerFactory is not a Logback LoggerContext but Logback is on the classpath. Either remove Logback or the competing implementation (class org.slf4j.impl.Log4jLoggerFactory loaded from file:/Users/igopinatha/workspace/git/ilayaperumalg/spark/assembly/target/scala-2.10/spark-assembly-1.2.1-hadoop2.2.0.jar). If you are using Weblogic you will need to add 'org.slf4j' to prefer-application-packages in WEB-INF/weblogic.xml Object of class [org.slf4j.impl.Log4jLoggerFactory] must be an instance of class ch.qos.logback.classic.LoggerContext      at org.springframework.util.Assert.isInstanceOf(Assert.java:339)      at org.springframework.boot.logging.logback.LogbackLoggingSystem.getLoggerContext(LogbackLoggingSystem.java:151)      at org.springframework.boot.logging.logback.LogbackLoggingSystem.getLogger(LogbackLoggingSystem.java:143)      at org.springframework.boot.logging.logback.LogbackLoggingSystem.beforeInitialize(LogbackLoggingSystem.java:89)      at org.springframework.boot.logging.LoggingApplicationListener.onApplicationStartedEvent(LoggingApplicationListener.java:152)      at org.springframework.boot.logging.LoggingApplicationListener.onApplicationEvent(LoggingApplicationListener.java:139)      at org.springframework.context.event.SimpleApplicationEventMulticaster.invokeListener(SimpleApplicationEventMulticaster.java:151)      at org.springframework.context.event.SimpleApplicationEventMulticaster.multicastEvent(SimpleApplicationEventMulticaster.java:128)      at org.springframework.boot.context.event.EventPublishingRunListener.publishEvent(EventPublishingRunListener.java:100)      at org.springframework.boot.context.event.EventPublishingRunListener.started(EventPublishingRunListener.java:54)      at org.springframework.boot.SpringApplication.run(SpringApplication.java:277)      at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:139)      at org.springframework.xd.dirt.plugins.spark.streaming.MessageBusConfiguration.createApplicationContext(MessageBusConfiguration.java:82)      at org.springframework.xd.dirt.plugins.spark.streaming.MessageBusSender.start(MessageBusSender.java:105)      at org.springframework.xd.spark.streaming.java.ModuleExecutor$1$1.call(ModuleExecutor.java:58)      at org.springframework.xd.spark.streaming.java.ModuleExecutor$1$1.call(ModuleExecutor.java:53) """"""",,2.0
1,XD-3133,BUG,Done,MEDIUM,"""Update YARN deployment classpath settings for HDP 2.2 and PHD 3.0""","""Need to update classpath settings for PHD 3.0 and HDP 2.2 ""","""""""Need to update classpath settings for PHD 3.0 and HDP 2.2 """"""",,1.0
1,XD-3132,BUG,Done,HIGH,"""Sqoop job doesn't run when deployed on YARN on Ambari deployed HDP""","""Got this error when submitting Sqoop job:  {code} 2015-06-01 19:09:42,932 INFO [main] sqoop.SqoopRunner (SqoopRunner.java:main(75)) - Sqoop command: import 2015-06-01 19:09:42,939 INFO [main] sqoop.SqoopRunner (SqoopRunner.java:main(76)) - Using args: [--table, XD_JOB_REGISTRY, --target-dir, /xd/sqoop2, -m=1] 2015-06-01 19:09:42,939 INFO [main] sqoop.SqoopRunner (SqoopRunner.java:main(77)) - Mapreduce home: /usr/hdp/2.2.4.2-2/hadoop-mapreduce 2015-06-01 19:09:42,977 WARN [main] conf.Configuration (Configuration.java:(646)) - DEPRECATED: hadoop-site.xml found in the classpath. Usage of hadoop-site.xml is deprecated. Instead use core-site.xml, mapred-site.xml and hdfs-site.xml to override properties of core-default.xml, mapred-default.xml and hdfs-default.xml respectively 2015-06-01 19:09:42,984 INFO [main] sqoop.SqoopRunner (SqoopRunner.java:setConfigurationProperty(168)) - Setting configuration property: fs.defaultFS=hdfs://hawaii:8020 2015-06-01 19:09:43,743 INFO [main] sqoop.SqoopRunner (SqoopRunner.java:setConfigurationProperty(168)) - Setting configuration property: yarn.resourcemanager.hostname=hawaii 2015-06-01 19:09:43,758 INFO [main] sqoop.SqoopRunner (SqoopRunner.java:setConfigurationProperty(168)) - Setting configuration property: yarn.resourcemanager.address=hawaii:8050 2015-06-01 19:09:43,758 INFO [main] sqoop.SqoopRunner (SqoopRunner.java:setConfigurationProperty(168)) - Setting configuration property: yarn.resourcemanager.scheduler.address=hawaii 2015-06-01 19:09:43,758 INFO [main] sqoop.SqoopRunner (SqoopRunner.java:setConfigurationProperty(168)) - Setting configuration property: mapreduce.framework.name=yarn 2015-06-01 19:09:43,758 INFO [main] sqoop.SqoopRunner (SqoopRunner.java:setConfigurationProperty(168)) - Setting configuration property: mapreduce.jobhistory.address=hawaii 2015-06-01 19:09:43,760 INFO [main] sqoop.SqoopRunner (SqoopRunner.java:createConfiguration(158)) - Setting configuration property: yarn.resourcemanager.scheduler.address=hawaii:8030 2015-06-01 19:09:43,760 INFO [main] sqoop.SqoopRunner (SqoopRunner.java:createConfiguration(158)) - Setting configuration property: mapreduce.application.classpath=$PWD/mr-framework/hadoop/share/hadoop/mapreduce/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/lib/*:$PWD/mr-framework/hadoop/share/hadoop/common/*:$PWD/mr-framework/hadoop/share/hadoop/common/lib/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/lib/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/lib/*:/usr/hdp/2.2.4.2-2/hadoop/lib/hadoop-lzo-0.6.0.2.2.4.2-2.jar:/etc/hadoop/conf/secure 2015-06-01 19:09:43,760 INFO [main] sqoop.SqoopRunner (SqoopRunner.java:createConfiguration(158)) - Setting configuration property: mapreduce.application.framework.path=/hdp/apps/2.2.4.2-2/mapreduce/mapreduce.tar.gz#mr-framework 2015-06-01 19:09:44,141 INFO [main] sqoop.Sqoop (Sqoop.java:(92)) - Running Sqoop version: 1.4.5 2015-06-01 19:09:44,691 INFO [main] manager.SqlManager (SqlManager.java:initOptionDefaults(98)) - Using default fetchSize of 1000 2015-06-01 19:09:44,691 INFO [main] tool.CodeGenTool (CodeGenTool.java:generateORM(92)) - Beginning code generation 2015-06-01 19:09:45,057 INFO [main] manager.SqlManager (SqlManager.java:execute(749)) - Executing SQL statement: SELECT t.* FROM XD_JOB_REGISTRY AS t WHERE 1=0 2015-06-01 19:09:45,074 INFO [main] manager.SqlManager (SqlManager.java:execute(749)) - Executing SQL statement: SELECT t.* FROM XD_JOB_REGISTRY AS t WHERE 1=0 2015-06-01 19:09:45,148 INFO [main] orm.CompilationManager (CompilationManager.java:findHadoopJars(94)) - HADOOP_MAPRED_HOME is /usr/hdp/2.2.4.2-2/hadoop-mapreduce 2015-06-01 19:09:45,230 ERROR [main] orm.CompilationManager (CompilationManager.java:compile(184)) - It seems as though you are running sqoop with a JRE. 2015-06-01 19:09:45,230 ERROR [main] orm.CompilationManager (CompilationManager.java:compile(185)) - Sqoop requires a JDK that can compile Java code. 2015-06-01 19:09:45,230 ERROR [main] orm.CompilationManager (CompilationManager.java:compile(186)) - Please install a JDK and set $JAVA_HOME to use it. 2015-06-01 19:09:45,232 ERROR [main] tool.ImportTool (ImportTool.java:run(609)) - Encountered IOException running import job: java.io.IOException: Could not start Java compiler. at org.apache.sqoop.orm.CompilationManager.compile(CompilationManager.java:187) at org.apache.sqoop.tool.CodeGenTool.generateORM(CodeGenTool.java:97) at org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:478) at org.apache.sqoop.tool.ImportTool.run(ImportTool.java:601) at org.apache.sqoop.Sqoop.run(Sqoop.java:143) at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70) at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:179) at org.apache.sqoop.Sqoop.runTool(Sqoop.java:218) at org.springframework.xd.sqoop.SqoopRunner.main(SqoopRunner.java:87) {code} ""","""""""Got this error when submitting Sqoop job:   """"""",""" 2015-06-01 19:09:42,932 INFO [main] sqoop.SqoopRunner (SqoopRunner.java:main(75)) - Sqoop command: import 2015-06-01 19:09:42,939 INFO [main] sqoop.SqoopRunner (SqoopRunner.java:main(76)) - Using args: [--table, XD_JOB_REGISTRY, --target-dir, /xd/sqoop2, -m=1] 2015-06-01 19:09:42,939 INFO [main] sqoop.SqoopRunner (SqoopRunner.java:main(77)) - Mapreduce home: /usr/hdp/2.2.4.2-2/hadoop-mapreduce 2015-06-01 19:09:42,977 WARN [main] conf.Configuration (Configuration.java:(646)) - DEPRECATED: hadoop-site.xml found in the classpath. Usage of hadoop-site.xml is deprecated. Instead use core-site.xml, mapred-site.xml and hdfs-site.xml to override properties of core-default.xml, mapred-default.xml and hdfs-default.xml respectively 2015-06-01 19:09:42,984 INFO [main] sqoop.SqoopRunner (SqoopRunner.java:setConfigurationProperty(168)) - Setting configuration property: fs.defaultFS=hdfs://hawaii:8020 2015-06-01 19:09:43,743 INFO [main] sqoop.SqoopRunner (SqoopRunner.java:setConfigurationProperty(168)) - Setting configuration property: yarn.resourcemanager.hostname=hawaii 2015-06-01 19:09:43,758 INFO [main] sqoop.SqoopRunner (SqoopRunner.java:setConfigurationProperty(168)) - Setting configuration property: yarn.resourcemanager.address=hawaii:8050 2015-06-01 19:09:43,758 INFO [main] sqoop.SqoopRunner (SqoopRunner.java:setConfigurationProperty(168)) - Setting configuration property: yarn.resourcemanager.scheduler.address=hawaii 2015-06-01 19:09:43,758 INFO [main] sqoop.SqoopRunner (SqoopRunner.java:setConfigurationProperty(168)) - Setting configuration property: mapreduce.framework.name=yarn 2015-06-01 19:09:43,758 INFO [main] sqoop.SqoopRunner (SqoopRunner.java:setConfigurationProperty(168)) - Setting configuration property: mapreduce.jobhistory.address=hawaii 2015-06-01 19:09:43,760 INFO [main] sqoop.SqoopRunner (SqoopRunner.java:createConfiguration(158)) - Setting configuration property: yarn.resourcemanager.scheduler.address=hawaii:8030 2015-06-01 19:09:43,760 INFO [main] sqoop.SqoopRunner (SqoopRunner.java:createConfiguration(158)) - Setting configuration property: mapreduce.application.classpath=$PWD/mr-framework/hadoop/share/hadoop/mapreduce/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/lib/*:$PWD/mr-framework/hadoop/share/hadoop/common/*:$PWD/mr-framework/hadoop/share/hadoop/common/lib/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/lib/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/lib/*:/usr/hdp/2.2.4.2-2/hadoop/lib/hadoop-lzo-0.6.0.2.2.4.2-2.jar:/etc/hadoop/conf/secure 2015-06-01 19:09:43,760 INFO [main] sqoop.SqoopRunner (SqoopRunner.java:createConfiguration(158)) - Setting configuration property: mapreduce.application.framework.path=/hdp/apps/2.2.4.2-2/mapreduce/mapreduce.tar.gz#mr-framework 2015-06-01 19:09:44,141 INFO [main] sqoop.Sqoop (Sqoop.java:(92)) - Running Sqoop version: 1.4.5 2015-06-01 19:09:44,691 INFO [main] manager.SqlManager (SqlManager.java:initOptionDefaults(98)) - Using default fetchSize of 1000 2015-06-01 19:09:44,691 INFO [main] tool.CodeGenTool (CodeGenTool.java:generateORM(92)) - Beginning code generation 2015-06-01 19:09:45,057 INFO [main] manager.SqlManager (SqlManager.java:execute(749)) - Executing SQL statement: SELECT t.* FROM XD_JOB_REGISTRY AS t WHERE 1=0 2015-06-01 19:09:45,074 INFO [main] manager.SqlManager (SqlManager.java:execute(749)) - Executing SQL statement: SELECT t.* FROM XD_JOB_REGISTRY AS t WHERE 1=0 2015-06-01 19:09:45,148 INFO [main] orm.CompilationManager (CompilationManager.java:findHadoopJars(94)) - HADOOP_MAPRED_HOME is /usr/hdp/2.2.4.2-2/hadoop-mapreduce 2015-06-01 19:09:45,230 ERROR [main] orm.CompilationManager (CompilationManager.java:compile(184)) - It seems as though you are running sqoop with a JRE. 2015-06-01 19:09:45,230 ERROR [main] orm.CompilationManager (CompilationManager.java:compile(185)) - Sqoop requires a JDK that can compile Java code. 2015-06-01 19:09:45,230 ERROR [main] orm.CompilationManager (CompilationManager.java:compile(186)) - Please install a JDK and set $JAVA_HOME to use it. 2015-06-01 19:09:45,232 ERROR [main] tool.ImportTool (ImportTool.java:run(609)) - Encountered IOException running import job: java.io.IOException: Could not start Java compiler. at org.apache.sqoop.orm.CompilationManager.compile(CompilationManager.java:187) at org.apache.sqoop.tool.CodeGenTool.generateORM(CodeGenTool.java:97) at org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:478) at org.apache.sqoop.tool.ImportTool.run(ImportTool.java:601) at org.apache.sqoop.Sqoop.run(Sqoop.java:143) at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70) at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:179) at org.apache.sqoop.Sqoop.runTool(Sqoop.java:218) at org.springframework.xd.sqoop.SqoopRunner.main(SqoopRunner.java:87) """,3.0
1,XD-3131,BUG,Done,MEDIUM,"""Spark streaming plugin shouldn't need tap listener cache""","""Since Spark streaming doesn't use ZK to keep track taps being created, we don't need the tap listener cache setup at the container startup.""","""""""Since Spark streaming doesn't use ZK to keep track taps being created, we don't need the tap listener cache setup at the container startup.""""""",,1.0
1,XD-3130,BUG,Done,MEDIUM,"""Move Bus cleaner util method from BusUtils""","""Since `Spark-streaming` uses `BusUtils`, we need to move the bus cleaner util method that builds rest template so that spark streaming doesn't depend on `httpClient`""","""""""Since `Spark-streaming` uses `BusUtils`, we need to move the bus cleaner util method that builds rest template so that spark streaming doesn't depend on `httpClient`""""""",,1.0
1,XD-3127,FEATURE,Done,MEDIUM,"""Create a landing section for OOTB batch jobs""","""As a user, I'd like to refer to OOTB batch jobs and the documentation, so I can jump to the right job section and review details. ""","""""""As a user, I'd like to refer to OOTB batch jobs and the documentation, so I can jump to the right job section and review details. """"""",,1.0
1,XD-3126,FEATURE,Done,MEDIUM,"""Support for registering custom Kryo Serializers""","""This is an enhancement to KryoClassRegistrar or a related mechanism to initialize codecs using custom serializers to improve serialization performance. Currently XD will support POJOs that implement the kryo Serializable interface to gain a 2x performance improvement, however initial benchmarks show that custom serializers are about 10% more performant than Serializable.""","""""""This is an enhancement to KryoClassRegistrar or a related mechanism to initialize codecs using custom serializers to improve serialization performance. Currently XD will support POJOs that implement the kryo Serializable interface to gain a 2x performance improvement, however initial benchmarks show that custom serializers are about 10% more performant than Serializable.""""""",,3.0
1,XD-3125,FEATURE,Done,MEDIUM,"""Fail fast on Kryo registration conflicts""","""Currently kryo class registration is hard coded in spring-xd-codec. Users may register their own classes using an extension mechanism, but it is possible to conflict with internal XD class registration, e.g., Tuple. Exposing this using the same extension mechanism will make it more transparent. ""","""""""Currently kryo class registration is hard coded in spring-xd-codec. Users may register their own classes using an extension mechanism, but it is possible to conflict with internal XD class registration, e.g., Tuple. Exposing this using the same extension mechanism will make it more transparent. """"""",,2.0
1,XD-3124,BUG,Done,URGENT,"""`minPartitionCount` is ignored by the consumer""","""`minPartitionCount` is ignored by the consumer, so downstream modules end up listening to fewer partitions""","""""""`minPartitionCount` is ignored by the consumer, so downstream modules end up listening to fewer partitions""""""",,3.0
1,XD-3123,BUG,Done,MEDIUM,"""Prevent classloader leakage thru javabeans infrastructure""","""Prevent classloader leakage thru javabeans infrastructure""","""Prevent classloader leakage thru javabeans infrastructure""",,1.0
1,XD-3118,FEATURE,Done,MEDIUM,"""Update RPM script to include number of containers to be started""","""As a user, I'd like to start multiple instances of {{xd-container}}'s through the RPM scripts, so I can easily spin-up instances on the same node/vm.""","""""""As a user, I'd like to start multiple instances of {{xd-container}}'s through the RPM scripts, so I can easily spin-up instances on the same node/vm.""""""",,2.0
1,XD-3117,FEATURE,Done,MEDIUM,"""Add Logging to ZooKeeperContainerRepository""","""Occasional CI test build failures:  {quote} Caused by: java.lang.IllegalStateException: Container cache not initialized (likely as a result of a ZooKeeper connection error)  at org.springframework.util.Assert.state(Assert.java:385)  at org.springframework.xd.dirt.container.store.ZooKeeperContainerRepository.ensureCache(ZooKeeperContainerRepository.java:184)  at org.springframework.xd.dirt.container.store.ZooKeeperContainerRepository.findOne(ZooKeeperContainerRepository.java:263) {quote}  e.g. https://build.spring.io/browse/XD-JDK8-JOB1-1514  Add logging to {{ensureCache()}} (e.g. in {{childEvent()}} ) and {{closeCache()}} to log that the cache was closed; it appears that's the only way the """"cache not initialized"""" message can be emitted.""","""""""Occasional CI test build failures:  {quote} Caused by: java.lang.IllegalStateException: Container cache not initialized (likely as a result of a ZooKeeper connection error)  at org.springframework.util.Assert.state(Assert.java:385)  at org.springframework.xd.dirt.container.store.ZooKeeperContainerRepository.ensureCache(ZooKeeperContainerRepository.java:184)  at org.springframework.xd.dirt.container.store.ZooKeeperContainerRepository.findOne(ZooKeeperContainerRepository.java:263) {quote}  e.g. https://build.spring.io/browse/XD-JDK8-JOB1-1514  Add logging to {{ensureCache()}} (e.g. in {{childEvent()}} ) and {{closeCache()}} to log that the cache was closed; it appears that's the only way the """"""""cache not initialized"""""""" message can be emitted.""""""",,1.0
1,XD-3116,FEATURE,Done,MEDIUM,"""Update redis.tar.gz bundled in distribution to be version 3.0.1""","""RPM scripts will need to change.""","""""""RPM scripts will need to change.""""""",,3.0
1,XD-3115,FEATURE,Done,MEDIUM,"""Improve ReactorReflectionUtils.extractGeneric to support classes with inheritance""","""Provide unit tests""","""""""Provide unit tests""""""",,1.0
1,XD-3114,FEATURE,Done,MEDIUM,"""Ensure proper lifecycle shutdown of processors in BroadcasterMessageHandler and MultipleBroadcasterMessageHandler ""","""Ensure proper lifecycle shutdown of processors in BroadcasterMessageHandler and MultipleBroadcasterMessageHandler ""","""Ensure proper lifecycle shutdown of processors in BroadcasterMessageHandler and MultipleBroadcasterMessageHandler """,,2.0
1,XD-3113,FEATURE,Done,MEDIUM,"""Review 'critical' sonar warning...""","""https://sonar.spring.io/drilldown/issues/org.springframework.xd:spring-xd?severity=CRITICAL""","""""""https://sonar.spring.io/drilldown/issues/org.springframework.xd:spring-xd?severity=CRITICAL""""""",,3.0
1,XD-3112,FEATURE,Done,MEDIUM,"""Hide the passwords used as module properties in streams from being displayed.""","""This type is used in password field in the jdbc sink module provided by Spring XD (defined in org.springframework.xd.jdbc.JdbcConnectionMixin class). It seems that Spring XD Admin UI is always displaying the password in plain text please see attached screen shot. Is there a way to somehow hide the passwords used as module properties in streams from being displayed in Spring XD Admin UI? This is similar issue as below and fixed in batch jobs but not in streams. https://github.com/spring-projects/spring-xd/pull/1325""","""""""This type is used in password field in the jdbc sink module provided by Spring XD (defined in org.springframework.xd.jdbc.JdbcConnectionMixin class). It seems that Spring XD Admin UI is always displaying the password in plain text please see attached screen shot. Is there a way to somehow hide the passwords used as module properties in streams from being displayed in Spring XD Admin UI? This is similar issue as below and fixed in batch jobs but not in streams. https://github.com/spring-projects/spring-xd/pull/1325""""""",,2.0
1,XD-3111,FEATURE,Done,MEDIUM,"""Upgrade to 1.2.0 RC1 SIK release""","""Upgrade to 1.2.0 RC1 SIK release""","""Upgrade to 1.2.0 RC1 SIK release""",,1.0
1,XD-3110,FEATURE,Done,MEDIUM,"""Clean-up compiler and javadoc warnings from the build""","""As a developer, I'd like to clean-up compiler and javadoc warnings from the build, so we don't see  the warnings in build sysout.""","""""""As a developer, I'd like to clean-up compiler and javadoc warnings from the build, so we don't see  the warnings in build sysout.""""""",,2.0
1,XD-3109,BUG,Done,MEDIUM,"""SFTP socket closed error. Infinite loop""","""Having the follow messages poping up on xd log. It seems they are being generated indefinitely.   Log files getting huge.   [2015-05-27 15:57:51.039] boot - 2774  INFO [task-scheduler-1] --- jsch: kex: server: ssh-rsa,ssh-dss [2015-05-27 15:57:51.039] boot - 2774  INFO [task-scheduler-1] --- jsch: kex: server: aes256-ctr,aes192-ctr,aes128-ctr,arcfour256 [2015-05-27 15:57:51.039] boot - 2774  INFO [task-scheduler-1] --- jsch: kex: server: aes256-ctr,aes192-ctr,aes128-ctr,arcfour256 [2015-05-27 15:57:51.039] boot - 2774  INFO [task-scheduler-1] --- jsch: kex: server: hmac-sha2-512,hmac-sha2-256,hmac-sha1,hmac-ripemd160 [2015-05-27 15:57:51.039] boot - 2774  INFO [task-scheduler-1] --- jsch: kex: server: hmac-sha2-512,hmac-sha2-256,hmac-sha1,hmac-ripemd160 [2015-05-27 15:57:51.039] boot - 2774  INFO [task-scheduler-1] --- jsch: kex: server: none,<EMAIL> [2015-05-27 15:57:51.039] boot - 2774  INFO [task-scheduler-1] --- jsch: kex: server: none,<EMAIL> [2015-05-27 15:57:51.039] boot - 2774  INFO [task-scheduler-1] --- jsch: kex: server: [2015-05-27 15:57:51.039] boot - 2774  INFO [task-scheduler-1] --- jsch: kex: server: [2015-05-27 15:57:51.040] boot - 2774  INFO [task-scheduler-1] --- jsch: kex: client: diffie-hellman-group1-sha1,diffie-hellman-group-exchange-sha1 [2015-05-27 15:57:51.040] boot - 2774  INFO [task-scheduler-1] --- jsch: kex: client: ssh-rsa,ssh-dss [2015-05-27 15:57:51.040] boot - 2774  INFO [task-scheduler-1] --- jsch: kex: client: aes128-ctr,aes128-cbc,3des-ctr,3des-cbc,blowfish-cbc [2015-05-27 15:57:51.040] boot - 2774  INFO [task-scheduler-1] --- jsch: kex: client: aes128-ctr,aes128-cbc,3des-ctr,3des-cbc,blowfish-cbc [2015-05-27 15:57:51.040] boot - 2774  INFO [task-scheduler-1] --- jsch: kex: client: hmac-md5,hmac-sha1,hmac-sha2-256,hmac-sha1-96,hmac-md5-96 [2015-05-27 15:57:51.040] boot - 2774  INFO [task-scheduler-1] --- jsch: kex: client: hmac-md5,hmac-sha1,hmac-sha2-256,hmac-sha1-96,hmac-md5-96 [2015-05-27 15:57:51.040] boot - 2774  INFO [task-scheduler-1] --- jsch: kex: client: none [2015-05-27 15:57:51.040] boot - 2774  INFO [task-scheduler-1] --- jsch: kex: client: none [2015-05-27 15:57:51.040] boot - 2774  INFO [task-scheduler-1] --- jsch: kex: client: [2015-05-27 15:57:51.040] boot - 2774  INFO [task-scheduler-1] --- jsch: kex: client: [2015-05-27 15:57:51.040] boot - 2774  INFO [task-scheduler-1] --- jsch: kex: server->client aes128-ctr hmac-sha1 none [2015-05-27 15:57:51.040] boot - 2774  INFO [task-scheduler-1] --- jsch: kex: client->server aes128-ctr hmac-sha1 none [2015-05-27 15:57:51.044] boot - 2774  INFO [task-scheduler-1] --- jsch: SSH_MSG_KEXDH_INIT sent [2015-05-27 15:57:51.044] boot - 2774  INFO [task-scheduler-1] --- jsch: expecting SSH_MSG_KEXDH_REPLY [2015-05-27 15:57:51.049] boot - 2774  INFO [task-scheduler-1] --- jsch: ssh_rsa_verify: signature true [2015-05-27 15:57:51.049] boot - 2774  INFO [task-scheduler-1] --- jsch: Host 'XX.XXX.XX.X' is known and mathces the RSA host key [2015-05-27 15:57:51.049] boot - 2774  INFO [task-scheduler-1] --- jsch: SSH_MSG_NEWKEYS sent [2015-05-27 15:57:51.049] boot - 2774  INFO [task-scheduler-1] --- jsch: SSH_MSG_NEWKEYS received [2015-05-27 15:57:51.050] boot - 2774  INFO [task-scheduler-1] --- jsch: SSH_MSG_SERVICE_REQUEST sent [2015-05-27 15:57:51.050] boot - 2774  INFO [task-scheduler-1] --- jsch: SSH_MSG_SERVICE_ACCEPT received [2015-05-27 15:57:51.052] boot - 2774  INFO [task-scheduler-1] --- jsch: Authentications that can continue: gssapi-with-mic,publickey,keyboard-interactive,password [2015-05-27 15:57:51.052] boot - 2774  INFO [task-scheduler-1] --- jsch: Next authentication method: gssapi-with-mic [2015-05-27 15:57:51.054] boot - 2774  INFO [task-scheduler-1] --- jsch: Authentications that can continue: publickey,keyboard-interactive,password [2015-05-27 15:57:51.054] boot - 2774  INFO [task-scheduler-1] --- jsch: Next authentication method: publickey [2015-05-27 15:57:51.086] boot - 2774  INFO [task-scheduler-1] --- jsch: Authentication succeeded (publickey). [2015-05-27 15:57:51.113] boot - 2774  INFO [task-scheduler-1] --- jsch: Disconnecting from 10.100.103.5 port 22 [2015-05-27 15:57:51.113] boot - 2774  INFO [Connect thread XX.XXX.XXX.X session] --- jsch: Caught an exception, leaving main loop due to Socket closed""","""""""Having the follow messages poping up on xd log. It seems they are being generated indefinitely.   Log files getting huge.   [2015-05-27 15:57:51.039] boot - 2774  INFO [task-scheduler-1] --- jsch: kex: server: ssh-rsa,ssh-dss [2015-05-27 15:57:51.039] boot - 2774  INFO [task-scheduler-1] --- jsch: kex: server: aes256-ctr,aes192-ctr,aes128-ctr,arcfour256 [2015-05-27 15:57:51.039] boot - 2774  INFO [task-scheduler-1] --- jsch: kex: server: aes256-ctr,aes192-ctr,aes128-ctr,arcfour256 [2015-05-27 15:57:51.039] boot - 2774  INFO [task-scheduler-1] --- jsch: kex: server: hmac-sha2-512,hmac-sha2-256,hmac-sha1,hmac-ripemd160 [2015-05-27 15:57:51.039] boot - 2774  INFO [task-scheduler-1] --- jsch: kex: server: hmac-sha2-512,hmac-sha2-256,hmac-sha1,hmac-ripemd160 [2015-05-27 15:57:51.039] boot - 2774  INFO [task-scheduler-1] --- jsch: kex: server: none,<EMAIL> [2015-05-27 15:57:51.039] boot - 2774  INFO [task-scheduler-1] --- jsch: kex: server: none,<EMAIL> [2015-05-27 15:57:51.039] boot - 2774  INFO [task-scheduler-1] --- jsch: kex: server: [2015-05-27 15:57:51.039] boot - 2774  INFO [task-scheduler-1] --- jsch: kex: server: [2015-05-27 15:57:51.040] boot - 2774  INFO [task-scheduler-1] --- jsch: kex: client: diffie-hellman-group1-sha1,diffie-hellman-group-exchange-sha1 [2015-05-27 15:57:51.040] boot - 2774  INFO [task-scheduler-1] --- jsch: kex: client: ssh-rsa,ssh-dss [2015-05-27 15:57:51.040] boot - 2774  INFO [task-scheduler-1] --- jsch: kex: client: aes128-ctr,aes128-cbc,3des-ctr,3des-cbc,blowfish-cbc [2015-05-27 15:57:51.040] boot - 2774  INFO [task-scheduler-1] --- jsch: kex: client: aes128-ctr,aes128-cbc,3des-ctr,3des-cbc,blowfish-cbc [2015-05-27 15:57:51.040] boot - 2774  INFO [task-scheduler-1] --- jsch: kex: client: hmac-md5,hmac-sha1,hmac-sha2-256,hmac-sha1-96,hmac-md5-96 [2015-05-27 15:57:51.040] boot - 2774  INFO [task-scheduler-1] --- jsch: kex: client: hmac-md5,hmac-sha1,hmac-sha2-256,hmac-sha1-96,hmac-md5-96 [2015-05-27 15:57:51.040] boot - 2774  INFO [task-scheduler-1] --- jsch: kex: client: none [2015-05-27 15:57:51.040] boot - 2774  INFO [task-scheduler-1] --- jsch: kex: client: none [2015-05-27 15:57:51.040] boot - 2774  INFO [task-scheduler-1] --- jsch: kex: client: [2015-05-27 15:57:51.040] boot - 2774  INFO [task-scheduler-1] --- jsch: kex: client: [2015-05-27 15:57:51.040] boot - 2774  INFO [task-scheduler-1] --- jsch: kex: server->client aes128-ctr hmac-sha1 none [2015-05-27 15:57:51.040] boot - 2774  INFO [task-scheduler-1] --- jsch: kex: client->server aes128-ctr hmac-sha1 none [2015-05-27 15:57:51.044] boot - 2774  INFO [task-scheduler-1] --- jsch: SSH_MSG_KEXDH_INIT sent [2015-05-27 15:57:51.044] boot - 2774  INFO [task-scheduler-1] --- jsch: expecting SSH_MSG_KEXDH_REPLY [2015-05-27 15:57:51.049] boot - 2774  INFO [task-scheduler-1] --- jsch: ssh_rsa_verify: signature true [2015-05-27 15:57:51.049] boot - 2774  INFO [task-scheduler-1] --- jsch: Host 'XX.XXX.XX.X' is known and mathces the RSA host key [2015-05-27 15:57:51.049] boot - 2774  INFO [task-scheduler-1] --- jsch: SSH_MSG_NEWKEYS sent [2015-05-27 15:57:51.049] boot - 2774  INFO [task-scheduler-1] --- jsch: SSH_MSG_NEWKEYS received [2015-05-27 15:57:51.050] boot - 2774  INFO [task-scheduler-1] --- jsch: SSH_MSG_SERVICE_REQUEST sent [2015-05-27 15:57:51.050] boot - 2774  INFO [task-scheduler-1] --- jsch: SSH_MSG_SERVICE_ACCEPT received [2015-05-27 15:57:51.052] boot - 2774  INFO [task-scheduler-1] --- jsch: Authentications that can continue: gssapi-with-mic,publickey,keyboard-interactive,password [2015-05-27 15:57:51.052] boot - 2774  INFO [task-scheduler-1] --- jsch: Next authentication method: gssapi-with-mic [2015-05-27 15:57:51.054] boot - 2774  INFO [task-scheduler-1] --- jsch: Authentications that can continue: publickey,keyboard-interactive,password [2015-05-27 15:57:51.054] boot - 2774  INFO [task-scheduler-1] --- jsch: Next authentication method: publickey [2015-05-27 15:57:51.086] boot - 2774  INFO [task-scheduler-1] --- jsch: Authentication succeeded (publickey). [2015-05-27 15:57:51.113] boot - 2774  INFO [task-scheduler-1] --- jsch: Disconnecting from 10.100.103.5 port 22 [2015-05-27 15:57:51.113] boot - 2774  INFO [Connect thread XX.XXX.XXX.X session] --- jsch: Caught an exception, leaving main loop due to Socket closed""""""",,2.0
1,XD-3107,FEATURE,Done,MEDIUM,"""Fix gradle build issues""","""As a XD build master, I'd like to fix (local ./gradlew dist and distZip targets) the outstanding build issues, so I can evaluate that publish builds works as expected. ""","""""""As a XD build master, I'd like to fix (local ./gradlew dist and distZip targets) the outstanding build issues, so I can evaluate that publish builds works as expected. """"""",,3.0
1,XD-3106,FEATURE,Done,MEDIUM,"""Support XD_JMX_ENABLED configuration""","""XD-EC2 needs  to allow user to set the XD_JMX_ENABLED flag in the environment prior to admin or container  startups. ""","""""""XD-EC2 needs  to allow user to set the XD_JMX_ENABLED flag in the environment prior to admin or container  startups. """"""",,2.0
1,XD-3105,FEATURE,Done,MEDIUM,"""Turn-off JMX by default""","""As a developer, I'd like to have JMX turned-off by default, so I can take advantage of the performance throughput benefits. ""","""""""As a developer, I'd like to have JMX turned-off by default, so I can take advantage of the performance throughput benefits. """"""",,1.0
1,XD-3104,FEATURE,Done,MEDIUM,"""Update to Reactor 2.0.3 ""","""Update to Reactor 2.0.3 ""","""Update to Reactor 2.0.3 """,,1.0
1,XD-3103,FEATURE,Done,MEDIUM,"""Adapt to XD Reactor processor fixes and improvements""","""As a developer, I'd like to upgrade to 2.0.3 release of Reactor, so I can inherit the latest optimizations to further improve XD performance characteristics. ""","""""""As a developer, I'd like to upgrade to 2.0.3 release of Reactor, so I can inherit the latest optimizations to further improve XD performance characteristics. """"""",,3.0
1,XD-3102,FEATURE,Done,MEDIUM,"""Benchmark XD RC1 using Kafka 0.8.2 as transport""","""As a developer, I'd like to rerun _baseline_, _Tuple_, and _Serialized_ payloads, so I can compare the differences in performance between 0.8.1 and 0.8.2 Kafka releases.   Sinks to be included in test: In-Memory Transport > Hdfs sink Direct Binding Transport > Hdfs Sink Kafka > Hdfs Sink""","""""""As a developer, I'd like to rerun _baseline_, _Tuple_, and _Serialized_ payloads, so I can compare the differences in performance between 0.8.1 and 0.8.2 Kafka releases.   Sinks to be included in test: In-Memory Transport > Hdfs sink Direct Binding Transport > Hdfs Sink Kafka > Hdfs Sink""""""",,8.0
1,XD-3101,FEATURE,Done,MEDIUM,"""Fix Gradle dist build task""","""Fix Gradle dist build task""","""Fix Gradle dist build task""",,1.0
1,XD-3100,BUG,Done,URGENT,"""module.*.count > 1 duplicates messages on taps""","""Using module.name.count > 1 when deploying taps causes duplication of messages in those modules. This impacts balancing of the containers and modules in a cluster as messages should not be duplicated across modules if the same module is deployed twice to two containers in order to spread the load.  We use taps quite heavily in our project mainly for analytics of the life feed in real time but due to issue we have discovered and described in this bug we are currently facing a limitation where heavily processing modules can not be load balanced across the cluster as they are causing duplication of the messages and therefore the same module deployed to two  containers would still process the same message twice.  To demonstrate the problem please see test case scenario set up below:  h4. 1. Environment  - Spring-XD version 1.1.1-RELEASE - Running two spring-xd containers and one spring-xd admin  h4. 2. Set up  Stream definition is as follows:   {quote}stream create --name test-module-count --definition """"syslog-udp --port=5140 | transform | log"""" stream deploy --name test-module-count --properties """"module.*.count=2"""" stream create --name tap-test-module-count --definition """"tap:stream:test-module-count.syslog-udp > transform --expression='payload.toString() + \""""TAPPED\""""' | log"""" stream deploy --name tap-test-module-count --properties """"module.*.count=2""""{quote}   Please refer to the screen shots attached to see that after deploying those two streams we have:  - streams successfully deployed ( module-count-spring-xd-streams.png ) - streams successfully deployed with count=2 to both containers ( module-count-spring-xd-containers.png )  - 5 queues created in Rabbit ( module-count-rabbit.png ) where two were created for the syslog-udp collector as a result of using module.syslog-udp.count=2 - this is causing messages to be duplicated. Normally the expectation would be to have only one queue for the tap  h4. 3. Test input data  I have sent a very simple UDP message to the listening udp collector running on second container:   {quote}echo test-module-count >> /dev/udp/host02/5140{quote}  h4. 4. Test output data in the logs ( module-count-container01.log and module-count-container02.log )  h5. Expected result:  Below messages logged only on 1 container (it does not matter which one) {quote}2015-05-26 09:52:21,630 1.1.1.RELEASE  INFO xdbus.test-module-count.1-1 sink.test-module-count - {UNDECODED=test-module-count}{quote} Below message logged only on one container (it does not matter which one)  {quote}2015-05-26 09:52:21,843 1.1.1.RELEASE  INFO xdbus.tap-test-module-count.0-1 sink.tap-test-module-count - {UNDECODED=test-module-count }TAPPED{quote}  h5. Actual result:  Stream that has been create as a tap has duplicated the same message and as a result the same message was proccessed twice on both containers by the same module ( transformer ) and logged twice to the console on both containers  Container01: {quote}2015-05-26 14:52:21,143 1.1.1.RELEASE  INFO xdbus.tap-test-module-count.0-1 sink.tap-test-module-count - {UNDECODED=test-module-count }TAPPED{quote}  Container02: {quote}2015-05-26 09:52:21,630 1.1.1.RELEASE  INFO xdbus.test-module-count.1-1 sink.test-module-count - {UNDECODED=test-module-count } 2015-05-26 09:52:21,843 1.1.1.RELEASE  INFO xdbus.tap-test-module-count.0-1 sink.tap-test-module-count - {UNDECODED=test-module-count }TAPPED{quote}   ""","""""""Using module.name.count > 1 when deploying taps causes duplication of messages in those modules. This impacts balancing of the containers and modules in a cluster as messages should not be duplicated across modules if the same module is deployed twice to two containers in order to spread the load.  We use taps quite heavily in our project mainly for analytics of the life feed in real time but due to issue we have discovered and described in this bug we are currently facing a limitation where heavily processing modules can not be load balanced across the cluster as they are causing duplication of the messages and therefore the same module deployed to two  containers would still process the same message twice.  To demonstrate the problem please see test case scenario set up below:  h4. 1. Environment  - Spring-XD version 1.1.1-RELEASE - Running two spring-xd containers and one spring-xd admin  h4. 2. Set up  Stream definition is as follows:   {quote}stream create --name test-module-count --definition """"""""syslog-udp --port=5140 | transform | log"""""""" stream deploy --name test-module-count --properties """"""""module.*.count=2"""""""" stream create --name tap-test-module-count --definition """"""""tap:stream:test-module-count.syslog-udp > transform --expression='payload.toString() + \""""""""TAPPED\""""""""' | log"""""""" stream deploy --name tap-test-module-count --properties """"""""module.*.count=2""""""""{quote}   Please refer to the screen shots attached to see that after deploying those two streams we have:  - streams successfully deployed ( module-count-spring-xd-streams.png ) - streams successfully deployed with count=2 to both containers ( module-count-spring-xd-containers.png )  - 5 queues created in Rabbit ( module-count-rabbit.png ) where two were created for the syslog-udp collector as a result of using module.syslog-udp.count=2 - this is causing messages to be duplicated. Normally the expectation would be to have only one queue for the tap  h4. 3. Test input data  I have sent a very simple UDP message to the listening udp collector running on second container:   {quote}echo test-module-count >> /dev/udp/host02/5140{quote}  h4. 4. Test output data in the logs ( module-count-container01.log and module-count-container02.log )  h5. Expected result:  Below messages logged only on 1 container (it does not matter which one) {quote}2015-05-26 09:52:21,630 1.1.1.RELEASE  INFO xdbus.test-module-count.1-1 sink.test-module-count - {UNDECODED=test-module-count}{quote} Below message logged only on one container (it does not matter which one)  {quote}2015-05-26 09:52:21,843 1.1.1.RELEASE  INFO xdbus.tap-test-module-count.0-1 sink.tap-test-module-count - {UNDECODED=test-module-count }TAPPED{quote}  h5. Actual result:  Stream that has been create as a tap has duplicated the same message and as a result the same message was proccessed twice on both containers by the same module ( transformer ) and logged twice to the console on both containers  Container01: {quote}2015-05-26 14:52:21,143 1.1.1.RELEASE  INFO xdbus.tap-test-module-count.0-1 sink.tap-test-module-count - {UNDECODED=test-module-count }TAPPED{quote}  Container02: {quote}2015-05-26 09:52:21,630 1.1.1.RELEASE  INFO xdbus.test-module-count.1-1 sink.test-module-count - {UNDECODED=test-module-count } 2015-05-26 09:52:21,843 1.1.1.RELEASE  INFO xdbus.tap-test-module-count.0-1 sink.tap-test-module-count - {UNDECODED=test-module-count }TAPPED{quote}   """"""",,5.0
1,XD-3098,FEATURE,Done,MEDIUM,"""Message Bus optimizations (Kafka + Redis)""","""Message Bus optimizations (Kafka + Redis)""","""Message Bus optimizations (Kafka + Redis)""",,5.0
1,XD-3096,FEATURE,Done,MEDIUM,"""Support for BroadcasterMessageHandler to work with concurrent producing threads""","""Also provide better lifecycle (shutdown) mgmt of handler.""","""""""Also provide better lifecycle (shutdown) mgmt of handler.""""""",,3.0
1,XD-3095,FEATURE,Done,MEDIUM,"""Update to Reactor 2.0.2""","""Update to Reactor 2.0.2""","""Update to Reactor 2.0.2""",,1.0
1,XD-3094,BUG,Done,MEDIUM,"""SparkApp batch job is not running""","""From the latest master, I couldn't run sparkApp as batch job. The spark application process gets launched and it doesn't complete and there are no errors at the output.""","""""""From the latest master, I couldn't run sparkApp as batch job. The spark application process gets launched and it doesn't complete and there are no errors at the output.""""""",,2.0
1,XD-3093,BUG,Done,MEDIUM,"""Sqoop list-tables doesn't work oob""","""Commands from docs:  xd:>job create sqoopListTables --definition """"sqoop --command=list-tables"""" --deploy xd:>job launch --name sqoopListTables  2015-05-21 19:12:36,211 1.2.0.M1 ERROR task-scheduler-1 sqoop.SqoopTasklet - Sqoop job for 'list-tables' finished with exit code: 1 2015-05-21 19:12:36,212 1.2.0.M1 ERROR task-scheduler-1 sqoop.SqoopTasklet - Sqoop err: Error: Required argument --connect is missing.  Adding --connect results  xd:>job create sqoopListTables --definition """"sqoop --command=list-tables --connect=jdbc:hsqldb:hsql://localhost:9101/xdjob"""" --deploy Command failed org.springframework.xd.rest.client.impl.SpringXDException: Error with option(s) for module sqoop of type job:     connect: option named 'connect' is not supported   This is with singlenode.""","""""""Commands from docs:  xd:>job create sqoopListTables --definition """"""""sqoop --command=list-tables"""""""" --deploy xd:>job launch --name sqoopListTables  2015-05-21 19:12:36,211 1.2.0.M1 ERROR task-scheduler-1 sqoop.SqoopTasklet - Sqoop job for 'list-tables' finished with exit code: 1 2015-05-21 19:12:36,212 1.2.0.M1 ERROR task-scheduler-1 sqoop.SqoopTasklet - Sqoop err: Error: Required argument --connect is missing.  Adding --connect results  xd:>job create sqoopListTables --definition """"""""sqoop --command=list-tables --connect=jdbc:hsqldb:hsql://localhost:9101/xdjob"""""""" --deploy Command failed org.springframework.xd.rest.client.impl.SpringXDException: Error with option(s) for module sqoop of type job:     connect: option named 'connect' is not supported   This is with singlenode.""""""",,1.0
1,XD-3092,FEATURE,Done,MEDIUM,"""Synchronous deployment/undeployments""","""There are a range of issues (such as XD-3083, XD-2671) that are caused by asynchronous deployments issued by the REST API. The flow of events is: * deploy/undeploy request received by REST API * controller queues up request to be processed by supervisor * controller returns HTTP 2xx  This proposal is to have the thread executing the deploy/undeploy request block until the request has been processed by the supervisor. This will have the side effect of deploys appearing to take longer, but when the HTTP request completes, the deployment/undeployment will have been fulfilled. ""","""""""There are a range of issues (such as XD-3083, XD-2671) that are caused by asynchronous deployments issued by the REST API. The flow of events is: * deploy/undeploy request received by REST API * controller queues up request to be processed by supervisor * controller returns HTTP 2xx  This proposal is to have the thread executing the deploy/undeploy request block until the request has been processed by the supervisor. This will have the side effect of deploys appearing to take longer, but when the HTTP request completes, the deployment/undeployment will have been fulfilled. """"""",,2.0
1,XD-3091,FEATURE,Done,MEDIUM,"""Update build to use SHDP 2.2.0.RC1""","""Update build to use SHDP 2.2.0.RC1""","""Update build to use SHDP 2.2.0.RC1""",,1.0
1,XD-3090,BUG,Done,HIGH,"""JdbcHdfsTests sporadically fail""","""Acceptance tests sporadically fail after https://github.com/spring-projects/spring-xd/pull/1623 was merged XD-2309.  Additional tests were added but used fixed timeouts.  Will replace them with waitForJob.   ""","""""""Acceptance tests sporadically fail after https://github.com/spring-projects/spring-xd/pull/1623 was merged XD-2309.  Additional tests were added but used fixed timeouts.  Will replace them with waitForJob.   """"""",,2.0
1,XD-3089,FEATURE,Done,MEDIUM,"""Add incremental load feature to batch docs""","""The incremental load introduced with XD-2309 should be added to the batch docs""","""""""The incremental load introduced with XD-2309 should be added to the batch docs""""""",,1.0
1,XD-3086,BUG,Done,MEDIUM,"""Fix random Spark streaming test failures""","""The {{testTapSparkProcessor}} has the test that checks the contents at the output of spark streaming word count processor. It turns out that the order in which these messages are processed are not always in order.""","""""""The {{testTapSparkProcessor}} has the test that checks the contents at the output of spark streaming word count processor. It turns out that the order in which these messages are processed are not always in order.""""""",,1.0
1,XD-3083,BUG,Done,MEDIUM,"""Creating multiple Stream/Job definitions from command file is broken""","""I have a file that has the DSLs: stream create a1 --definition """"http | log"""" stream deploy a1  xd-shell --cmdfile test.cmd May 19, 2015 1:49:29 PM org.springframework.shell.core.AbstractShell handleExecutionResult INFO: Created new stream 'a1' May 19, 2015 1:49:29 PM org.springframework.shell.core.SimpleExecutionStrategy invoke SEVERE: Command failed org.springframework.xd.rest.client.impl.SpringXDException: There is no stream definition named 'a1'""","""""""I have a file that has the DSLs: stream create a1 --definition """"""""http | log"""""""" stream deploy a1  xd-shell --cmdfile test.cmd May 19, 2015 1:49:29 PM org.springframework.shell.core.AbstractShell handleExecutionResult INFO: Created new stream 'a1' May 19, 2015 1:49:29 PM org.springframework.shell.core.SimpleExecutionStrategy invoke SEVERE: Command failed org.springframework.xd.rest.client.impl.SpringXDException: There is no stream definition named 'a1'""""""",,3.0
1,XD-3082,FEATURE,Done,MEDIUM,"""UI: Add Analytics Tab""","""UI: Add Analytics Tab""","""UI: Add Analytics Tab""",,3.0
1,XD-3081,BUG,Done,URGENT,"""When using file as a source and sink user can not use file sink --mode""","""Cluster Type: SingleNode Machine: Mac PR: https://github.com/spring-projects/spring-xd/pull/1624,https://github.com/spring-projects/spring-xd/pull/1626 Stream that reproduces the problem: {noformat} stream create foo --definition """"filein: file --dir=/tmp/xd/a0180520-c7fa-4d9d-8cc3-e36fbf59496a --pattern=de59d1b8-f99c-4c43-a8c0-2f6043546689.out --mode=contents | fileout: file --binary=true --mode=replace """" {noformat} Error Message: {noformat} Command failed org.springframework.xd.rest.client.impl.SpringXDException: Error with option(s) for module file of type sink:     mode: Failed to convert property value of type 'java.lang.String' to required type 'org.springframework.xd.dirt.modules.metadata.FileSinkOptionsMetadata$Mode' for property 'mode'; nested exception is java.lang.IllegalStateException: Cannot convert value of type [java.lang.String] to required type [org.springframework.xd.dirt.modules.metadata.FileSinkOptionsMetadata$Mode] for property 'mode': no matching editors or conversion strategy found {noformat} Stacktrace: {noformat} 2015-05-19 14:30:56,329 1.2.0.SNAP ERROR qtp671416633-35 rest.RestControllerAdvice - Caught exception while handling a request org.springframework.xd.dirt.plugins.ModuleConfigurationException: Error with option(s) for module file of type sink:     mode: Failed to convert property value of type 'java.lang.String' to required type 'org.springframework.xd.dirt.modules.metadata.FileSinkOptionsMetadata$Mode' for property 'mode'; nested exception is java.lang.IllegalStateException: Cannot convert value of type [java.lang.String] to required type [org.springframework.xd.dirt.modules.metadata.FileSinkOptionsMetadata$Mode] for property 'mode': no matching editors or conversion strategy found  at org.springframework.xd.dirt.plugins.ModuleConfigurationException.fromBindException(ModuleConfigurationException.java:55)  at org.springframework.xd.dirt.stream.XDStreamParser.buildModuleDescriptors(XDStreamParser.java:191)  at org.springframework.xd.dirt.stream.XDStreamParser.parse(XDStreamParser.java:122)  at org.springframework.xd.dirt.stream.AbstractDeployer.validateBeforeSave(AbstractDeployer.java:115)  at org.springframework.xd.dirt.rest.XDController.save(XDController.java:260)  at sun.reflect.GeneratedMethodAccessor191.invoke(Unknown Source)  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)  at java.lang.reflect.Method.invoke(Method.java:606)  at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:221)  at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:137)  at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:110)  at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandleMethod(RequestMappingHandlerAdapter.java:776)  at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:705)  at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:85)  at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:959)  at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:893)  at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:966)  at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:868)  at javax.servlet.http.HttpServlet.service(HttpServlet.java:755)  at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:842)  at javax.servlet.http.HttpServlet.service(HttpServlet.java:848)  at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:684)  at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1496)  at org.springframework.boot.actuate.autoconfigure.EndpointWebMvcAutoConfiguration$ApplicationContextHeaderFilter.doFilterInternal(EndpointWebMvcAutoConfiguration.java:291)  at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)  at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)  at org.springframework.web.filter.HiddenHttpMethodFilter.doFilterInternal(HiddenHttpMethodFilter.java:77)  at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)  at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)  at org.springframework.web.filter.HttpPutFormContentFilter.doFilterInternal(HttpPutFormContentFilter.java:87)  at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)  at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)  at org.springframework.boot.actuate.trace.WebRequestTraceFilter.doFilterInternal(WebRequestTraceFilter.java:102)  at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)  at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)  at org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:186)  at org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:160)  at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)  at org.springframework.boot.actuate.autoconfigure.MetricFilterAutoConfiguration$MetricsFilter.doFilterInternal(MetricFilterAutoConfiguration.java:90)  at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)  at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)  at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:499)  at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:137)  at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:557)  at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:231)  at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1086)  at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:428)  at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:193)  at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1020)  at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)  at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)  at org.eclipse.jetty.server.Server.handle(Server.java:370)  at org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:494)  at org.eclipse.jetty.server.AbstractHttpConnection.content(AbstractHttpConnection.java:982)  at org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.content(AbstractHttpConnection.java:1043)  at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:865)  at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:240)  at org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82)  at org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:667)  at org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:52)  at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)  at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)  at java.lang.Thread.run(Thread.java:745) Caused by: org.springframework.validation.BindException: org.springframework.validation.BeanPropertyBindingResult: 1 errors Field error in object 'target' on field 'mode': rejected value [replace]; codes [typeMismatch.target.mode,typeMismatch.mode,typeMismatch.org.springframework.xd.dirt.modules.metadata.FileSinkOptionsMetadata$Mode,typeMismatch]; arguments [org.springframework.context.support.DefaultMessageSourceResolvable: codes [target.mode,mode]; arguments []; default message [mode]]; default message [Failed to convert property value of type 'java.lang.String' to required type 'org.springframework.xd.dirt.modules.metadata.FileSinkOptionsMetadata$Mode' for property 'mode'; nested exception is java.lang.IllegalStateException: Cannot convert value of type [java.lang.String] to required type [org.springframework.xd.dirt.modules.metadata.FileSinkOptionsMetadata$Mode] for property 'mode': no matching editors or conversion strategy found]  at org.springframework.xd.module.options.PojoModuleOptionsMetadata.bindAndValidate(PojoModuleOptionsMetadata.java:205)  at org.springframework.xd.module.options.PojoModuleOptionsMetadata.interpolate(PojoModuleOptionsMetadata.java:139)  at org.springframework.xd.module.options.FlattenedCompositeModuleOptionsMetadata.interpolate(FlattenedCompositeModuleOptionsMetadata.java:152)  at org.springframework.xd.module.options.EnvironmentAwareModuleOptionsMetadataResolver$ModuleOptionsMetadataWithDefaults.interpolate(EnvironmentAwareModuleOptionsMetadataResolver.java:168)  at org.springframework.xd.dirt.stream.XDStreamParser.buildModuleDescriptors(XDStreamParser.java:188)  ... 61 more {noformat}""","""""""Cluster Type: SingleNode Machine: Mac PR: https://github.com/spring-projects/spring-xd/pull/1624,https://github.com/spring-projects/spring-xd/pull/1626 Stream that reproduces the problem:  Error Message:  Stacktrace: """"""",""" stream create foo --definition """"""""filein: file --dir=/tmp/xd/a0180520-c7fa-4d9d-8cc3-e36fbf59496a --pattern=de59d1b8-f99c-4c43-a8c0-2f6043546689.out --mode=contents | fileout: file --binary=true --mode=replace """"""""  Command failed org.springframework.xd.rest.client.impl.SpringXDException: Error with option(s) for module file of type sink:     mode: Failed to convert property value of type 'java.lang.String' to required type 'org.springframework.xd.dirt.modules.metadata.FileSinkOptionsMetadata$Mode' for property 'mode'; nested exception is java.lang.IllegalStateException: Cannot convert value of type [java.lang.String] to required type [org.springframework.xd.dirt.modules.metadata.FileSinkOptionsMetadata$Mode] for property 'mode': no matching editors or conversion strategy found  2015-05-19 14:30:56,329 1.2.0.SNAP ERROR qtp671416633-35 rest.RestControllerAdvice - Caught exception while handling a request org.springframework.xd.dirt.plugins.ModuleConfigurationException: Error with option(s) for module file of type sink:     mode: Failed to convert property value of type 'java.lang.String' to required type 'org.springframework.xd.dirt.modules.metadata.FileSinkOptionsMetadata$Mode' for property 'mode'; nested exception is java.lang.IllegalStateException: Cannot convert value of type [java.lang.String] to required type [org.springframework.xd.dirt.modules.metadata.FileSinkOptionsMetadata$Mode] for property 'mode': no matching editors or conversion strategy found  at org.springframework.xd.dirt.plugins.ModuleConfigurationException.fromBindException(ModuleConfigurationException.java:55)  at org.springframework.xd.dirt.stream.XDStreamParser.buildModuleDescriptors(XDStreamParser.java:191)  at org.springframework.xd.dirt.stream.XDStreamParser.parse(XDStreamParser.java:122)  at org.springframework.xd.dirt.stream.AbstractDeployer.validateBeforeSave(AbstractDeployer.java:115)  at org.springframework.xd.dirt.rest.XDController.save(XDController.java:260)  at sun.reflect.GeneratedMethodAccessor191.invoke(Unknown Source)  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)  at java.lang.reflect.Method.invoke(Method.java:606)  at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:221)  at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:137)  at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:110)  at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandleMethod(RequestMappingHandlerAdapter.java:776)  at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:705)  at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:85)  at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:959)  at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:893)  at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:966)  at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:868)  at javax.servlet.http.HttpServlet.service(HttpServlet.java:755)  at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:842)  at javax.servlet.http.HttpServlet.service(HttpServlet.java:848)  at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:684)  at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1496)  at org.springframework.boot.actuate.autoconfigure.EndpointWebMvcAutoConfiguration$ApplicationContextHeaderFilter.doFilterInternal(EndpointWebMvcAutoConfiguration.java:291)  at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)  at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)  at org.springframework.web.filter.HiddenHttpMethodFilter.doFilterInternal(HiddenHttpMethodFilter.java:77)  at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)  at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)  at org.springframework.web.filter.HttpPutFormContentFilter.doFilterInternal(HttpPutFormContentFilter.java:87)  at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)  at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)  at org.springframework.boot.actuate.trace.WebRequestTraceFilter.doFilterInternal(WebRequestTraceFilter.java:102)  at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)  at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)  at org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:186)  at org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:160)  at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)  at org.springframework.boot.actuate.autoconfigure.MetricFilterAutoConfiguration$MetricsFilter.doFilterInternal(MetricFilterAutoConfiguration.java:90)  at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)  at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)  at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:499)  at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:137)  at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:557)  at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:231)  at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1086)  at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:428)  at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:193)  at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1020)  at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)  at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)  at org.eclipse.jetty.server.Server.handle(Server.java:370)  at org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:494)  at org.eclipse.jetty.server.AbstractHttpConnection.content(AbstractHttpConnection.java:982)  at org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.content(AbstractHttpConnection.java:1043)  at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:865)  at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:240)  at org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82)  at org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:667)  at org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:52)  at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)  at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)  at java.lang.Thread.run(Thread.java:745) Caused by: org.springframework.validation.BindException: org.springframework.validation.BeanPropertyBindingResult: 1 errors Field error in object 'target' on field 'mode': rejected value [replace]; codes [typeMismatch.target.mode,typeMismatch.mode,typeMismatch.org.springframework.xd.dirt.modules.metadata.FileSinkOptionsMetadata$Mode,typeMismatch]; arguments [org.springframework.context.support.DefaultMessageSourceResolvable: codes [target.mode,mode]; arguments []; default message [mode]]; default message [Failed to convert property value of type 'java.lang.String' to required type 'org.springframework.xd.dirt.modules.metadata.FileSinkOptionsMetadata$Mode' for property 'mode'; nested exception is java.lang.IllegalStateException: Cannot convert value of type [java.lang.String] to required type [org.springframework.xd.dirt.modules.metadata.FileSinkOptionsMetadata$Mode] for property 'mode': no matching editors or conversion strategy found]  at org.springframework.xd.module.options.PojoModuleOptionsMetadata.bindAndValidate(PojoModuleOptionsMetadata.java:205)  at org.springframework.xd.module.options.PojoModuleOptionsMetadata.interpolate(PojoModuleOptionsMetadata.java:139)  at org.springframework.xd.module.options.FlattenedCompositeModuleOptionsMetadata.interpolate(FlattenedCompositeModuleOptionsMetadata.java:152)  at org.springframework.xd.module.options.EnvironmentAwareModuleOptionsMetadataResolver$ModuleOptionsMetadataWithDefaults.interpolate(EnvironmentAwareModuleOptionsMetadataResolver.java:168)  at org.springframework.xd.dirt.stream.XDStreamParser.buildModuleDescriptors(XDStreamParser.java:188)  ... 61 more """,3.0
1,XD-3080,IMPROVEMENT,Done,HIGH,"""Add support for cluster rebalancing""","""When deploying streams to clustered Spring XD modules are deployed to all avilable Spring XD containers (assuming no additional deployment properties are specificed). Each container gets similar number of modules.   When Spring XD container departs from the cluster all departing modules will be migrated to remaining containers. After container rejoins the cluster the modules are not redeployed to it and the cluster looks unbalanced and the node is not used.  !unbalanced-cluster.png!""","""""""When deploying streams to clustered Spring XD modules are deployed to all avilable Spring XD containers (assuming no additional deployment properties are specificed). Each container gets similar number of modules.   When Spring XD container departs from the cluster all departing modules will be migrated to remaining containers. After container rejoins the cluster the modules are not redeployed to it and the cluster looks unbalanced and the node is not used.  !unbalanced-cluster.png!""""""",,5.0
1,XD-3079,BUG,Done,HIGH,"""Create a new Kerberos ticket instead of renew the current one""","""Running Spring-XD singlenode with a kerberized hadoop cluster on CDH 5.3.2. with JDK 1.7 and JCE 1.7. The kerberos ticket policies are: * expiration: 24 hours * renew: 7 days  I need to keep the Spring XD server running constantly because my flows are always waiting for incoming files to be ingested into the HDFS, but the kerberos session expires if there aren't jobs to run before the expiration date. The expiration policies can't be changed due internal company policies.  Is there a way which Spring XD can generate a new ticket instead of renew the current one when a job or stream start executing?  The Spring XD server has configured the hadoop.properties like:  # Use servers.yml to change URI for namenode # You can add additional properties in this file dfs.namenode.kerberos.principal=hdfs/<EMAIL> yarn.resourcemanager.principal=yarn/<EMAIL>  yarn.application.classpath=/opt/cloudera/parcels/CDH/lib/hadoop/*,/opt/cloudera/parcels/CDH/lib/hadoop/lib/*,/opt/cloudera/parcels/CDH/lib/hadoop-hdfs/*,/opt/cloudera/parcels/CDH/lib/hadoop-hdfs/lib/*,/opt/cloudera/parcels/CDH/lib/hadoop-yarn/*,/opt/cloudera/parcels/CDH/lib/hadoop-yarn/lib/*,/opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/*,/opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/lib/*  hadoop.security.authorization=true hadoop.security.authentication=kerberos  spring.hadoop.userKeytab=file:///export/home/user/user.keytab spring.hadoop.userPrincipal=<EMAIL>  #Connecting to Kerberized Hadoop (Spring XD doc configuration Appendix D) spring.hadoop.security.authMethod=kerberos spring.hadoop.security.userKeytab=/export/home/user/user.keytab spring.hadoop.security.userPrincipal=<EMAIL> spring.hadoop.security.namenodePrincipal=hdfs/<EMAIL> spring.hadoop.security.rmManagerPrincipal=yarn/<EMAIL>""","""""""Running Spring-XD singlenode with a kerberized hadoop cluster on CDH 5.3.2. with JDK 1.7 and JCE 1.7. The kerberos ticket policies are: * expiration: 24 hours * renew: 7 days  I need to keep the Spring XD server running constantly because my flows are always waiting for incoming files to be ingested into the HDFS, but the kerberos session expires if there aren't jobs to run before the expiration date. The expiration policies can't be changed due internal company policies.  Is there a way which Spring XD can generate a new ticket instead of renew the current one when a job or stream start executing?  The Spring XD server has configured the hadoop.properties like:  # Use servers.yml to change URI for namenode # You can add additional properties in this file dfs.namenode.kerberos.principal=hdfs/<EMAIL> yarn.resourcemanager.principal=yarn/<EMAIL>  yarn.application.classpath=/opt/cloudera/parcels/CDH/lib/hadoop/*,/opt/cloudera/parcels/CDH/lib/hadoop/lib/*,/opt/cloudera/parcels/CDH/lib/hadoop-hdfs/*,/opt/cloudera/parcels/CDH/lib/hadoop-hdfs/lib/*,/opt/cloudera/parcels/CDH/lib/hadoop-yarn/*,/opt/cloudera/parcels/CDH/lib/hadoop-yarn/lib/*,/opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/*,/opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/lib/*  hadoop.security.authorization=true hadoop.security.authentication=kerberos  spring.hadoop.userKeytab=file:///export/home/user/user.keytab spring.hadoop.userPrincipal=<EMAIL>  #Connecting to Kerberized Hadoop (Spring XD doc configuration Appendix D) spring.hadoop.security.authMethod=kerberos spring.hadoop.security.userKeytab=/export/home/user/user.keytab spring.hadoop.security.userPrincipal=<EMAIL> spring.hadoop.security.namenodePrincipal=hdfs/<EMAIL> spring.hadoop.security.rmManagerPrincipal=yarn/<EMAIL>""""""",,5.0
1,XD-3078,BUG,Done,URGENT,"""Spring XD admin fails to redeploy modules after Spring XD container successfully reconnectes to Zookeeper""","""We are running Spring XD 1.1.1 in our production environment and Zookeeper 3.4.5.  Zookeeper is running in failover mode and consists of three independent nodes set up on three separate VMs. From time to time we get """"Connection to Zookeeper Suspended"""" event which causes one of the containers in the cluster to be removed from the SpringXD cluster. Modules being deployed on this removed node fail to be re-deployed to other containers in the cluster.  Affected versions: - SpringXD 1.1.1 - Zookeeper 3.4.5 and 3.4.6  Cluster set up in PROD environment where error occurs: - 4 Spring-XD dedicated servers - 4 spring-xd containers (each running on designated server ) - 2 spring-xd admins ( each running alongside one spring-xd container) - 3 Zookeeper nodes ( 3 designated servers on PAITO environment )  Cluster set up in TEST environment where error also occurred: - 2 Spring-XD dedicated servers running one spring-xd container and one spring-xd admin each - 3 Zookeeper nodes running on 3 dedicated servers (PAITO Test environment)  Cluster set up to reproduce error found in PROD environment: - 1 spring-xd admin - 3 spring xd-containers (each running on a designated VM ) - 3 zookeeper servers running on one VM  Steps to reproduce:  1) Set up three node Zookeeper cluster. Attached is example zoo.cfg, we are using default configuration values. In this particular test case we run all Zookeeper nodes on a single VM as we were not testing network layer interruptions. 2) Set up one Spring XD admin node. Please note that we have also observed this on two node Spring XD admin cluster.  3) Set up three Spring XD container nodes. All of them belong to one group (SA) and two of them also belong to second group (HA1). This is configured in $XD_HOME/config/servers.yml however so far group configuration never influenced test outcome. 4) Create and deploy a test stream using following XD Shell commands: stream create --name test-zookeeper-failover --definition """"syslog-udp --port=5140 | transform | file --dir='/opt/pivotal/spring-xd/xd/output'"""" stream deploy --name test-zookeeper-failover --properties """"module.syslog-udp.criteria=groups.contains('HA1'),module.syslog-udp.count=2,module.file.criteria=groups.contains('SA'),module.file.count=3,module.transform.criteria=groups.contains('SA')"""" 5) Ensure that test stream works and handles traffic on UDP port 5140 6) Shutdown one of the Zookeeper nodes by issuing a stop command. 7) Two Spring XD containers were not affected and remained in Spring XD cluster. 8) One Spring XD container was kicked out of Spring XD cluster and was no longer visible on Spring XD admin Web UI. Modules previously deployed to this container were not redeployed to other cluster members. 9) On the failed Spring XD container we have observed CONNECTION_SUSPEND, CONNECTION_RECONECTED and CHILD_REMOVE Zookeeper events (attached is container-log.txt). Please note that Java process is still running and we see ConnectionStateManager-0 server.ContainerRegistrar - Waiting for supervisor to clean up prior deployments messages. 10) Spring XD admin failed with exception in DepartingContainerModuleRedeployer (attached is admin-log.txt).  11) We have observed that departing container node in Zookeeper (/sa/deployments/modules/allocated/1d3fd4cc-5a70-47ed-b4f3-22deef1f4d4f/) had no children. We did this after few minutes so we are not sure at which point it was cleared.  12) Restarting failed Spring XD container fixed the problem, modules were correctly redeployed. Exception from point 10 is very similar to XD-1983 and this code was rewritten in XD-2004. ""","""""""We are running Spring XD 1.1.1 in our production environment and Zookeeper 3.4.5.  Zookeeper is running in failover mode and consists of three independent nodes set up on three separate VMs. From time to time we get """"""""Connection to Zookeeper Suspended"""""""" event which causes one of the containers in the cluster to be removed from the SpringXD cluster. Modules being deployed on this removed node fail to be re-deployed to other containers in the cluster.  Affected versions: - SpringXD 1.1.1 - Zookeeper 3.4.5 and 3.4.6  Cluster set up in PROD environment where error occurs: - 4 Spring-XD dedicated servers - 4 spring-xd containers (each running on designated server ) - 2 spring-xd admins ( each running alongside one spring-xd container) - 3 Zookeeper nodes ( 3 designated servers on PAITO environment )  Cluster set up in TEST environment where error also occurred: - 2 Spring-XD dedicated servers running one spring-xd container and one spring-xd admin each - 3 Zookeeper nodes running on 3 dedicated servers (PAITO Test environment)  Cluster set up to reproduce error found in PROD environment: - 1 spring-xd admin - 3 spring xd-containers (each running on a designated VM ) - 3 zookeeper servers running on one VM  Steps to reproduce:  1) Set up three node Zookeeper cluster. Attached is example zoo.cfg, we are using default configuration values. In this particular test case we run all Zookeeper nodes on a single VM as we were not testing network layer interruptions. 2) Set up one Spring XD admin node. Please note that we have also observed this on two node Spring XD admin cluster.  3) Set up three Spring XD container nodes. All of them belong to one group (SA) and two of them also belong to second group (HA1). This is configured in $XD_HOME/config/servers.yml however so far group configuration never influenced test outcome. 4) Create and deploy a test stream using following XD Shell commands: stream create --name test-zookeeper-failover --definition """"""""syslog-udp --port=5140 | transform | file --dir='/opt/pivotal/spring-xd/xd/output'"""""""" stream deploy --name test-zookeeper-failover --properties """"""""module.syslog-udp.criteria=groups.contains('HA1'),module.syslog-udp.count=2,module.file.criteria=groups.contains('SA'),module.file.count=3,module.transform.criteria=groups.contains('SA')"""""""" 5) Ensure that test stream works and handles traffic on UDP port 5140 6) Shutdown one of the Zookeeper nodes by issuing a stop command. 7) Two Spring XD containers were not affected and remained in Spring XD cluster. 8) One Spring XD container was kicked out of Spring XD cluster and was no longer visible on Spring XD admin Web UI. Modules previously deployed to this container were not redeployed to other cluster members. 9) On the failed Spring XD container we have observed CONNECTION_SUSPEND, CONNECTION_RECONECTED and CHILD_REMOVE Zookeeper events (attached is container-log.txt). Please note that Java process is still running and we see ConnectionStateManager-0 server.ContainerRegistrar - Waiting for supervisor to clean up prior deployments messages. 10) Spring XD admin failed with exception in DepartingContainerModuleRedeployer (attached is admin-log.txt).  11) We have observed that departing container node in Zookeeper (/sa/deployments/modules/allocated/1d3fd4cc-5a70-47ed-b4f3-22deef1f4d4f/) had no children. We did this after few minutes so we are not sure at which point it was cleared.  12) Restarting failed Spring XD container fixed the problem, modules were correctly redeployed. Exception from point 10 is very similar to XD-1983 and this code was rewritten in XD-2004. """"""",,8.0
1,XD-3077,FEATURE,Done,MEDIUM,"""Default HDFS as custom module registry for YARN deployments""","""As a developer, I'd like to default to HDFS as distributed remote location for custom module registry, so I can use xd-shell or the REST-API directly to upload the custom module bits. I would also like to remove custom-modules.zip artifact from YARN distribution.""","""""""As a developer, I'd like to default to HDFS as distributed remote location for custom module registry, so I can use xd-shell or the REST-API directly to upload the custom module bits. I would also like to remove custom-modules.zip artifact from YARN distribution.""""""",,3.0
1,XD-3076,FEATURE,Done,MEDIUM,"""Add SSL properties to the Mail source""","""As a user, I'd like to use the _Mail_ source to connect to secured IMAP and/or SMTP mail servers.   _Mail_ source config file requires a <util:properties/> bean (with ssl/tls properties), provided to the adapter via the java-mail-properties attribute. [Ref. Example|http://docs.spring.io/spring-integration/docs/latest-ga/reference/html/mail.html].  {code:xml}    <beans:beans profile=""""default"""">         <util:properties id=""""javaMailProperties"""">             <beans:prop key=""""mail.imap.socketFactory.class"""">javax.net.ssl.SSLSocketFactory</beans:prop>             <beans:prop key=""""mail.imap.socketFactory.fallback"""">false</beans:prop>             <beans:prop key=""""mail.store.protocol"""">imaps</beans:prop>             <beans:prop key=""""mail.debug"""">false</beans:prop>         </util:properties>     </beans:beans> {code}  [List of all java-mail properties|https://javamail.java.net/nonav/docs/api/com/sun/mail/smtp/package-summary.html]""","""""""As a user, I'd like to use the _Mail_ source to connect to secured IMAP and/or SMTP mail servers.   _Mail_ source config file requires a <util:properties/> bean (with ssl/tls properties), provided to the adapter via the java-mail-properties attribute. [Ref. Example|http://docs.spring.io/spring-integration/docs/latest-ga/reference/html/mail.html].    [List of all java-mail properties|https://javamail.java.net/nonav/docs/api/com/sun/mail/smtp/package-summary.html]""""""","""    <beans:beans profile=""""""""default"""""""">         <util:properties id=""""""""javaMailProperties"""""""">             <beans:prop key=""""""""mail.imap.socketFactory.class"""""""">javax.net.ssl.SSLSocketFactory</beans:prop>             <beans:prop key=""""""""mail.imap.socketFactory.fallback"""""""">false</beans:prop>             <beans:prop key=""""""""mail.store.protocol"""""""">imaps</beans:prop>             <beans:prop key=""""""""mail.debug"""""""">false</beans:prop>         </util:properties>     </beans:beans> """,1.0
1,XD-3075,BUG,Done,MEDIUM,"""Backport Kafka Sink input fix""","""As part of XD-2958, we've changed the input type of the Kafka sink from String to byte[]. The main reason for the change was that it required an arbitrary and often unneeded (but expensive) conversion to String for the bus payloads.   Apply the same change to 1.1 branch.""","""""""As part of XD-2958, we've changed the input type of the Kafka sink from String to byte[]. The main reason for the change was that it required an arbitrary and often unneeded (but expensive) conversion to String for the bus payloads.   Apply the same change to 1.1 branch.""""""",,1.0
1,XD-3074,BUG,Done,MEDIUM,"""Backport metadata retrieval stability improvements""","""Backport stability improvements added as part of XD-2958 to the 1.1.x branch.""","""""""Backport stability improvements added as part of XD-2958 to the 1.1.x branch.""""""",,1.0
1,XD-3072,FEATURE,Done,MEDIUM,"""Flo parser improvements""","""As a Flo developer, I'd like to add improvements to existing Flo parser endpoints, so I can streamline the error reporting strategy.""","""""""As a Flo developer, I'd like to add improvements to existing Flo parser endpoints, so I can streamline the error reporting strategy.""""""",,3.0
1,XD-3071,FEATURE,Done,MEDIUM,"""Spike: Produce Rabbit baseline on rackspace infrastructure""","""As a developer, I'd like to bench Rabbit on rackspace infrastructure, so I can have a sense on how it scales as we add more _xd-container_ nodes.""","""""""As a developer, I'd like to bench Rabbit on rackspace infrastructure, so I can have a sense on how it scales as we add more _xd-container_ nodes.""""""",,8.0
1,XD-3070,FEATURE,Done,HIGH,"""Spike: introduce xolpoc-admin to XD Admin""","""The POC for XD on Lattice uses the following interface for module deployment:  https://github.com/markfisher/xolpoc-admin/blob/master/src/main/java/xolpoc/spi/ModuleDeployer.java  {code} public interface ModuleDeployer {   void deploy(ModuleDescriptor descriptor);   void undeploy(ModuleDescriptor descriptor);   ModuleStatus getStatus(ModuleDescriptor descriptor);  } {code}  This spike is to introduce this interface and the Lattice implementation in the XD admin. The goals are to: * Demo a POC showing simple stream deployment with the existing shell/admin to Lattice * Learn from the experience to help guide the re-architecture/splitting of stream/job repositories (especially in regard to {{AbstractDeployer}} and related classes).  Note that this work will not necessarily be merged into XD itself, although some of the concepts may be included in a future PR.""","""""""The POC for XD on Lattice uses the following interface for module deployment:  https://github.com/markfisher/xolpoc-admin/blob/master/src/main/java/xolpoc/spi/ModuleDeployer.java    This spike is to introduce this interface and the Lattice implementation in the XD admin. The goals are to: * Demo a POC showing simple stream deployment with the existing shell/admin to Lattice * Learn from the experience to help guide the re-architecture/splitting of stream/job repositories (especially in regard to {{AbstractDeployer}} and related classes).  Note that this work will not necessarily be merged into XD itself, although some of the concepts may be included in a future PR.""""""",""" public interface ModuleDeployer {   void deploy(ModuleDescriptor descriptor);   void undeploy(ModuleDescriptor descriptor);   ModuleStatus getStatus(ModuleDescriptor descriptor);  } """,5.0
1,XD-3067,BUG,Done,HIGH,"""Spark streaming integration module fails to initialize codec""","""XD Spark streaming module fails to load:  Caused by: org.springframework.beans.factory.CannotLoadBeanClassException: Error loading class [org.springframework.xd.tuple.serializer.kryo.TupleCodec] for bean with name 'org.springframework.xd.tuple.serializer.kryo.TupleCodec#2e8f5f36' defined in class path resource [META-INF/spring-xd/bus/codec.xml]: problem with class file or dependent class; nested exception is java.lang.IllegalAccessError: class org.springframework.xd.tuple.serializer.kryo.TupleCodec cannot access its superclass org.springframework.xd.dirt.integration.bus.serializer.kryo.AbstractKryoCodec  at org.springframework.beans.factory.support.AbstractBeanFactory.resolveBeanClass(AbstractBeanFactory.java:1331)  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:453)  at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveInnerBean(BeanDefinitionValueResolver.java:299)  ... 67 more Caused by: java.lang.IllegalAccessError: class org.springframework.xd.tuple.serializer.kryo.TupleCodec cannot access its superclass org.springframework.xd.dirt.integration.bus.serializer.kryo.AbstractKryoCodec  at java.lang.ClassLoader.defineClass1(Native Method)  at java.lang.ClassLoader.defineClass(ClassLoader.java:760)  at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)  at java.net.URLClassLoader.defineClass(URLClassLoader.java:455)  at java.net.URLClassLoader.access$100(URLClassLoader.java:73)  at java.net.URLClassLoader$1.run(URLClassLoader.java:367)  at java.net.URLClassLoader$1.run(URLClassLoader.java:361)  at java.security.AccessController.doPrivileged(Native Method)  at java.net.URLClassLoader.findClass(URLClassLoader.java:360)  at java.lang.ClassLoader.loadClass(ClassLoader.java:424)  at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)  at java.lang.ClassLoader.loadClass(ClassLoader.java:357)  at org.springframework.util.ClassUtils.forName(ClassUtils.java:249)  at org.springframework.beans.factory.support.AbstractBeanDefinition.resolveBeanClass(AbstractBeanDefinition.java:395)  at org.springframework.beans.factory.support.AbstractBeanFactory.doResolveBeanClass(AbstractBeanFactory.java:1349)  at org.springframework.beans.factory.support.AbstractBeanFactory.resolveBeanClass(AbstractBeanFactory.java:1320)""","""""""XD Spark streaming module fails to load:  Caused by: org.springframework.beans.factory.CannotLoadBeanClassException: Error loading class [org.springframework.xd.tuple.serializer.kryo.TupleCodec] for bean with name 'org.springframework.xd.tuple.serializer.kryo.TupleCodec#2e8f5f36' defined in class path resource [META-INF/spring-xd/bus/codec.xml]: problem with class file or dependent class; nested exception is java.lang.IllegalAccessError: class org.springframework.xd.tuple.serializer.kryo.TupleCodec cannot access its superclass org.springframework.xd.dirt.integration.bus.serializer.kryo.AbstractKryoCodec  at org.springframework.beans.factory.support.AbstractBeanFactory.resolveBeanClass(AbstractBeanFactory.java:1331)  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:453)  at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveInnerBean(BeanDefinitionValueResolver.java:299)  ... 67 more Caused by: java.lang.IllegalAccessError: class org.springframework.xd.tuple.serializer.kryo.TupleCodec cannot access its superclass org.springframework.xd.dirt.integration.bus.serializer.kryo.AbstractKryoCodec  at java.lang.ClassLoader.defineClass1(Native Method)  at java.lang.ClassLoader.defineClass(ClassLoader.java:760)  at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)  at java.net.URLClassLoader.defineClass(URLClassLoader.java:455)  at java.net.URLClassLoader.access$100(URLClassLoader.java:73)  at java.net.URLClassLoader$1.run(URLClassLoader.java:367)  at java.net.URLClassLoader$1.run(URLClassLoader.java:361)  at java.security.AccessController.doPrivileged(Native Method)  at java.net.URLClassLoader.findClass(URLClassLoader.java:360)  at java.lang.ClassLoader.loadClass(ClassLoader.java:424)  at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)  at java.lang.ClassLoader.loadClass(ClassLoader.java:357)  at org.springframework.util.ClassUtils.forName(ClassUtils.java:249)  at org.springframework.beans.factory.support.AbstractBeanDefinition.resolveBeanClass(AbstractBeanDefinition.java:395)  at org.springframework.beans.factory.support.AbstractBeanFactory.doResolveBeanClass(AbstractBeanFactory.java:1349)  at org.springframework.beans.factory.support.AbstractBeanFactory.resolveBeanClass(AbstractBeanFactory.java:1320)""""""",,3.0
1,XD-3066,FEATURE,Done,MEDIUM,"""Make Enum Conversions for ModuleOptions more lenient""","""If you have a an option *--mode=textLine*, presently the enum MUST be named *textLine*.  I think it would improve the user-experience if we allowed users to pass in values such as:  * --mode=textLine * --mode=text_line * --mode=TEXT_LINE  ""","""""""If you have a an option *--mode=textLine*, presently the enum MUST be named *textLine*.  I think it would improve the user-experience if we allowed users to pass in values such as:  * --mode=textLine * --mode=text_line * --mode=TEXT_LINE  """"""",,3.0
1,XD-3064,BUG,Done,URGENT,"""HdfsMongoDB Job failing due because of missing ID in Default Tuple""","""Looks to have been introduced by https://github.com/spring-projects/spring-xd/pull/1577 Deployment: single admin, 2 container deployment using +RabbitMQ+ as the transport. Below is a partial stacktrace (please check log for full stacktrace). Log is attached. {noformat) 2015-05-15 10:50:15,843 1.2.0.SNAP ERROR xdbus.job:ec2Job3-1 step.AbstractStep - Encountered an error executing step readResourcesStep in job ec2Job3 org.springframework.dao.InvalidDataAccessApiUsageException: Cannot autogenerate id of type java.util.UUID for entity of type org.springframework.xd.tuple.DefaultTuple!         at org.springframework.data.mongodb.core.MongoTemplate.assertUpdateableIdIfNotSet(MongoTemplate.java:1153)         at org.springframework.data.mongodb.core.MongoTemplate.doSave(MongoTemplate.java:882)         at org.springframework.data.mongodb.core.MongoTemplate.save(MongoTemplate.java:837)         at org.springframework.batch.item.data.MongoItemWriter.doWrite(MongoItemWriter.java:128)         at org.springframework.batch.item.data.MongoItemWriter$1.beforeCommit(MongoItemWriter.java:156)         at org.springframework.transaction.support.TransactionSynchronizationUtils.triggerBeforeCommit(TransactionSynchronizationUtils.java:95)         at org.springframework.transaction.support.AbstractPlatformTransactionManager.triggerBeforeCommit(AbstractPlatformTransactionManager.java:928)         at org.springframework.transaction.support.AbstractPlatformTransactionManager.processCommit(AbstractPlatformTransactionManager.java:740)         at org.springframework.transaction.support.AbstractPlatformTransactionManager.commit(AbstractPlatformTransactionManager.java:726)         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)         at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)         at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)         at java.lang.reflect.Method.invoke(Method.java:606) {noformat)""","""""""Looks to have been introduced by https://github.com/spring-projects/spring-xd/pull/1577 Deployment: single admin, 2 container deployment using +RabbitMQ+ as the transport. Below is a partial stacktrace (please check log for full stacktrace). Log is attached. {noformat) 2015-05-15 10:50:15,843 1.2.0.SNAP ERROR xdbus.job:ec2Job3-1 step.AbstractStep - Encountered an error executing step readResourcesStep in job ec2Job3 org.springframework.dao.InvalidDataAccessApiUsageException: Cannot autogenerate id of type java.util.UUID for entity of type org.springframework.xd.tuple.DefaultTuple!         at org.springframework.data.mongodb.core.MongoTemplate.assertUpdateableIdIfNotSet(MongoTemplate.java:1153)         at org.springframework.data.mongodb.core.MongoTemplate.doSave(MongoTemplate.java:882)         at org.springframework.data.mongodb.core.MongoTemplate.save(MongoTemplate.java:837)         at org.springframework.batch.item.data.MongoItemWriter.doWrite(MongoItemWriter.java:128)         at org.springframework.batch.item.data.MongoItemWriter$1.beforeCommit(MongoItemWriter.java:156)         at org.springframework.transaction.support.TransactionSynchronizationUtils.triggerBeforeCommit(TransactionSynchronizationUtils.java:95)         at org.springframework.transaction.support.AbstractPlatformTransactionManager.triggerBeforeCommit(AbstractPlatformTransactionManager.java:928)         at org.springframework.transaction.support.AbstractPlatformTransactionManager.processCommit(AbstractPlatformTransactionManager.java:740)         at org.springframework.transaction.support.AbstractPlatformTransactionManager.commit(AbstractPlatformTransactionManager.java:726)         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)         at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)         at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)         at java.lang.reflect.Method.invoke(Method.java:606) {noformat)""""""","""""""Looks to have been introduced by https://github.com/spring-projects/spring-xd/pull/1577 Deployment: single admin, 2 container deployment using +RabbitMQ+ as the transport. Below is a partial stacktrace (please check log for full stacktrace). Log is attached. {noformat) 2015-05-15 10:50:15,843 1.2.0.SNAP ERROR xdbus.job:ec2Job3-1 step.AbstractStep - Encountered an error executing step readResourcesStep in job ec2Job3 org.springframework.dao.InvalidDataAccessApiUsageException: Cannot autogenerate id of type java.util.UUID for entity of type org.springframework.xd.tuple.DefaultTuple!         at org.springframework.data.mongodb.core.MongoTemplate.assertUpdateableIdIfNotSet(MongoTemplate.java:1153)         at org.springframework.data.mongodb.core.MongoTemplate.doSave(MongoTemplate.java:882)         at org.springframework.data.mongodb.core.MongoTemplate.save(MongoTemplate.java:837)         at org.springframework.batch.item.data.MongoItemWriter.doWrite(MongoItemWriter.java:128)         at org.springframework.batch.item.data.MongoItemWriter$1.beforeCommit(MongoItemWriter.java:156)         at org.springframework.transaction.support.TransactionSynchronizationUtils.triggerBeforeCommit(TransactionSynchronizationUtils.java:95)         at org.springframework.transaction.support.AbstractPlatformTransactionManager.triggerBeforeCommit(AbstractPlatformTransactionManager.java:928)         at org.springframework.transaction.support.AbstractPlatformTransactionManager.processCommit(AbstractPlatformTransactionManager.java:740)         at org.springframework.transaction.support.AbstractPlatformTransactionManager.commit(AbstractPlatformTransactionManager.java:726)         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)         at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)         at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)         at java.lang.reflect.Method.invoke(Method.java:606) {noformat)""""""",3.0
1,XD-3063,FEATURE,Done,MEDIUM,"""Add Property maxMessagesPerPoll to All Polled Sources""","""Polled message sources return only one message per poll by default.  When polling, say, a file directory with many files, files will be emitted once per {{fixedDelay}}.  As a user I need to configure a limit for the number of messages that will be emitted per poll.""","""""""Polled message sources return only one message per poll by default.  When polling, say, a file directory with many files, files will be emitted once per {{fixedDelay}}.  As a user I need to configure a limit for the number of messages that will be emitted per poll.""""""",,3.0
1,XD-3061,IMPROVEMENT,Done,MEDIUM,"""Remove id/timestamp from tuple and related methods on tuplebuilder""","""As a developer, I'd like to remove _ID_ and _TimeStamp_ attributes from the {{Tuple}} class, so I can improve performance characteristics by not having them go through _serde_; instead, we could leverage message headers to collect such information.  ""","""""""As a developer, I'd like to remove _ID_ and _TimeStamp_ attributes from the {{Tuple}} class, so I can improve performance characteristics by not having them go through _serde_; instead, we could leverage message headers to collect such information.  """"""",,3.0
1,XD-3056,FEATURE,Done,MEDIUM,"""Add a new source module to capture video frame from camera or video files""","""This is a source module for video ingestion: the modules captures video frames from a camera or from a video file. For camera, the frames are grabbed from the rtsp video stream. This module will generate message with the frame image (encoded with JPEG) as the payload.   ""","""""""This is a source module for video ingestion: the modules captures video frames from a camera or from a video file. For camera, the frames are grabbed from the rtsp video stream. This module will generate message with the frame image (encoded with JPEG) as the payload.   """"""",,8.0
1,XD-3054,BUG,Done,MEDIUM,"""Deployment validation when processing the deployment message""","""Currently, the deployment message is being validated before pushed the ZK distributed queue for deployment. When the message is consumed, there is no validation done. Since, the consumer consumes the messages asynchronously, we need validation at both sides.""","""""""Currently, the deployment message is being validated before pushed the ZK distributed queue for deployment. When the message is consumed, there is no validation done. Since, the consumer consumes the messages asynchronously, we need validation at both sides.""""""",,1.0
1,XD-3052,FEATURE,Done,MEDIUM,"""Move gpfdist sink from spring-xd-modules repo to the core""","""As a developer, I'd like to move the project reactor based [gpfdist|https://github.com/spring-projects/spring-xd-modules/tree/master/gpfdist] from spring-xd-module repo to the core, so I can natively use this sink to write to GPDB/HAWQ. ""","""""""As a developer, I'd like to move the project reactor based [gpfdist|https://github.com/spring-projects/spring-xd-modules/tree/master/gpfdist] from spring-xd-module repo to the core, so I can natively use this sink to write to GPDB/HAWQ. """"""",,2.0
1,XD-3051,BUG,Done,MEDIUM,"""Gradle launch task is broken""","""Spring XD has a gradle task available in the build called launch that starts a single node instance.  This is currently broken.  The command I was using for this command was: {code} $ ./gradlew clean build -x test -x javadoc launch {code}""","""""""Spring XD has a gradle task available in the build called launch that starts a single node instance.  This is currently broken.  The command I was using for this command was: """"""",""" $ ./gradlew clean build -x test -x javadoc launch """,1.0
1,XD-3050,FEATURE,Done,MEDIUM,"""Move Reactor based processor module from spring-xd-modules to core""","""As a developer, I'd like to move the project reactor based [data processor module|https://github.com/spring-projects/spring-xd-modules/tree/master/spring-xd-reactor] from _spring-xd-module_ repo to the core, so I can natively use Reactor's Stream API to build processor modules. ""","""""""As a developer, I'd like to move the project reactor based [data processor module|https://github.com/spring-projects/spring-xd-modules/tree/master/spring-xd-reactor] from _spring-xd-module_ repo to the core, so I can natively use Reactor's Stream API to build processor modules. """"""",,3.0
1,XD-3049,FEATURE,Done,MEDIUM,"""Profile byte array on In-Memory and Kafka Transport""","""Identify and report hotspots while running the load-generator source and the throughput sink on : # Singlenode -> In Memory Transport # Singlenode -> Kafka Transport # Admin/Container -> Kafka Transport""","""""""Identify and report hotspots while running the load-generator source and the throughput sink on : # Singlenode -> In Memory Transport # Singlenode -> Kafka Transport # Admin/Container -> Kafka Transport""""""",,3.0
1,XD-3048,BUG,Done,HIGH,"""RabbitMQ queue cleanup uses wildcard unexpectedly""","""Calling the API to delete queues uses a wildcard-like behaviour unexpectedly. If I request to delete:  {{test-1}}  I expect it to delete streams named with the pattern:  {{test-1.*}}  For example, it would delete:  {{test-1.0, test-1.1, etc}}  In fact I believe it wildcards before and after the period, e.g.:  {{test-1*.*}}  And hence would delete:  {{test-1.0, test-11.0, test-123.0, etc}}  That way of working is potentially helpful, but it's also dangerous because it removes the ability to know that you're only deleting the exact queue you want to in all cases.  For the record the commit (https://github.com/spring-projects/spring-xd/commit/2d5f3f706330a6ead8e91c9a7a23d4372715614d) implies that it should work in the more restricted way above, not the less restricted way.  (Note: I've marked this as an improvement because, absent documentation, I don't know what the correct functionality is and hence can't say this is a bug)""","""""""Calling the API to delete queues uses a wildcard-like behaviour unexpectedly. If I request to delete:  {{test-1}}  I expect it to delete streams named with the pattern:  {{test-1.*}}  For example, it would delete:  {{test-1.0, test-1.1, etc}}  In fact I believe it wildcards before and after the period, e.g.:  {{test-1*.*}}  And hence would delete:  {{test-1.0, test-11.0, test-123.0, etc}}  That way of working is potentially helpful, but it's also dangerous because it removes the ability to know that you're only deleting the exact queue you want to in all cases.  For the record the commit (https://github.com/spring-projects/spring-xd/commit/2d5f3f706330a6ead8e91c9a7a23d4372715614d) implies that it should work in the more restricted way above, not the less restricted way.  (Note: I've marked this as an improvement because, absent documentation, I don't know what the correct functionality is and hence can't say this is a bug)""""""",,1.0
1,XD-3047,FEATURE,Done,HIGH,"""Complete Camera Ready DEBS submission""","""Complete and submit DEBS 2015 paper as described here:  http://www.debs2015.org/camera-ready-instructions.html""","""""""Complete and submit DEBS 2015 paper as described here:  http://www.debs2015.org/camera-ready-instructions.html""""""",,5.0
1,XD-3046,BUG,Done,MEDIUM,"""Fix compilation errors after moving SingleNodeApplication package""","""Samples including Singlenode tests need to update for package changes""","""""""Samples including Singlenode tests need to update for package changes""""""",,2.0
1,XD-3043,FEATURE,Done,MEDIUM,"""Document GF specific configuration properties""","""As a user, I'd like to use the GF source along with native GF authentication enabled, so I can consume data from GF in a secured way. I'd like to refer to documentation on where the GF specific native and security properties needs configured.   See this [SC post|https://gopivotal-com.socialcast.com/messages/24377202] for more details.""","""""""As a user, I'd like to use the GF source along with native GF authentication enabled, so I can consume data from GF in a secured way. I'd like to refer to documentation on where the GF specific native and security properties needs configured.   See this [SC post|https://gopivotal-com.socialcast.com/messages/24377202] for more details.""""""",,1.0
1,XD-3042,FEATURE,Done,MEDIUM,"""Upgrade to Reactor 2.0.1""","""Upgrade to Reactor 2.0.1""","""Upgrade to Reactor 2.0.1""",,5.0
1,XD-3041,IMPROVEMENT,Done,MEDIUM,"""Disable MongoDB boot autoconfiguration at XD runtime""","""XD runtime (admin, container and singlenode) have MongoDB boot autoconfiguration enabled. This spins off MongoDB client and thereby the cleaner thread running on all these runtime.  The MongoDB based modules won't have any impact when we disable this autoconfiguration.  ""","""""""XD runtime (admin, container and singlenode) have MongoDB boot autoconfiguration enabled. This spins off MongoDB client and thereby the cleaner thread running on all these runtime.  The MongoDB based modules won't have any impact when we disable this autoconfiguration.  """"""",,1.0
1,XD-3040,FEATURE,Done,MEDIUM,"""Create Boot based ModuleRunner (Phase III)""","""As a user, I'd like to use Boot-based {{ModuleRunner}} for use in container-managed environments, so I can run XD without _xd-containers_.  Scope: * ""","""""""As a user, I'd like to use Boot-based {{ModuleRunner}} for use in container-managed environments, so I can run XD without _xd-containers_.  Scope: * """"""",,5.0
1,XD-3037,MAINTENANCE,Done,MEDIUM,"""Create documentation for kafka source multiple topic support""","""As a user, I want to have a documentation that shows how to configure multiple topics with Kafka source module.""","""""""As a user, I want to have a documentation that shows how to configure multiple topics with Kafka source module.""""""",,1.0
1,XD-3036,FEATURE,Done,MEDIUM,"""Fix section headers in reference TOC""","""See: http://docs.spring.io/spring-xd/docs/current-SNAPSHOT/reference/html/#_introduction_26  There should be chapter/section title before this.""","""""""See: http://docs.spring.io/spring-xd/docs/current-SNAPSHOT/reference/html/#_introduction_26  There should be chapter/section title before this.""""""",,1.0
1,XD-3033,FEATURE,Done,MEDIUM,"""Upgrade to Spring Data Fowler Release""","""Spring Data Gemfire is version 8.0.0 in Folwer, which is the same as in BDS (Should check minor version number in BDS).  ATM we are using gemfire 7.0.x""","""""""Spring Data Gemfire is version 8.0.0 in Folwer, which is the same as in BDS (Should check minor version number in BDS).  ATM we are using gemfire 7.0.x""""""",,3.0
1,XD-3030,BUG,Done,URGENT,"""RabbitMQ queues not being removed on stream destroy""","""As part of p-spring-xd testing we create, deploy, exercise, undeploy and destroy a RabbitMQ to RabbitMQ stream every minute. Spring XD does not appear to be deleting the queues it creates internally for each stream. We have seen as many as ~9800 xdbus queues (via the RabbitMQ web ui) before RabbitMQ runs out of memory and blocks.""","""""""As part of p-spring-xd testing we create, deploy, exercise, undeploy and destroy a RabbitMQ to RabbitMQ stream every minute. Spring XD does not appear to be deleting the queues it creates internally for each stream. We have seen as many as ~9800 xdbus queues (via the RabbitMQ web ui) before RabbitMQ runs out of memory and blocks.""""""",,5.0
1,XD-3029,BUG,Done,MEDIUM,"""SqoopRunner class not found errror ""","""We have installed the SpringXD 1.2 M1 release via the rpm and it seems that the sqoop-1.4.5-hadoop200.jar file are not part of the rpm. The sqoop jar file are not in the xd/lib directory.  This is causing a problem during customer module development if we include the sqoop-1.4.5-hadoop200 dependency as part of the pom file and forces us to redeploy the our jar as separate deployment.  Should we be referencing different dependencies or have or should the sqoop-1.4.5-hadoop200.jar be part of the rpm definition so it part of the xd/lib?  I have currently the following dependency in the pom file:  {code}   <!-- Sqoop -->   <dependency>    <groupId>org.apache.sqoop</groupId>    <artifactId>sqoop</artifactId>    <version>1.4.5</version>    <classifier>hadoop200</classifier>   </dependency> {code}  It would be great be great if the sqoop jar are part of rpm so we don't have to do any additional jar deployment.  Thanks, ""","""""""We have installed the SpringXD 1.2 M1 release via the rpm and it seems that the sqoop-1.4.5-hadoop200.jar file are not part of the rpm. The sqoop jar file are not in the xd/lib directory.  This is causing a problem during customer module development if we include the sqoop-1.4.5-hadoop200 dependency as part of the pom file and forces us to redeploy the our jar as separate deployment.  Should we be referencing different dependencies or have or should the sqoop-1.4.5-hadoop200.jar be part of the rpm definition so it part of the xd/lib?  I have currently the following dependency in the pom file:    It would be great be great if the sqoop jar are part of rpm so we don't have to do any additional jar deployment.  Thanks, """"""","""   <!-- Sqoop -->   <dependency>    <groupId>org.apache.sqoop</groupId>    <artifactId>sqoop</artifactId>    <version>1.4.5</version>    <classifier>hadoop200</classifier>   </dependency> """,2.0
1,XD-3027,FEATURE,Done,MEDIUM,"""Update Spring Integration / Spring AMQP Versions""","""4.1.4 and 1.4.5 respectively.""","""""""4.1.4 and 1.4.5 respectively.""""""",,1.0
1,XD-3025,BUG,Done,HIGH,"""SpringXD sqoop module is hanging""","""The SpringXD Sqoop module is in execution status until it times out, it is hanging.   The container logs show:  2015-05-04 15:15:45,365 1.2.0.M1  INFO DeploymentsPathChildrenCache-0 container.DeploymentListener - Deploying job 'sqoop_lookup' 2015-05-04 15:15:45,536 1.2.0.M1  INFO DeploymentsPathChildrenCache-0 container.DeploymentListener - Deploying module [ModuleDescriptor@239b037a moduleName = 'sqoop', moduleLabel = 'sqoop', group = 'sqoop_lookup', sourceChannelName = [null], sinkChannelName = [null], index = 0, type = job, parameters = map['args' -> '--connect=jdbc:oracle:thin:@************:****/******* username=******** --password-file=/user/zeybeb/workspace/secure-files/gdw.password --table=MASTERDATA.W_LOOKUP_D --target-dir=/user/zeybeb/workspace/ent/masterdata_src/lookup_d -m 1', 'command' -> 'import'], children = list[[empty]]] 2015-05-04 15:16:17,061 1.2.0.M1  INFO inbound.job:sqoop_lookup-redis:queue-inbound-channel-adapter1 sqoop.SqoopTasklet - Sqoop system.out: /tmp/Sqoop-948322291323951735.out  The /tmp/Sqoop-948322291323951735.out file content is:  15:16:17,612  INFO main sqoop.SqoopRunner - Sqoop command: import 15:16:17,613  INFO main sqoop.SqoopRunner - Using args: [--connect=jdbc:oracle:thin:@************:****/*******, username=*********, --password-file=/user/zeybeb/workspace/secure-files/gdw.password, --table=MASTERDATA.W_LOOKUP_D, --target-dir=/user/zeybeb/workspace/ent/masterdata_src/lookup_d, -m, 1] 15:16:17,613  INFO main sqoop.SqoopRunner - Mapreduce home: /opt/pivotal/spring-xd-1.2.0.M1/xd/lib/phd21 15:16:17,631  INFO main sqoop.SqoopRunner - Setting configuration property: fs.defaultFS=hdfs://ilabphd07.isus.emc.com:8020 15:16:17,753  INFO main sqoop.SqoopRunner - Setting configuration property: yarn.resourcemanager.address=ilabphd08.isus.emc.com:8050 15:16:17,753  INFO main sqoop.SqoopRunner - Setting configuration property: yarn.application.classpath=/usr/hdp/2.2.0.0-2041/etc/hadoop/conf.empty,/usr/hdp/2.2.0.0-2041/hadoop/*,/usr/hdp/2.2.0.0-2041/hadoop/lib/*,/usr/hdp/2.2.0.0-2041/hadoop-hdfs/*,/usr/hdp/2.2.0.0-2041/hadoop-hdfs/lib/*,/usr/hdp/2.2.0.0-2041/hadoop-yarn/*,/usr/hdp/2.2.0.0-2041/hadoop-yarn/lib/*,/usr/hdp/2.2.0.0-2041/hadoop-mapreduce/*,/usr/hdp/2.2.0.0-2041/hadoop-mapreduce/lib/*,/usr/hdp/2.2.0.0-2041/sqoop/*,/usr/hdp/2.2.0.0-2041/sqoop/lib/*,/usr/hdp/2.2.0.0-2041/flume/*,/usr/hdp/2.2.0.0-2041/flume/lib/*,/usr/hdp/2.2.0.0-2041/storm/*,/usr/hdp/2.2.0.0-2041/storm/lib/* 15:16:17,754  INFO main sqoop.SqoopRunner - Setting configuration property: mapreduce.framework.name=yarn 15:16:17,837  WARN main tool.SqoopTool - $SQOOP_CONF_DIR has not been set in the environment. Cannot check for additional configuration. 15:16:17,907  INFO main sqoop.Sqoop - Running Sqoop version: 1.4.5 15:16:18,282  WARN main util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable 15:16:19,552  WARN main sqoop.ConnFactory - $SQOOP_CONF_DIR has not been set in the environment. Cannot check for additional configuration. 15:16:19,657  INFO main oracle.OraOopManagerFactory - Data Connector for Oracle and Hadoop is disabled. 15:16:19,673  INFO main manager.SqlManager - Using default fetchSize of 1000 15:16:19,673  INFO main tool.CodeGenTool - Beginning code generation 15:16:20,639  INFO main manager.OracleManager - Time zone has been set to GMT 15:16:20,853  INFO main manager.SqlManager - Executing SQL statement: SELECT t.* FROM MASTERDATA.W_LOOKUP_D t WHERE 1=0 15:16:21,018  INFO main orm.CompilationManager - HADOOP_MAPRED_HOME is /opt/pivotal/spring-xd-1.2.0.M1/xd/lib/phd21 15:16:23,171  INFO main orm.CompilationManager - Writing jar file: /tmp/sqoop-spring-xd/compile/4e11123a52fa36d6677efdb47bcdc43b/MASTERDATA.W_LOOKUP_D.jar 15:16:23,191  INFO main manager.OracleManager - Time zone has been set to GMT 15:16:24,109  INFO main manager.OracleManager - Time zone has been set to GMT 15:16:24,825  INFO main mapreduce.ImportJobBase - Beginning import of MASTERDATA.W_LOOKUP_D 15:16:24,848  INFO main manager.OracleManager - Time zone has been set to GMT 15:16:24,876  WARN main mapreduce.JobBase - SQOOP_HOME is unset. May not be able to find all job dependencies. 15:16:25,083  INFO main client.RMProxy - Connecting to ResourceManager at ilabphd08.isus.emc.com/10.15.232.191:8050 15:16:25,977  INFO main db.DBInputFormat - Using read commited transaction isolation 15:16:26,117  INFO main mapreduce.JobSubmitter - number of splits:1 15:16:26,361  INFO main mapreduce.JobSubmitter - Submitting tokens for job: job_1429280992648_0019 15:16:26,717  INFO main impl.YarnClientImpl - Submitted application application_1429280992648_0019 to ResourceManager at ilabphd08.isus.emc.com/10.15.232.191:8050 15:16:26,782  INFO main mapreduce.Job - The url to track the job: http://http://ilabphd08.isus.emc.com:8088/proxy/application_1429280992648_0019/ 15:16:26,783  INFO main mapreduce.Job - Running job: job_1429280992648_0019  The logs on the Hadoop side are:  Showing 4096 bytes. Click here for full log mumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS) 2015-05-04 15:36:04,026 INFO [main] org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS) 2015-05-04 15:36:05,033 INFO [main] org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS) 2015-05-04 15:36:06,042 INFO [main] org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS) 2015-05-04 15:36:07,049 INFO [main] org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS) 2015-05-04 15:36:08,056 INFO [main] org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS) 2015-05-04 15:36:09,062 INFO [main] org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS) 2015-05-04 15:36:10,069 INFO [main] org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS) 2015-05-04 15:36:41,093 INFO [main] org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS) 2015-05-04 15:36:42,100 INFO [main] org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS) 2015-05-04 15:36:43,107 INFO [main] org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS) 2015-05-04 15:36:44,113 INFO [main] org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS) 2015-05-04 15:36:45,120 INFO [main] org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS) 2015-05-04 15:36:46,129 INFO [main] org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS) 2015-05-04 15:36:47,136 INFO [main] org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS) 2015-05-04 15:36:48,143 INFO [main] org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS) 2015-05-04 15:36:49,150 INFO [main] org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS) 2015-05-04 15:36:50,156 INFO [main] org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)  Even though  the maxRetries is set to 10 the process is going into several sets of retrials.   ""","""""""The SpringXD Sqoop module is in execution status until it times out, it is hanging.   The container logs show:  2015-05-04 15:15:45,365 1.2.0.M1  INFO DeploymentsPathChildrenCache-0 container.DeploymentListener - Deploying job 'sqoop_lookup' 2015-05-04 15:15:45,536 1.2.0.M1  INFO DeploymentsPathChildrenCache-0 container.DeploymentListener - Deploying module [ModuleDescriptor@239b037a moduleName = 'sqoop', moduleLabel = 'sqoop', group = 'sqoop_lookup', sourceChannelName = [null], sinkChannelName = [null], index = 0, type = job, parameters = map['args' -> '--connect=jdbc:oracle:thin:@************:****/******* username=******** --password-file=/user/zeybeb/workspace/secure-files/gdw.password --table=MASTERDATA.W_LOOKUP_D --target-dir=/user/zeybeb/workspace/ent/masterdata_src/lookup_d -m 1', 'command' -> 'import'], children = list[[empty]]] 2015-05-04 15:16:17,061 1.2.0.M1  INFO inbound.job:sqoop_lookup-redis:queue-inbound-channel-adapter1 sqoop.SqoopTasklet - Sqoop system.out: /tmp/Sqoop-948322291323951735.out  The /tmp/Sqoop-948322291323951735.out file content is:  15:16:17,612  INFO main sqoop.SqoopRunner - Sqoop command: import 15:16:17,613  INFO main sqoop.SqoopRunner - Using args: [--connect=jdbc:oracle:thin:@************:****/*******, username=*********, --password-file=/user/zeybeb/workspace/secure-files/gdw.password, --table=MASTERDATA.W_LOOKUP_D, --target-dir=/user/zeybeb/workspace/ent/masterdata_src/lookup_d, -m, 1] 15:16:17,613  INFO main sqoop.SqoopRunner - Mapreduce home: /opt/pivotal/spring-xd-1.2.0.M1/xd/lib/phd21 15:16:17,631  INFO main sqoop.SqoopRunner - Setting configuration property: fs.defaultFS=hdfs://ilabphd07.isus.emc.com:8020 15:16:17,753  INFO main sqoop.SqoopRunner - Setting configuration property: yarn.resourcemanager.address=ilabphd08.isus.emc.com:8050 15:16:17,753  INFO main sqoop.SqoopRunner - Setting configuration property: yarn.application.classpath=/usr/hdp/2.2.0.0-2041/etc/hadoop/conf.empty,/usr/hdp/2.2.0.0-2041/hadoop/*,/usr/hdp/2.2.0.0-2041/hadoop/lib/*,/usr/hdp/2.2.0.0-2041/hadoop-hdfs/*,/usr/hdp/2.2.0.0-2041/hadoop-hdfs/lib/*,/usr/hdp/2.2.0.0-2041/hadoop-yarn/*,/usr/hdp/2.2.0.0-2041/hadoop-yarn/lib/*,/usr/hdp/2.2.0.0-2041/hadoop-mapreduce/*,/usr/hdp/2.2.0.0-2041/hadoop-mapreduce/lib/*,/usr/hdp/2.2.0.0-2041/sqoop/*,/usr/hdp/2.2.0.0-2041/sqoop/lib/*,/usr/hdp/2.2.0.0-2041/flume/*,/usr/hdp/2.2.0.0-2041/flume/lib/*,/usr/hdp/2.2.0.0-2041/storm/*,/usr/hdp/2.2.0.0-2041/storm/lib/* 15:16:17,754  INFO main sqoop.SqoopRunner - Setting configuration property: mapreduce.framework.name=yarn 15:16:17,837  WARN main tool.SqoopTool - $SQOOP_CONF_DIR has not been set in the environment. Cannot check for additional configuration. 15:16:17,907  INFO main sqoop.Sqoop - Running Sqoop version: 1.4.5 15:16:18,282  WARN main util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable 15:16:19,552  WARN main sqoop.ConnFactory - $SQOOP_CONF_DIR has not been set in the environment. Cannot check for additional configuration. 15:16:19,657  INFO main oracle.OraOopManagerFactory - Data Connector for Oracle and Hadoop is disabled. 15:16:19,673  INFO main manager.SqlManager - Using default fetchSize of 1000 15:16:19,673  INFO main tool.CodeGenTool - Beginning code generation 15:16:20,639  INFO main manager.OracleManager - Time zone has been set to GMT 15:16:20,853  INFO main manager.SqlManager - Executing SQL statement: SELECT t.* FROM MASTERDATA.W_LOOKUP_D t WHERE 1=0 15:16:21,018  INFO main orm.CompilationManager - HADOOP_MAPRED_HOME is /opt/pivotal/spring-xd-1.2.0.M1/xd/lib/phd21 15:16:23,171  INFO main orm.CompilationManager - Writing jar file: /tmp/sqoop-spring-xd/compile/4e11123a52fa36d6677efdb47bcdc43b/MASTERDATA.W_LOOKUP_D.jar 15:16:23,191  INFO main manager.OracleManager - Time zone has been set to GMT 15:16:24,109  INFO main manager.OracleManager - Time zone has been set to GMT 15:16:24,825  INFO main mapreduce.ImportJobBase - Beginning import of MASTERDATA.W_LOOKUP_D 15:16:24,848  INFO main manager.OracleManager - Time zone has been set to GMT 15:16:24,876  WARN main mapreduce.JobBase - SQOOP_HOME is unset. May not be able to find all job dependencies. 15:16:25,083  INFO main client.RMProxy - Connecting to ResourceManager at ilabphd08.isus.emc.com/10.15.232.191:8050 15:16:25,977  INFO main db.DBInputFormat - Using read commited transaction isolation 15:16:26,117  INFO main mapreduce.JobSubmitter - number of splits:1 15:16:26,361  INFO main mapreduce.JobSubmitter - Submitting tokens for job: job_1429280992648_0019 15:16:26,717  INFO main impl.YarnClientImpl - Submitted application application_1429280992648_0019 to ResourceManager at ilabphd08.isus.emc.com/10.15.232.191:8050 15:16:26,782  INFO main mapreduce.Job - The url to track the job: http://http://ilabphd08.isus.emc.com:8088/proxy/application_1429280992648_0019/ 15:16:26,783  INFO main mapreduce.Job - Running job: job_1429280992648_0019  The logs on the Hadoop side are:  Showing 4096 bytes. Click here for full log mumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS) 2015-05-04 15:36:04,026 INFO [main] org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS) 2015-05-04 15:36:05,033 INFO [main] org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS) 2015-05-04 15:36:06,042 INFO [main] org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS) 2015-05-04 15:36:07,049 INFO [main] org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS) 2015-05-04 15:36:08,056 INFO [main] org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS) 2015-05-04 15:36:09,062 INFO [main] org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS) 2015-05-04 15:36:10,069 INFO [main] org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS) 2015-05-04 15:36:41,093 INFO [main] org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS) 2015-05-04 15:36:42,100 INFO [main] org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS) 2015-05-04 15:36:43,107 INFO [main] org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS) 2015-05-04 15:36:44,113 INFO [main] org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS) 2015-05-04 15:36:45,120 INFO [main] org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS) 2015-05-04 15:36:46,129 INFO [main] org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS) 2015-05-04 15:36:47,136 INFO [main] org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS) 2015-05-04 15:36:48,143 INFO [main] org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS) 2015-05-04 15:36:49,150 INFO [main] org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS) 2015-05-04 15:36:50,156 INFO [main] org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)  Even though  the maxRetries is set to 10 the process is going into several sets of retrials.   """"""",,5.0
1,XD-3024,FEATURE,Done,MEDIUM,"""Add new REST-API to get all the counters, gauges, and rich-gauges""","""As a user, I'd like to have a REST-API to get all the _counters_, _gauges_, and _rich-gauges_ in a single request, so I don't have to issue multiple request to fetch each one of the metrics by name/id for custom dashboards.  *Example:* {code} /metrics/counters/all (fetches all available counters) /metrics/gauges/all (fetches all available gauges) /metrics/rich-gauges/all (fetches all available rich-gauges) {code}""","""""""As a user, I'd like to have a REST-API to get all the _counters_, _gauges_, and _rich-gauges_ in a single request, so I don't have to issue multiple request to fetch each one of the metrics by name/id for custom dashboards.  *Example:* """"""",""" /metrics/counters/all (fetches all available counters) /metrics/gauges/all (fetches all available gauges) /metrics/rich-gauges/all (fetches all available rich-gauges) """,5.0
1,XD-3023,FEATURE,Done,URGENT,"""SSL Config for RabbitMessageBus Connections is Ignored""","""SSL Config for RabbitMessageBus Connections is Ignored""","""SSL Config for RabbitMessageBus Connections is Ignored""",,1.0
1,XD-3022,BUG,Done,HIGH,"""Kafka Message Bus ignores consumer concurrency when computing partition count""","""This is a combination of two issues: - the internal property `next.module.concurrency` is computed from `concurrency` when it should be computed from `consumer.concurrency` - even if `next.module.concurrency` is set, the KafkaMessageBus rejects it, since it's not set in SUPPORTED_CONSUMER_PROPERTIES  As a result, the value used in partition calculation is always 1.  A workaround exists, by setting the `module.[moduleName].producer.minPartitionCount` property to the expected total value. ""","""""""This is a combination of two issues: - the internal property `next.module.concurrency` is computed from `concurrency` when it should be computed from `consumer.concurrency` - even if `next.module.concurrency` is set, the KafkaMessageBus rejects it, since it's not set in SUPPORTED_CONSUMER_PROPERTIES  As a result, the value used in partition calculation is always 1.  A workaround exists, by setting the `module.[moduleName].producer.minPartitionCount` property to the expected total value. """"""",,3.0
1,XD-3019,FEATURE,Done,MEDIUM,"""Document how to use the module registry backed by HDFS""","""As a user, I'd like to refer to the documentation, so I can configure HDFS backed module registry (XD-2287) as recommended. ""","""""""As a user, I'd like to refer to the documentation, so I can configure HDFS backed module registry (XD-2287) as recommended. """"""",,1.0
1,XD-3018,FEATURE,Done,MEDIUM,"""Update to spring-data-hadoop 2.2.0.M1""","""We should update to use spring-data-hadoop 2.2.0.M1in order to use the fixes available for the HDFS writing there (syncable writes, timeout).  A few things to keep in mind: - this updates Cloudera CDH to 5.3.3 - Kite version is now 1.0 - need to test the hdfs-dataset sink ""","""""""We should update to use spring-data-hadoop 2.2.0.M1in order to use the fixes available for the HDFS writing there (syncable writes, timeout).  A few things to keep in mind: - this updates Cloudera CDH to 5.3.3 - Kite version is now 1.0 - need to test the hdfs-dataset sink """"""",,2.0
1,XD-3017,FEATURE,Done,MEDIUM,"""Fix package tangles""","""See:   https://sonar43.spring.io/drilldown/measures/7173?metric=package_tangle_index""","""""""See:   https://sonar43.spring.io/drilldown/measures/7173?metric=package_tangle_index""""""",,5.0
1,XD-3016,FEATURE,Done,MEDIUM,"""Document Kafka message bus properties""","""For example, how to specify the partition count for topics that are created by the message bus.""","""""""For example, how to specify the partition count for topics that are created by the message bus.""""""",,1.0
1,XD-3015,BUG,Done,URGENT,"""RemoteFileToHadoopTests fails on 1.1.x""","""This error surfaced recently as a result of a fix to a bug in HostNotWindowsRule which disabled this test in all environments. Now the test has been reactivated it is failing on the 1.1.x branch.  The test runs OK on master. {noformat} Encountered an error executing step step1-master in job job org.springframework.messaging.MessageDeliveryException: failed to send Message to channel 'null'; nested exception is java.lang.IllegalStateException: ThreadPoolTaskExecutor not initialized  at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:292)  at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:239)  at org.springframework.xd.dirt.integration.bus.local.LocalMessageBus$3.handleMessage(LocalMessageBus.java:262)  at org.springframework.integration.dispatcher.AbstractDispatcher.tryOptimizedDispatch(AbstractDispatcher.java:116)  at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:101)  at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)  at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)  at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:277)  at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:239)  at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:115)  at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:45)  at org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:95)  at org.springframework.integration.handler.AbstractMessageProducingHandler.sendOutput(AbstractMessageProducingHandler.java:248)  at org.springframework.integration.handler.AbstractMessageProducingHandler.produceOutput(AbstractMessageProducingHandler.java:171)  at org.springframework.integration.handler.AbstractMessageProducingHandler.sendOutputs(AbstractMessageProducingHandler.java:119)  at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:105)  at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78)  at org.springframework.integration.dispatcher.AbstractDispatcher.tryOptimizedDispatch(AbstractDispatcher.java:116)  at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:101)  at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)  at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)  at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:277)  at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:239)  at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:115)  at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:45)  at org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:95)  at org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:85)  at org.springframework.batch.integration.partition.MessageChannelPartitionHandler.handle(MessageChannelPartitionHandler.java:224)  at org.springframework.batch.core.partition.support.PartitionStep.doExecute(PartitionStep.java:106)  at org.springframework.batch.core.step.AbstractStep.execute(AbstractStep.java:198)  at org.springframework.batch.core.job.SimpleStepHandler.handleStep(SimpleStepHandler.java:148)  at org.springframework.batch.core.job.flow.JobFlowExecutor.executeStep(JobFlowExecutor.java:64)  at org.springframework.batch.core.job.flow.support.state.StepState.handle(StepState.java:67)  at org.springframework.batch.core.job.flow.support.SimpleFlow.resume(SimpleFlow.java:165)  at org.springframework.batch.core.job.flow.support.SimpleFlow.start(SimpleFlow.java:144)  at org.springframework.batch.core.job.flow.FlowJob.doExecute(FlowJob.java:134)  at org.springframework.batch.core.job.AbstractJob.execute(AbstractJob.java:304)  at org.springframework.batch.core.launch.support.SimpleJobLauncher$1.run(SimpleJobLauncher.java:135)  at org.springframework.core.task.SyncTaskExecutor.execute(SyncTaskExecutor.java:50)  at org.springframework.batch.core.launch.support.SimpleJobLauncher.run(SimpleJobLauncher.java:128)  at org.springframework.batch.integration.x.RemoteFileToHadoopTests.testSimple(RemoteFileToHadoopTests.java:161)  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)  at java.lang.reflect.Method.invoke(Method.java:483)  at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)  at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)  at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)  at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)  at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)  at org.springframework.test.context.junit4.statements.RunBeforeTestMethodCallbacks.evaluate(RunBeforeTestMethodCallbacks.java:73)  at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)  at org.springframework.test.context.junit4.statements.RunAfterTestMethodCallbacks.evaluate(RunAfterTestMethodCallbacks.java:82)  at org.springframework.test.context.junit4.statements.SpringRepeat.evaluate(SpringRepeat.java:73)  at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)  at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.runChild(SpringJUnit4ClassRunner.java:217)  at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.runChild(SpringJUnit4ClassRunner.java:83)  at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)  at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)  at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)  at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)  at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)  at org.springframework.test.context.junit4.statements.RunBeforeTestClassCallbacks.evaluate(RunBeforeTestClassCallbacks.java:61)  at org.springframework.test.context.junit4.statements.RunAfterTestClassCallbacks.evaluate(RunAfterTestClassCallbacks.java:68)  at org.springframework.xd.test.HostNotWindowsRule$1.evaluate(HostNotWindowsRule.java:38)  at org.junit.rules.RunRules.evaluate(RunRules.java:20)  at org.junit.runners.ParentRunner.run(ParentRunner.java:363)  at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.run(SpringJUnit4ClassRunner.java:163)  at org.junit.runner.JUnitCore.run(JUnitCore.java:137)  at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:74)  at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:211)  at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:67)  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)  at java.lang.reflect.Method.invoke(Method.java:483)  at com.intellij.rt.execution.application.AppMain.main(AppMain.java:134) Caused by: java.lang.IllegalStateException: ThreadPoolTaskExecutor not initialized  at org.springframework.util.Assert.state(Assert.java:385)  at org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor.getThreadPoolExecutor(ThreadPoolTaskExecutor.java:221)  at org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor.execute(ThreadPoolTaskExecutor.java:252)  at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:89)  at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)  at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:277)  ... 76 more  java.lang.AssertionError:  Expected :exitCode=COMPLETED;exitDescription= Actual   :exitCode=FAILED;exitDescription=    <Click to see difference>    at org.junit.Assert.fail(Assert.java:88)  at org.junit.Assert.failNotEquals(Assert.java:834)  at org.junit.Assert.assertEquals(Assert.java:118)  at org.junit.Assert.assertEquals(Assert.java:144)  at org.springframework.batch.integration.x.RemoteFileToHadoopTests.testSimple(RemoteFileToHadoopTests.java:162)  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)  at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)  at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)  at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)  at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)  at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)  at org.springframework.test.context.junit4.statements.RunBeforeTestMethodCallbacks.evaluate(RunBeforeTestMethodCallbacks.java:73)  at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)  at org.springframework.test.context.junit4.statements.RunAfterTestMethodCallbacks.evaluate(RunAfterTestMethodCallbacks.java:82)  at org.springframework.test.context.junit4.statements.SpringRepeat.evaluate(SpringRepeat.java:73)  at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)  at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.runChild(SpringJUnit4ClassRunner.java:217)  at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.runChild(SpringJUnit4ClassRunner.java:83)  at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)  at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)  at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)  at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)  at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)  at org.springframework.test.context.junit4.statements.RunBeforeTestClassCallbacks.evaluate(RunBeforeTestClassCallbacks.java:61)  at org.springframework.test.context.junit4.statements.RunAfterTestClassCallbacks.evaluate(RunAfterTestClassCallbacks.java:68)  at org.springframework.xd.test.HostNotWindowsRule$1.evaluate(HostNotWindowsRule.java:38)  at org.junit.rules.RunRules.evaluate(RunRules.java:20)  at org.junit.runners.ParentRunner.run(ParentRunner.java:363)  at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.run(SpringJUnit4ClassRunner.java:163)  at org.junit.runner.JUnitCore.run(JUnitCore.java:137)  at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:74)  at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:211)  at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:67)  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)  at com.intellij.rt.execution.application.AppMain.main(AppMain.java:134) {noformat}""","""""""This error surfaced recently as a result of a fix to a bug in HostNotWindowsRule which disabled this test in all environments. Now the test has been reactivated it is failing on the 1.1.x branch.  The test runs OK on master. """"""",""" Encountered an error executing step step1-master in job job org.springframework.messaging.MessageDeliveryException: failed to send Message to channel 'null'; nested exception is java.lang.IllegalStateException: ThreadPoolTaskExecutor not initialized  at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:292)  at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:239)  at org.springframework.xd.dirt.integration.bus.local.LocalMessageBus$3.handleMessage(LocalMessageBus.java:262)  at org.springframework.integration.dispatcher.AbstractDispatcher.tryOptimizedDispatch(AbstractDispatcher.java:116)  at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:101)  at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)  at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)  at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:277)  at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:239)  at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:115)  at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:45)  at org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:95)  at org.springframework.integration.handler.AbstractMessageProducingHandler.sendOutput(AbstractMessageProducingHandler.java:248)  at org.springframework.integration.handler.AbstractMessageProducingHandler.produceOutput(AbstractMessageProducingHandler.java:171)  at org.springframework.integration.handler.AbstractMessageProducingHandler.sendOutputs(AbstractMessageProducingHandler.java:119)  at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:105)  at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78)  at org.springframework.integration.dispatcher.AbstractDispatcher.tryOptimizedDispatch(AbstractDispatcher.java:116)  at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:101)  at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)  at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)  at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:277)  at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:239)  at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:115)  at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:45)  at org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:95)  at org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:85)  at org.springframework.batch.integration.partition.MessageChannelPartitionHandler.handle(MessageChannelPartitionHandler.java:224)  at org.springframework.batch.core.partition.support.PartitionStep.doExecute(PartitionStep.java:106)  at org.springframework.batch.core.step.AbstractStep.execute(AbstractStep.java:198)  at org.springframework.batch.core.job.SimpleStepHandler.handleStep(SimpleStepHandler.java:148)  at org.springframework.batch.core.job.flow.JobFlowExecutor.executeStep(JobFlowExecutor.java:64)  at org.springframework.batch.core.job.flow.support.state.StepState.handle(StepState.java:67)  at org.springframework.batch.core.job.flow.support.SimpleFlow.resume(SimpleFlow.java:165)  at org.springframework.batch.core.job.flow.support.SimpleFlow.start(SimpleFlow.java:144)  at org.springframework.batch.core.job.flow.FlowJob.doExecute(FlowJob.java:134)  at org.springframework.batch.core.job.AbstractJob.execute(AbstractJob.java:304)  at org.springframework.batch.core.launch.support.SimpleJobLauncher$1.run(SimpleJobLauncher.java:135)  at org.springframework.core.task.SyncTaskExecutor.execute(SyncTaskExecutor.java:50)  at org.springframework.batch.core.launch.support.SimpleJobLauncher.run(SimpleJobLauncher.java:128)  at org.springframework.batch.integration.x.RemoteFileToHadoopTests.testSimple(RemoteFileToHadoopTests.java:161)  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)  at java.lang.reflect.Method.invoke(Method.java:483)  at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)  at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)  at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)  at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)  at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)  at org.springframework.test.context.junit4.statements.RunBeforeTestMethodCallbacks.evaluate(RunBeforeTestMethodCallbacks.java:73)  at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)  at org.springframework.test.context.junit4.statements.RunAfterTestMethodCallbacks.evaluate(RunAfterTestMethodCallbacks.java:82)  at org.springframework.test.context.junit4.statements.SpringRepeat.evaluate(SpringRepeat.java:73)  at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)  at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.runChild(SpringJUnit4ClassRunner.java:217)  at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.runChild(SpringJUnit4ClassRunner.java:83)  at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)  at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)  at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)  at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)  at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)  at org.springframework.test.context.junit4.statements.RunBeforeTestClassCallbacks.evaluate(RunBeforeTestClassCallbacks.java:61)  at org.springframework.test.context.junit4.statements.RunAfterTestClassCallbacks.evaluate(RunAfterTestClassCallbacks.java:68)  at org.springframework.xd.test.HostNotWindowsRule$1.evaluate(HostNotWindowsRule.java:38)  at org.junit.rules.RunRules.evaluate(RunRules.java:20)  at org.junit.runners.ParentRunner.run(ParentRunner.java:363)  at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.run(SpringJUnit4ClassRunner.java:163)  at org.junit.runner.JUnitCore.run(JUnitCore.java:137)  at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:74)  at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:211)  at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:67)  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)  at java.lang.reflect.Method.invoke(Method.java:483)  at com.intellij.rt.execution.application.AppMain.main(AppMain.java:134) Caused by: java.lang.IllegalStateException: ThreadPoolTaskExecutor not initialized  at org.springframework.util.Assert.state(Assert.java:385)  at org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor.getThreadPoolExecutor(ThreadPoolTaskExecutor.java:221)  at org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor.execute(ThreadPoolTaskExecutor.java:252)  at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:89)  at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)  at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:277)  ... 76 more  java.lang.AssertionError:  Expected :exitCode=COMPLETED;exitDescription= Actual   :exitCode=FAILED;exitDescription=    <Click to see difference>    at org.junit.Assert.fail(Assert.java:88)  at org.junit.Assert.failNotEquals(Assert.java:834)  at org.junit.Assert.assertEquals(Assert.java:118)  at org.junit.Assert.assertEquals(Assert.java:144)  at org.springframework.batch.integration.x.RemoteFileToHadoopTests.testSimple(RemoteFileToHadoopTests.java:162)  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)  at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)  at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)  at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)  at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)  at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)  at org.springframework.test.context.junit4.statements.RunBeforeTestMethodCallbacks.evaluate(RunBeforeTestMethodCallbacks.java:73)  at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)  at org.springframework.test.context.junit4.statements.RunAfterTestMethodCallbacks.evaluate(RunAfterTestMethodCallbacks.java:82)  at org.springframework.test.context.junit4.statements.SpringRepeat.evaluate(SpringRepeat.java:73)  at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)  at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.runChild(SpringJUnit4ClassRunner.java:217)  at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.runChild(SpringJUnit4ClassRunner.java:83)  at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)  at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)  at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)  at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)  at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)  at org.springframework.test.context.junit4.statements.RunBeforeTestClassCallbacks.evaluate(RunBeforeTestClassCallbacks.java:61)  at org.springframework.test.context.junit4.statements.RunAfterTestClassCallbacks.evaluate(RunAfterTestClassCallbacks.java:68)  at org.springframework.xd.test.HostNotWindowsRule$1.evaluate(HostNotWindowsRule.java:38)  at org.junit.rules.RunRules.evaluate(RunRules.java:20)  at org.junit.runners.ParentRunner.run(ParentRunner.java:363)  at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.run(SpringJUnit4ClassRunner.java:163)  at org.junit.runner.JUnitCore.run(JUnitCore.java:137)  at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:74)  at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:211)  at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:67)  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)  at com.intellij.rt.execution.application.AppMain.main(AppMain.java:134) """,2.0
1,XD-3014,FEATURE,Done,HIGH,"""Add documentation for connecting to HDFS with HA Namenode""","""Add documentation for connecting to HDFS with HA Namenode""","""Add documentation for connecting to HDFS with HA Namenode""",,1.0
1,XD-3013,FEATURE,Done,MEDIUM,"""Support --ref=true/false for sftp source""","""The file and ftp sources allow working with either the java.io.File or its contents. For consistency, the sftp source should support the same mechanism. ""","""""""The file and ftp sources allow working with either the java.io.File or its contents. For consistency, the sftp source should support the same mechanism. """"""",,2.0
1,XD-3009,BUG,Done,URGENT,"""Manual acknowledgement with Kafka bus doesn't work""","""When the kafka message headers are expected to be set with acknowledgement flags to manually acknowledge the messages at the consumer side, the message headers are missing.""","""""""When the kafka message headers are expected to be set with acknowledgement flags to manually acknowledge the messages at the consumer side, the message headers are missing.""""""",,1.0
1,XD-3008,BUG,Done,MEDIUM,"""Version info not available when security enabled""","""When security is enabled, the VersionController REST endpoint isn't visible.""","""""""When security is enabled, the VersionController REST endpoint isn't visible.""""""",,1.0
1,XD-3007,BUG,Done,MEDIUM,"""Message rate collection throws warning level exception""","""When the container doesn't have any modules deployed, the jolokia response returns stacktrace with """"NoInstanceFoundException"""". This exception is thrown at the admin log as:  2015-04-28 13:09:35,952 1.2.0.SNAP  WARN qtp1648225666-27 rest.ContainersController - Error getting message rate metrics for 713255e5-49b2-4158-b69c-2d203cfe50d3 org.codehaus.jettison.json.JSONException: JSONObject[""""value""""] not found.  at org.codehaus.jettison.json.JSONObject.get(JSONObject.java:360)  at org.codehaus.jettison.json.JSONObject.getJSONObject(JSONObject.java:454)  at org.springframework.xd.dirt.rest.ContainersController.setMessageRates(ContainersController.java:134)  at org.springframework.xd.dirt.rest.ContainersController.list(ContainersController.java:114)  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)  at java.lang.reflect.Method.invoke(Method.java:483)  at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:221)  at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:137)  at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:110)  at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandleMethod(RequestMappingHandlerAdapter.java:777)  at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:706)  at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:85)  at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:943)  at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:877)  at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:966)  at org.springframework.web.servlet.FrameworkServlet.doGet(FrameworkServlet.java:857)  at javax.servlet.http.HttpServlet.service(HttpServlet.java:735)  at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:842)  at javax.servlet.http.HttpServlet.service(HttpServlet.java:848)  at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:684)  at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1496)  at org.springframework.boot.actuate.autoconfigure.EndpointWebMvcAutoConfiguration$ApplicationContextHeaderFilter.doFilterInternal(EndpointWebMvcAutoConfiguration.java:291)  at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)  at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)  at org.springframework.web.filter.HiddenHttpMethodFilter.doFilterInternal(HiddenHttpMethodFilter.java:77)  at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)  at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)  at org.springframework.web.filter.HttpPutFormContentFilter.doFilterInternal(HttpPutFormContentFilter.java:87)  at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)  at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)  at org.springframework.boot.actuate.trace.WebRequestTraceFilter.doFilterInternal(WebRequestTraceFilter.java:100)  at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)  at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)  at org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:186)  at org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:160)  at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)  at org.springframework.boot.actuate.autoconfigure.MetricFilterAutoConfiguration$MetricsFilter.doFilterInternal(MetricFilterAutoConfiguration.java:90)  at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)  at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)  at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:499)  at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:137)  at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:557)  at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:231)  at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1086)  at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:428)  at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:193)  at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1020)  at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)  at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)  at org.eclipse.jetty.server.Server.handle(Server.java:370)  at org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:494)  at org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:971)  at org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:1033)  at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:644)  at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235)  at org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82)  at org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:667)  at org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:52)  at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)  at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)  at java.lang.Thread.run(Thread.java:745)""","""""""When the container doesn't have any modules deployed, the jolokia response returns stacktrace with """"""""NoInstanceFoundException"""""""". This exception is thrown at the admin log as:  2015-04-28 13:09:35,952 1.2.0.SNAP  WARN qtp1648225666-27 rest.ContainersController - Error getting message rate metrics for 713255e5-49b2-4158-b69c-2d203cfe50d3 org.codehaus.jettison.json.JSONException: JSONObject[""""""""value""""""""] not found.  at org.codehaus.jettison.json.JSONObject.get(JSONObject.java:360)  at org.codehaus.jettison.json.JSONObject.getJSONObject(JSONObject.java:454)  at org.springframework.xd.dirt.rest.ContainersController.setMessageRates(ContainersController.java:134)  at org.springframework.xd.dirt.rest.ContainersController.list(ContainersController.java:114)  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)  at java.lang.reflect.Method.invoke(Method.java:483)  at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:221)  at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:137)  at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:110)  at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandleMethod(RequestMappingHandlerAdapter.java:777)  at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:706)  at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:85)  at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:943)  at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:877)  at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:966)  at org.springframework.web.servlet.FrameworkServlet.doGet(FrameworkServlet.java:857)  at javax.servlet.http.HttpServlet.service(HttpServlet.java:735)  at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:842)  at javax.servlet.http.HttpServlet.service(HttpServlet.java:848)  at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:684)  at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1496)  at org.springframework.boot.actuate.autoconfigure.EndpointWebMvcAutoConfiguration$ApplicationContextHeaderFilter.doFilterInternal(EndpointWebMvcAutoConfiguration.java:291)  at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)  at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)  at org.springframework.web.filter.HiddenHttpMethodFilter.doFilterInternal(HiddenHttpMethodFilter.java:77)  at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)  at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)  at org.springframework.web.filter.HttpPutFormContentFilter.doFilterInternal(HttpPutFormContentFilter.java:87)  at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)  at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)  at org.springframework.boot.actuate.trace.WebRequestTraceFilter.doFilterInternal(WebRequestTraceFilter.java:100)  at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)  at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)  at org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:186)  at org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:160)  at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)  at org.springframework.boot.actuate.autoconfigure.MetricFilterAutoConfiguration$MetricsFilter.doFilterInternal(MetricFilterAutoConfiguration.java:90)  at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)  at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)  at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:499)  at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:137)  at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:557)  at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:231)  at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1086)  at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:428)  at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:193)  at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1020)  at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)  at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)  at org.eclipse.jetty.server.Server.handle(Server.java:370)  at org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:494)  at org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:971)  at org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:1033)  at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:644)  at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235)  at org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82)  at org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:667)  at org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:52)  at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)  at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)  at java.lang.Thread.run(Thread.java:745)""""""",,1.0
1,XD-3006,FEATURE,Done,HIGH,"""db.password not being passed when Sqoop job --create called from Spring-XD Batch job""","""Created a single definition Sqoop Job --create statement. When processed through Sqoop direct (see attach image for props in repos. jobname sqoop2)   Notice: job definition set db.require.password = false and has a propname called db.password that holds the value for password.   When same definition is created as a batch job via Spring-XD (see image jobname lookupd_job), the db.require.password prop is set to """"TRUE"""" and the db.password is not even created as a row.  This causes a password prompt when trying to execute the job in Sqoop direct, that was created via a spring-XD batch job.   Note the job definition for Spring-XD job contains meta-connect with username and password defined as well as import jdbc: --connect username and password. When created via Sqoop (no Spring-XD) password is stored and job is able to execute with no prompt.  Please see attachment for repository job values. Highlighted in yellow is the discrepant property and value attribution. Please reach out if there are questions or need further details.  To automate Spring-XD batch job using Sqoop via Spring we can not be prompted for password.  Spring-XD Job definition: job create sqoop_lookup_d_job_v1 --definition """"sqoop --command=job  --args='--create lookupd_job --meta-connect \""""jdbc:postgresql://xx.xxx.xx.xx:5433/bdl?user=xxxxx&password=xxxxxx\"""" -- import  --connect jdbc:oracle:thin:@xxxxxx.com:xxxx/GDWP --username xxxxx --password xxxxx --query \""""SELECT ROW_WID, LKUP_ID, TRIM(TRANSLATE(LKUP_CD, CHR(10)||CHR(13), ''-'')) AS LKUP_CD, TRIM(TRANSLATE(LKUP_DESC, CHR(10)||CHR(13), ''-'')) AS LKUP_DESC, TRIM(TRANSLATE(LKUP_TYPE, CHR(10)||CHR(13), ''-'')) AS LKUP_TYPE, TRIM(TRANSLATE(LKUP_VAL, CHR(10)||CHR(13), ''-'')) AS LKUP_VAL, SRC_STRT_DT, SRC_END_DT, W_INSERT_DT, W_UPDATE_DT FROM MASTERDATA.W_LOOKUP_D WHERE 1=1 AND $CONDITIONS\"""" --incremental append --check-column ROW_WID --last-value 10150 --target-dir /user/<USER>bdl/data/sqoop/jobtest --num-mappers 1 --fields-terminated-by \""""|\"""" --append'""""  Sqoop Direct Definition: sqoop job --create sqoop2 --meta-connect 'jdbc:postgresql://xx.xxx.xx.xx:5433/bdl?user=xxxxx&password=xxxxxx' -- import  --connect jdbc:oracle:thin:@xxxxxx.com:xxxx/GDWP --username xxxxx --password xxxxx --query """"SELECT ROW_WID, LKUP_ID, TRIM(TRANSLATE(LKUP_CD, CHR(10)||CHR(13), '-')) AS LKUP_CD, TRIM(TRANSLATE(LKUP_DESC, CHR(10)||CHR(13), '-')) AS LKUP_DESC, TRIM(TRANSLATE(LKUP_TYPE, CHR(10)||CHR(13), '-')) AS LKUP_TYPE, TRIM(TRANSLATE(LKUP_VAL, CHR(10)||CHR(13), '-')) AS LKUP_VAL, SRC_STRT_DT, SRC_END_DT, W_INSERT_DT, W_UPDATE_DT FROM MASTERDATA.W_LOOKUP_D WHERE 1=1 AND \$CONDITIONS """" --incremental append --check-column ROW_WID --last-value 10150 --target-dir /user/<USER>bdl/data/sqoop/jobtest --num-mappers 1 --fields-terminated-by '|' --append ""","""""""Created a single definition Sqoop Job --create statement. When processed through Sqoop direct (see attach image for props in repos. jobname sqoop2)   Notice: job definition set db.require.password = false and has a propname called db.password that holds the value for password.   When same definition is created as a batch job via Spring-XD (see image jobname lookupd_job), the db.require.password prop is set to """"""""TRUE"""""""" and the db.password is not even created as a row.  This causes a password prompt when trying to execute the job in Sqoop direct, that was created via a spring-XD batch job.   Note the job definition for Spring-XD job contains meta-connect with username and password defined as well as import jdbc: --connect username and password. When created via Sqoop (no Spring-XD) password is stored and job is able to execute with no prompt.  Please see attachment for repository job values. Highlighted in yellow is the discrepant property and value attribution. Please reach out if there are questions or need further details.  To automate Spring-XD batch job using Sqoop via Spring we can not be prompted for password.  Spring-XD Job definition: job create sqoop_lookup_d_job_v1 --definition """"""""sqoop --command=job  --args='--create lookupd_job --meta-connect \""""""""jdbc:postgresql://xx.xxx.xx.xx:5433/bdl?user=xxxxx&password=xxxxxx\"""""""" -- import  --connect jdbc:oracle:thin:@xxxxxx.com:xxxx/GDWP --username xxxxx --password xxxxx --query \""""""""SELECT ROW_WID, LKUP_ID, TRIM(TRANSLATE(LKUP_CD, CHR(10)||CHR(13), ''-'')) AS LKUP_CD, TRIM(TRANSLATE(LKUP_DESC, CHR(10)||CHR(13), ''-'')) AS LKUP_DESC, TRIM(TRANSLATE(LKUP_TYPE, CHR(10)||CHR(13), ''-'')) AS LKUP_TYPE, TRIM(TRANSLATE(LKUP_VAL, CHR(10)||CHR(13), ''-'')) AS LKUP_VAL, SRC_STRT_DT, SRC_END_DT, W_INSERT_DT, W_UPDATE_DT FROM MASTERDATA.W_LOOKUP_D WHERE 1=1 AND $CONDITIONS\"""""""" --incremental append --check-column ROW_WID --last-value 10150 --target-dir /user/<USER>bdl/data/sqoop/jobtest --num-mappers 1 --fields-terminated-by \""""""""|\"""""""" --append'""""""""  Sqoop Direct Definition: sqoop job --create sqoop2 --meta-connect 'jdbc:postgresql://xx.xxx.xx.xx:5433/bdl?user=xxxxx&password=xxxxxx' -- import  --connect jdbc:oracle:thin:@xxxxxx.com:xxxx/GDWP --username xxxxx --password xxxxx --query """"""""SELECT ROW_WID, LKUP_ID, TRIM(TRANSLATE(LKUP_CD, CHR(10)||CHR(13), '-')) AS LKUP_CD, TRIM(TRANSLATE(LKUP_DESC, CHR(10)||CHR(13), '-')) AS LKUP_DESC, TRIM(TRANSLATE(LKUP_TYPE, CHR(10)||CHR(13), '-')) AS LKUP_TYPE, TRIM(TRANSLATE(LKUP_VAL, CHR(10)||CHR(13), '-')) AS LKUP_VAL, SRC_STRT_DT, SRC_END_DT, W_INSERT_DT, W_UPDATE_DT FROM MASTERDATA.W_LOOKUP_D WHERE 1=1 AND \$CONDITIONS """""""" --incremental append --check-column ROW_WID --last-value 10150 --target-dir /user/<USER>bdl/data/sqoop/jobtest --num-mappers 1 --fields-terminated-by '|' --append """"""",,5.0
1,XD-3005,BUG,Done,HIGH,"""spring-xd services randomly report failure after correct shutdown""","""On multiple occasions we have seen that {{spring-xd-container}} and {{spring-xd-admin}} services can exit reporting {{FAILED}} state, however most of the times the Java process is correctly terminated.  {code} $ sudo service spring-xd-container stop Stopping xd-container:                                     [FAILED] $ ps uax | grep [x]d-container $ {code}  and in the container logs:  {code} [info 2015/04/28 09:42:15.167 EDT  <Distributed system shutdown hook> tid=0x88] VM is exiting - shutting down distributed system  [info 2015/04/28 09:42:15.168 EDT  <Distributed system shutdown hook> tid=0x88] GemFireCache[id = 2029162775; isClosing = true; isShutDownAll = false; closingGatewayHubsByShutdownAll = false; created = Mon Apr 27 10:59:34 EDT 2015; se  [info 2015/04/28 09:42:15.171 EDT  <Distributed system shutdown hook> tid=0x53] VM is exiting - shutting down distributed system  [info 2015/04/28 09:42:15.171 EDT  <Distributed system shutdown hook> tid=0x53] GemFireCache[id = 389249684; isClosing = true; isShutDownAll = false; closingGatewayHubsByShutdownAll = false; created = Mon Apr 27 10:59:14 EDT 2015; ser  [info 2015/04/28 09:42:15.177 EDT  <Distributed system shutdown hook> tid=0x77] VM is exiting - shutting down distributed system  [info 2015/04/28 09:42:15.179 EDT  <Distributed system shutdown hook> tid=0x77] GemFireCache[id = 1768828050; isClosing = true; isShutDownAll = false; closingGatewayHubsByShutdownAll = false; created = Mon Apr 27 10:59:27 EDT 2015; se  [info 2015/04/28 09:42:15.181 EDT <Distributed system shutdown hook> tid=0x63] VM is exiting - shutting down distributed system  [info 2015/04/28 09:42:15.199 EDT <Distributed system shutdown hook> tid=0x63] GemFireCache[id = 548938309; isClosing = true; isShutDownAll = false; closingGatewayHubsByShutdownAll = false; created = Mon Apr 27 10:59:19 EDT 2015; serv 2015-04-28 09:42:15,410 1.1.1.RELEASE DEBUG xdbus.queue:radius-1 transform.RadiusMsgTransformer - Transformed message: GenericMessage [payload=FACILITY=22, SEVERITY=5, TIMESTAMP=Tue Apr 28 09:42:15 EDT 2015, HOST=hopisepsnprd01, Messa  [info 2015/04/28 09:42:15.626 EDT  <Distributed system shutdown hook> tid=0x53] Resetting original MemoryPoolMXBean heap threshold bytes 0 on pool PS Old Gen  [info 2015/04/28 09:42:15.630 EDT  <Distributed system shutdown hook> tid=0x77] Resetting original MemoryPoolMXBean heap threshold bytes 0 on pool PS Old Gen  [info 2015/04/28 09:42:15.621 EDT  <Distributed system shutdown hook> tid=0x88] Resetting original MemoryPoolMXBean heap threshold bytes 0 on pool PS Old Gen  [info 2015/04/28 09:42:15.662 EDT <Distributed system shutdown hook> tid=0x63] Resetting original MemoryPoolMXBean heap threshold bytes 0 on pool PS Old Gen 2015-04-28 09:42:15,854 1.1.1.RELEASE  WARN Thread-4 support.DisposableBeanAdapter - Invocation of destroy method failed on bean with name 'client-pool': java.lang.IllegalStateException: Pool could not be destroyed because it is still  [config 2015/04/28 09:42:15.857 EDT <Thread-4> tid=0x12] Destroying connection pool client-pool  [config 2015/04/28 09:42:15.897 EDT  <Thread-4> tid=0x12] Destroying connection pool client-pool  [config 2015/04/28 09:42:15.898 EDT  <Distributed system shutdown hook> tid=0x53] Destroying connection pool client-pool  [config 2015/04/28 09:42:15.908 EDT  <Distributed system shutdown hook> tid=0x88] Destroying connection pool client-pool {code}""","""""""On multiple occasions we have seen that {{spring-xd-container}} and {{spring-xd-admin}} services can exit reporting {{FAILED}} state, however most of the times the Java process is correctly terminated.    and in the container logs:  """"""",""" $ sudo service spring-xd-container stop Stopping xd-container:                                     [FAILED] $ ps uax | grep [x]d-container $  [info 2015/04/28 09:42:15.167 EDT  <Distributed system shutdown hook> tid=0x88] VM is exiting - shutting down distributed system  [info 2015/04/28 09:42:15.168 EDT  <Distributed system shutdown hook> tid=0x88] GemFireCache[id = 2029162775; isClosing = true; isShutDownAll = false; closingGatewayHubsByShutdownAll = false; created = Mon Apr 27 10:59:34 EDT 2015; se  [info 2015/04/28 09:42:15.171 EDT  <Distributed system shutdown hook> tid=0x53] VM is exiting - shutting down distributed system  [info 2015/04/28 09:42:15.171 EDT  <Distributed system shutdown hook> tid=0x53] GemFireCache[id = 389249684; isClosing = true; isShutDownAll = false; closingGatewayHubsByShutdownAll = false; created = Mon Apr 27 10:59:14 EDT 2015; ser  [info 2015/04/28 09:42:15.177 EDT  <Distributed system shutdown hook> tid=0x77] VM is exiting - shutting down distributed system  [info 2015/04/28 09:42:15.179 EDT  <Distributed system shutdown hook> tid=0x77] GemFireCache[id = 1768828050; isClosing = true; isShutDownAll = false; closingGatewayHubsByShutdownAll = false; created = Mon Apr 27 10:59:27 EDT 2015; se  [info 2015/04/28 09:42:15.181 EDT <Distributed system shutdown hook> tid=0x63] VM is exiting - shutting down distributed system  [info 2015/04/28 09:42:15.199 EDT <Distributed system shutdown hook> tid=0x63] GemFireCache[id = 548938309; isClosing = true; isShutDownAll = false; closingGatewayHubsByShutdownAll = false; created = Mon Apr 27 10:59:19 EDT 2015; serv 2015-04-28 09:42:15,410 1.1.1.RELEASE DEBUG xdbus.queue:radius-1 transform.RadiusMsgTransformer - Transformed message: GenericMessage [payload=FACILITY=22, SEVERITY=5, TIMESTAMP=Tue Apr 28 09:42:15 EDT 2015, HOST=hopisepsnprd01, Messa  [info 2015/04/28 09:42:15.626 EDT  <Distributed system shutdown hook> tid=0x53] Resetting original MemoryPoolMXBean heap threshold bytes 0 on pool PS Old Gen  [info 2015/04/28 09:42:15.630 EDT  <Distributed system shutdown hook> tid=0x77] Resetting original MemoryPoolMXBean heap threshold bytes 0 on pool PS Old Gen  [info 2015/04/28 09:42:15.621 EDT  <Distributed system shutdown hook> tid=0x88] Resetting original MemoryPoolMXBean heap threshold bytes 0 on pool PS Old Gen  [info 2015/04/28 09:42:15.662 EDT <Distributed system shutdown hook> tid=0x63] Resetting original MemoryPoolMXBean heap threshold bytes 0 on pool PS Old Gen 2015-04-28 09:42:15,854 1.1.1.RELEASE  WARN Thread-4 support.DisposableBeanAdapter - Invocation of destroy method failed on bean with name 'client-pool': java.lang.IllegalStateException: Pool could not be destroyed because it is still  [config 2015/04/28 09:42:15.857 EDT <Thread-4> tid=0x12] Destroying connection pool client-pool  [config 2015/04/28 09:42:15.897 EDT  <Thread-4> tid=0x12] Destroying connection pool client-pool  [config 2015/04/28 09:42:15.898 EDT  <Distributed system shutdown hook> tid=0x53] Destroying connection pool client-pool  [config 2015/04/28 09:42:15.908 EDT  <Distributed system shutdown hook> tid=0x88] Destroying connection pool client-pool """,1.0
1,XD-3004,FEATURE,Done,MEDIUM,"""Detailed module list performance improvement""","""The call to /modules?detailed=true that was introduced for Flo proves to be a performance hog, most certainly because of all the metadata resolution that has to occur there (and no caching takes place)""","""""""The call to /modules?detailed=true that was introduced for Flo proves to be a performance hog, most certainly because of all the metadata resolution that has to occur there (and no caching takes place)""""""",,3.0
1,XD-3003,FEATURE,Done,MEDIUM,"""Add support for using Sqoop metastore""","""As a user, I'd like to have the option to change the default Sqoop _metastore_, so I can implement a DB of my choice and not tied to default specifications.  Refer to this [thread|http://stackoverflow.com/questions/24078668/how-to-change-sqoop-metastore] for more details. ""","""""""As a user, I'd like to have the option to change the default Sqoop _metastore_, so I can implement a DB of my choice and not tied to default specifications.  Refer to this [thread|http://stackoverflow.com/questions/24078668/how-to-change-sqoop-metastore] for more details. """"""",,1.0
1,XD-3001,FEATURE,Done,HIGH,"""Unable to call Sqoop Job commands other than --create from within Spring-Xd Job""","""Running a (SQOOP Job --create) from within (Spring-XD JOB create) statement runs successfully.Unable to run the (Sqoop Job --exec) from within (Spring-XD Job)  CREATE DEFINITION: job create sqoop_lookup_d_job_v1 --definition """"sqoop --command=job  --args='--create lookupd_job --meta-connect \""""jdbc:postgresql://xx.xxx.xx.xx:5433/bdl?user=xxxxx&password=xxxxxx\"""" -- import  --connect jdbc:oracle:thin:@xxxxxx.com:xxxx/GDWP --username xxxxx --password xxxxx --query \""""SELECT ROW_WID, LKUP_ID, TRIM(TRANSLATE(LKUP_CD, CHR(10)||CHR(13), ''-'')) AS LKUP_CD, TRIM(TRANSLATE(LKUP_DESC, CHR(10)||CHR(13), ''-'')) AS LKUP_DESC, TRIM(TRANSLATE(LKUP_TYPE, CHR(10)||CHR(13), ''-'')) AS LKUP_TYPE, TRIM(TRANSLATE(LKUP_VAL, CHR(10)||CHR(13), ''-'')) AS LKUP_VAL, SRC_STRT_DT, SRC_END_DT, W_INSERT_DT, W_UPDATE_DT FROM MASTERDATA.W_LOOKUP_D WHERE 1=1 AND $CONDITIONS\"""" --incremental append --check-column ROW_WID --last-value 10150 --target-dir /user/<USER>bdl/data/sqoop/jobtest --num-mappers 1 --fields-terminated-by \""""|\"""" --append'""""  Validated job definition details stored in PostgreSQL repository  Trying to run EXECUTE In Same fashion.  EXECUTION DEFINITION: job create sqoop_lookup_d_exec_v1 --definition """"sqoop --command=job --args='--exec lookupd_job --meta-connect \""""jdbc:postgresql://xx.xxx.xx.xx:5433/bdl?user=xxxxx&password=xxxxx\""""'""""   Stack Trace: batch.stepType org.springframework.batch.core.step.tasklet.TaskletStep batch.taskletType org.springframework.xd.sqoop.SqoopTasklet sqoop.command job --exec lookupd_job --meta-connect """"jdbc:postgresql://xx.xxx.xx.xx:5433/bdl?user=xxxxxx&password=xxxxxxx"""" -- exec sqoop.errors No job operation specified Try --help for usage instructions. Exception in thread """"main"""" java.lang.RuntimeException: Sqoop failed - return code 1 at org.springframework.xd.sqoop.SqoopRunner.main(SqoopRunner.java:81) sqoop.log 16:01:45,668 INFO main sqoop.SqoopRunner - Sqoop command: job 16:01:45,668 INFO main sqoop.SqoopRunner - Using args: [--exec, lookupd_job, --meta-connect, """"jdbc:postgresql://xx.xxx.xx.xx:5433/bdl?user=xxxxxxx&password=xxxxx""""] 16:01:45,668 INFO main sqoop.SqoopRunner - Mapreduce home: /opt/pivotal/spring-xd-1.1.1.RELEASE/xd/lib/phd21 16:01:45,679 INFO main sqoop.SqoopRunner - Setting configuration property: fs.defaultFS=hdfs://xxxxxxxxxxxxx.com:8020 16:01:45,759 INFO main sqoop.SqoopRunner - Setting configuration property: yarn.resourcemanager.address=xxxxxxxxxxxx.com:8032 16:01:45,759 INFO main sqoop.SqoopRunner - Setting configuration property: yarn.application.classpath=$HADOOP_CONF_DIR,$HADOOP_COMMON_HOME/*,$HADOOP_COMMON_HOME/lib/*,$HADOOP_HDFS_HOME/*,$HADOOP_HDFS_HOME/lib/*,$HADOOP_MAPRED_HOME/*,$HADOOP_MAPRED_HOME/lib/*,$HADOOP_YARN_HOME/*,$HADOOP_YARN_HOME/lib/*,$USS_HOME/*,$USS_CONF 16:01:45,759 INFO main sqoop.SqoopRunner - Setting configuration property: mapreduce.framework.name=yarn 16:01:45,817 WARN main tool.SqoopTool - $SQOOP_CONF_DIR has not been set in the environment. Cannot check for additional configuration. 16:01:45,863 INFO main sqoop.Sqoop - Running Sqoop version: 1.4.5  I have tested --list, --exec,--delete they are all erroring same way. Looking at SqoopRunner not sure what makes --create any different from other saved-job functionality.  Please let me know how i can call or execute save job definition's through spring-xd Sqoop tasklet ""","""""""Running a (SQOOP Job --create) from within (Spring-XD JOB create) statement runs successfully.Unable to run the (Sqoop Job --exec) from within (Spring-XD Job)  CREATE DEFINITION: job create sqoop_lookup_d_job_v1 --definition """"""""sqoop --command=job  --args='--create lookupd_job --meta-connect \""""""""jdbc:postgresql://xx.xxx.xx.xx:5433/bdl?user=xxxxx&password=xxxxxx\"""""""" -- import  --connect jdbc:oracle:thin:@xxxxxx.com:xxxx/GDWP --username xxxxx --password xxxxx --query \""""""""SELECT ROW_WID, LKUP_ID, TRIM(TRANSLATE(LKUP_CD, CHR(10)||CHR(13), ''-'')) AS LKUP_CD, TRIM(TRANSLATE(LKUP_DESC, CHR(10)||CHR(13), ''-'')) AS LKUP_DESC, TRIM(TRANSLATE(LKUP_TYPE, CHR(10)||CHR(13), ''-'')) AS LKUP_TYPE, TRIM(TRANSLATE(LKUP_VAL, CHR(10)||CHR(13), ''-'')) AS LKUP_VAL, SRC_STRT_DT, SRC_END_DT, W_INSERT_DT, W_UPDATE_DT FROM MASTERDATA.W_LOOKUP_D WHERE 1=1 AND $CONDITIONS\"""""""" --incremental append --check-column ROW_WID --last-value 10150 --target-dir /user/<USER>bdl/data/sqoop/jobtest --num-mappers 1 --fields-terminated-by \""""""""|\"""""""" --append'""""""""  Validated job definition details stored in PostgreSQL repository  Trying to run EXECUTE In Same fashion.  EXECUTION DEFINITION: job create sqoop_lookup_d_exec_v1 --definition """"""""sqoop --command=job --args='--exec lookupd_job --meta-connect \""""""""jdbc:postgresql://xx.xxx.xx.xx:5433/bdl?user=xxxxx&password=xxxxx\""""""""'""""""""   Stack Trace: batch.stepType org.springframework.batch.core.step.tasklet.TaskletStep batch.taskletType org.springframework.xd.sqoop.SqoopTasklet sqoop.command job --exec lookupd_job --meta-connect """"""""jdbc:postgresql://xx.xxx.xx.xx:5433/bdl?user=xxxxxx&password=xxxxxxx"""""""" -- exec sqoop.errors No job operation specified Try --help for usage instructions. Exception in thread """"""""main"""""""" java.lang.RuntimeException: Sqoop failed - return code 1 at org.springframework.xd.sqoop.SqoopRunner.main(SqoopRunner.java:81) sqoop.log 16:01:45,668 INFO main sqoop.SqoopRunner - Sqoop command: job 16:01:45,668 INFO main sqoop.SqoopRunner - Using args: [--exec, lookupd_job, --meta-connect, """"""""jdbc:postgresql://xx.xxx.xx.xx:5433/bdl?user=xxxxxxx&password=xxxxx""""""""] 16:01:45,668 INFO main sqoop.SqoopRunner - Mapreduce home: /opt/pivotal/spring-xd-1.1.1.RELEASE/xd/lib/phd21 16:01:45,679 INFO main sqoop.SqoopRunner - Setting configuration property: fs.defaultFS=hdfs://xxxxxxxxxxxxx.com:8020 16:01:45,759 INFO main sqoop.SqoopRunner - Setting configuration property: yarn.resourcemanager.address=xxxxxxxxxxxx.com:8032 16:01:45,759 INFO main sqoop.SqoopRunner - Setting configuration property: yarn.application.classpath=$HADOOP_CONF_DIR,$HADOOP_COMMON_HOME/*,$HADOOP_COMMON_HOME/lib/*,$HADOOP_HDFS_HOME/*,$HADOOP_HDFS_HOME/lib/*,$HADOOP_MAPRED_HOME/*,$HADOOP_MAPRED_HOME/lib/*,$HADOOP_YARN_HOME/*,$HADOOP_YARN_HOME/lib/*,$USS_HOME/*,$USS_CONF 16:01:45,759 INFO main sqoop.SqoopRunner - Setting configuration property: mapreduce.framework.name=yarn 16:01:45,817 WARN main tool.SqoopTool - $SQOOP_CONF_DIR has not been set in the environment. Cannot check for additional configuration. 16:01:45,863 INFO main sqoop.Sqoop - Running Sqoop version: 1.4.5  I have tested --list, --exec,--delete they are all erroring same way. Looking at SqoopRunner not sure what makes --create any different from other saved-job functionality.  Please let me know how i can call or execute save job definition's through spring-xd Sqoop tasklet """"""",,5.0
1,XD-3000,FEATURE,Done,MEDIUM,"""Enhance TupleCodec performance""","""Profile TupleCodec and implement performance optimizations""","""""""Profile TupleCodec and implement performance optimizations""""""",,5.0
1,XD-2999,FEATURE,Done,MEDIUM,"""Benchmark with and without JMX activated""","""As a developer, I'd like to benchmark a stream with and without {{JMX}} enabled, so I can test in isolation, and document the differences in performance.""","""""""As a developer, I'd like to benchmark a stream with and without {{JMX}} enabled, so I can test in isolation, and document the differences in performance.""""""",,3.0
1,XD-2998,FEATURE,Done,MEDIUM,"""TupleCode should retain custom formatting settings""","""As a developer, I'd like to handle the non-default {{ConfigurableConversionService}} tuples in an uniform manner, so they're not reset after deserialization. ""","""""""As a developer, I'd like to handle the non-default {{ConfigurableConversionService}} tuples in an uniform manner, so they're not reset after deserialization. """"""",,5.0
1,XD-2996,BUG,Done,HIGH,"""Align Spring XD partitioning with Kafka partitioning for the Kafka message bus""","""As a developer, I want that the Spring XD partitioning process targets Kafka bus partitions directly, so that the design of my stream processing application is easier to understand and the order of messages is not altered  Current situation - Spring XD partitioning logic that builds on top of Kafka partitioning; - The number of Spring XD partitions is not explicitly configured (it's inferred from the number of consumer modules) - If the concurrency of the consumer modules is 1, then Spring XD partitions are matched 1:1 with Kafka partitions; -  If the concurrency of the consumer modules is n, then a Spring XD partition uses n Kafka partitions, and the message bus distributes messages across the Kafka partitions that correspond to the Spring XD partitions; - this could be confusing to the end user, especially if they are used to the Kafka partitioning process; - this can also lead to changes of ordering between messages, as messages within the same Spring XD partitions will be sent to different Kafka partitions (this only happens if the concurrency of the receiving module is higher than 1)  Improvement: - *For the Kafka message bus* the number of Spring XD partitions does not need to be equal to the number of modules (must be higher or equal, though, so that consumers can be created), and should be configured explicitly - using the `partitionCount` property - (as an option, the module count * concurrency can be used as a default)  - as a result, in the case of Kafka there will always be a 1:1 match between Kafka partitions and Spring XD partitions, optionally processed by fewer modules than the partition count;""","""""""As a developer, I want that the Spring XD partitioning process targets Kafka bus partitions directly, so that the design of my stream processing application is easier to understand and the order of messages is not altered  Current situation - Spring XD partitioning logic that builds on top of Kafka partitioning; - The number of Spring XD partitions is not explicitly configured (it's inferred from the number of consumer modules) - If the concurrency of the consumer modules is 1, then Spring XD partitions are matched 1:1 with Kafka partitions; -  If the concurrency of the consumer modules is n, then a Spring XD partition uses n Kafka partitions, and the message bus distributes messages across the Kafka partitions that correspond to the Spring XD partitions; - this could be confusing to the end user, especially if they are used to the Kafka partitioning process; - this can also lead to changes of ordering between messages, as messages within the same Spring XD partitions will be sent to different Kafka partitions (this only happens if the concurrency of the receiving module is higher than 1)  Improvement: - *For the Kafka message bus* the number of Spring XD partitions does not need to be equal to the number of modules (must be higher or equal, though, so that consumers can be created), and should be configured explicitly - using the `partitionCount` property - (as an option, the module count * concurrency can be used as a default)  - as a result, in the case of Kafka there will always be a 1:1 match between Kafka partitions and Spring XD partitions, optionally processed by fewer modules than the partition count;""""""",,5.0
1,XD-2995,BUG,Done,URGENT,"""Jobs are failing to be deployed""","""Error Started: Commit: 7087dc67e058edd6cbb1630ebd95b52e2c7e21e1  https://github.com/spring-projects/spring-xd/pull/1564  This can be reproduced by running the test with a admin and single container on Mac OSX.  Issue All Jobs fail to deploy with the following exception: {noformat} /  ___|          (-)             \ \ / /  _  \ \ `--. _ __  _ __ _ _ __   __ _   \ V /| | | |  `--. \ '_ \| '__| | '_ \ / _` |  / ^ \| | | | /\__/ / |_) | |  | | | | | (_| | / / \ \ |/ / \____/| .__/|_|  |_|_| |_|\__, | \/   \/___/       | |                  __/ |       |_|                 |___/ 1.2.0.BUILD-SNAPSHOT             eXtreme Data   Started : AdminServerApplication Documentation: https://github.com/spring-projects/spring-xd/wiki  2015-04-27 09:08:51,082 1.2.0.SNAP  WARN main config.IntegrationRegistrar - The '#jsonPath' SpEL function cannot be registered. The version of json-path found on the classpath is not supported. Supported json-path version is '0.9.1'. Upgrade to Spring Integration 4.2 or later to use json-path 1.0 or later. 2015-04-27 09:09:02,767 1.2.0.SNAP  INFO main util.XdConfigLoggingInitializer - XD Home: /Users/glennrenfro/project/spring-xd/build/dist/spring-xd/xd 2015-04-27 09:09:02,768 1.2.0.SNAP  INFO main util.XdConfigLoggingInitializer - Transport: redis 2015-04-27 09:09:02,768 1.2.0.SNAP  INFO main util.XdConfigLoggingInitializer - Hadoop Distro: hadoop26 2015-04-27 09:09:02,771 1.2.0.SNAP  INFO main util.XdConfigLoggingInitializer - Hadoop version detected from classpath 2.6.0 2015-04-27 09:09:02,771 1.2.0.SNAP  INFO main util.XdConfigLoggingInitializer - XD config location: file:/Users/glennrenfro/project/spring-xd/build/dist/spring-xd/xd/config// 2015-04-27 09:09:02,771 1.2.0.SNAP  INFO main util.XdConfigLoggingInitializer - XD config names: servers,application 2015-04-27 09:09:02,772 1.2.0.SNAP  INFO LeaderSelector-0 zk.DeploymentSupervisor - Leader Admin 172.31.99.83:9393 is watching for stream/job deployment requests. 2015-04-27 09:09:02,773 1.2.0.SNAP  INFO main util.XdConfigLoggingInitializer - XD module config location: file:/Users/glennrenfro/project/spring-xd/build/dist/spring-xd/xd/config//modules/ 2015-04-27 09:09:02,775 1.2.0.SNAP  INFO main util.XdConfigLoggingInitializer - XD module config name: modules 2015-04-27 09:09:02,775 1.2.0.SNAP  INFO main util.XdConfigLoggingInitializer - Admin web UI: http://Glenns-MacBook-Pro.local:9393/admin-ui 2015-04-27 09:09:02,775 1.2.0.SNAP  INFO main util.XdConfigLoggingInitializer - Zookeeper at: localhost:2181 2015-04-27 09:09:02,775 1.2.0.SNAP  INFO main util.XdConfigLoggingInitializer - Zookeeper namespace: xd 2015-04-27 09:09:02,776 1.2.0.SNAP  INFO main util.XdConfigLoggingInitializer - Analytics: redis 2015-04-27 09:09:02,813 1.2.0.SNAP  INFO DeploymentSupervisor-0 zk.ContainerListener - Path cache event: type=INITIALIZED 2015-04-27 09:09:02,845 1.2.0.SNAP  INFO main admin.AdminServerApplication - Started AdminServerApplication in 6.777 seconds (JVM running for 14.213) 2015-04-27 09:09:04,010 1.2.0.SNAP  INFO DeploymentSupervisor-0 zk.ContainerListener - Path cache event: path=/containers/dc7692b1-979b-4fa3-a11a-3f35cdedc319, type=CHILD_ADDED 2015-04-27 09:09:04,017 1.2.0.SNAP  INFO DeploymentSupervisor-0 zk.ContainerListener - Container arrived: Container{name='dc7692b1-979b-4fa3-a11a-3f35cdedc319', attributes={groups=, host=Glenns-MacBook-Pro.local, id=dc7692b1-979b-4fa3-a11a-3f35cdedc319, managementPort=9395, ip=172.31.99.83, pid=99669}} 2015-04-27 09:09:04,018 1.2.0.SNAP  INFO DeploymentSupervisor-0 zk.ContainerListener - Scheduling deployments to new container(s) in 15000 ms 2015-04-27 09:10:35,003 1.2.0.SNAP  INFO DeploymentSupervisor-0 zk.ZKJobDeploymentHandler - Deployment status for job 'tfphj4ffb45d5-0d6c-4f22-b407-c313ce82b449': DeploymentStatus{state=failed,error(s)=org.springframework.beans.factory.BeanDefinitionStoreException: Invalid bean definition with name 'objectNameProperties' defined in null: Could not resolve placeholder 'xd.stream.name' in string value """"${xd.stream.name}""""; nested exception is java.lang.IllegalArgumentException: Could not resolve placeholder 'xd.stream.name' in string value """"${xd.stream.name}""""  at org.springframework.beans.factory.config.PlaceholderConfigurerSupport.doProcessProperties(PlaceholderConfigurerSupport.java:211)  at org.springframework.context.support.PropertySourcesPlaceholderConfigurer.processProperties(PropertySourcesPlaceholderConfigurer.java:180)  at org.springframework.context.support.PropertySourcesPlaceholderConfigurer.postProcessBeanFactory(PropertySourcesPlaceholderConfigurer.java:155)  at org.springframework.context.support.PostProcessorRegistrationDelegate.invokeBeanFactoryPostProcessors(PostProcessorRegistrationDelegate.java:265)  at org.springframework.context.support.PostProcessorRegistrationDelegate.invokeBeanFactoryPostProcessors(PostProcessorRegistrationDelegate.java:162)  at org.springframework.context.support.AbstractApplicationContext.invokeBeanFactoryPostProcessors(AbstractApplicationContext.java:606)  at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:462)  at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:691)  at org.springframework.boot.SpringApplication.run(SpringApplication.java:321)  at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:139)  at org.springframework.xd.module.core.SimpleModule.initialize(SimpleModule.java:214)  at org.springframework.xd.dirt.module.ModuleDeployer.doDeploy(ModuleDeployer.java:217)  at org.springframework.xd.dirt.module.ModuleDeployer.deploy(ModuleDeployer.java:200)  at org.springframework.xd.dirt.server.container.DeploymentListener.deployModule(DeploymentListener.java:365)  at org.springframework.xd.dirt.server.container.DeploymentListener.deployJobModule(DeploymentListener.java:291)  at org.springframework.xd.dirt.server.container.DeploymentListener.onChildAdded(DeploymentListener.java:181)  at org.springframework.xd.dirt.server.container.DeploymentListener.childEvent(DeploymentListener.java:149)  at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:509)  at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:503)  at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)  at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297)  at org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:83)  at org.apache.curator.framework.recipes.cache.PathChildrenCache.callListeners(PathChildrenCache.java:500)  at org.apache.curator.framework.recipes.cache.EventOperation.invoke(EventOperation.java:35)  at org.apache.curator.framework.recipes.cache.PathChildrenCache$10.run(PathChildrenCache.java:762)  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)  at java.util.concurrent.FutureTask.run(FutureTask.java:262)  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)  at java.util.concurrent.FutureTask.run(FutureTask.java:262)  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)  at java.lang.Thread.run(Thread.java:745) Caused by: java.lang.IllegalArgumentException: Could not resolve placeholder 'xd.stream.name' in string value """"${xd.stream.name}""""  at org.springframework.util.PropertyPlaceholderHelper.parseStringValue(PropertyPlaceholderHelper.java:174)  at org.springframework.util.PropertyPlaceholderHelper.replacePlaceholders(PropertyPlaceholderHelper.java:126)  at org.springframework.core.env.AbstractPropertyResolver.doResolvePlaceholders(AbstractPropertyResolver.java:204)  at org.springframework.core.env.AbstractPropertyResolver.resolveRequiredPlaceholders(AbstractPropertyResolver.java:178)  at org.springframework.context.support.PropertySourcesPlaceholderConfigurer$2.resolveStringValue(PropertySourcesPlaceholderConfigurer.java:175)  at org.springframework.beans.factory.config.BeanDefinitionVisitor.resolveStringValue(BeanDefinitionVisitor.java:282)  at org.springframework.beans.factory.config.BeanDefinitionVisitor.resolveValue(BeanDefinitionVisitor.java:204)  at org.springframework.beans.factory.config.BeanDefinitionVisitor.visitMap(BeanDefinitionVisitor.java:262)  at org.springframework.beans.factory.config.BeanDefinitionVisitor.resolveValue(BeanDefinitionVisitor.java:198)  at org.springframework.beans.factory.config.BeanDefinitionVisitor.visitPropertyValues(BeanDefinitionVisitor.java:141)  at org.springframework.beans.factory.config.BeanDefinitionVisitor.visitBeanDefinition(BeanDefinitionVisitor.java:82)  at org.springframework.beans.factory.config.PlaceholderConfigurerSupport.doProcessProperties(PlaceholderConfigurerSupport.java:208)  ... 31 more } {noformat}""","""""""Error Started: Commit: 7087dc67e058edd6cbb1630ebd95b52e2c7e21e1  https://github.com/spring-projects/spring-xd/pull/1564  This can be reproduced by running the test with a admin and single container on Mac OSX.  Issue All Jobs fail to deploy with the following exception: """"""",""" /  ___|          (-)             \ \ / /  _  \ \ `--. _ __  _ __ _ _ __   __ _   \ V /| | | |  `--. \ '_ \| '__| | '_ \ / _` |  / ^ \| | | | /\__/ / |_) | |  | | | | | (_| | / / \ \ |/ / \____/| .__/|_|  |_|_| |_|\__, | \/   \/___/       | |                  __/ |       |_|                 |___/ 1.2.0.BUILD-SNAPSHOT             eXtreme Data   Started : AdminServerApplication Documentation: https://github.com/spring-projects/spring-xd/wiki  2015-04-27 09:08:51,082 1.2.0.SNAP  WARN main config.IntegrationRegistrar - The '#jsonPath' SpEL function cannot be registered. The version of json-path found on the classpath is not supported. Supported json-path version is '0.9.1'. Upgrade to Spring Integration 4.2 or later to use json-path 1.0 or later. 2015-04-27 09:09:02,767 1.2.0.SNAP  INFO main util.XdConfigLoggingInitializer - XD Home: /Users/glennrenfro/project/spring-xd/build/dist/spring-xd/xd 2015-04-27 09:09:02,768 1.2.0.SNAP  INFO main util.XdConfigLoggingInitializer - Transport: redis 2015-04-27 09:09:02,768 1.2.0.SNAP  INFO main util.XdConfigLoggingInitializer - Hadoop Distro: hadoop26 2015-04-27 09:09:02,771 1.2.0.SNAP  INFO main util.XdConfigLoggingInitializer - Hadoop version detected from classpath 2.6.0 2015-04-27 09:09:02,771 1.2.0.SNAP  INFO main util.XdConfigLoggingInitializer - XD config location: file:/Users/glennrenfro/project/spring-xd/build/dist/spring-xd/xd/config// 2015-04-27 09:09:02,771 1.2.0.SNAP  INFO main util.XdConfigLoggingInitializer - XD config names: servers,application 2015-04-27 09:09:02,772 1.2.0.SNAP  INFO LeaderSelector-0 zk.DeploymentSupervisor - Leader Admin 172.31.99.83:9393 is watching for stream/job deployment requests. 2015-04-27 09:09:02,773 1.2.0.SNAP  INFO main util.XdConfigLoggingInitializer - XD module config location: file:/Users/glennrenfro/project/spring-xd/build/dist/spring-xd/xd/config//modules/ 2015-04-27 09:09:02,775 1.2.0.SNAP  INFO main util.XdConfigLoggingInitializer - XD module config name: modules 2015-04-27 09:09:02,775 1.2.0.SNAP  INFO main util.XdConfigLoggingInitializer - Admin web UI: http://Glenns-MacBook-Pro.local:9393/admin-ui 2015-04-27 09:09:02,775 1.2.0.SNAP  INFO main util.XdConfigLoggingInitializer - Zookeeper at: localhost:2181 2015-04-27 09:09:02,775 1.2.0.SNAP  INFO main util.XdConfigLoggingInitializer - Zookeeper namespace: xd 2015-04-27 09:09:02,776 1.2.0.SNAP  INFO main util.XdConfigLoggingInitializer - Analytics: redis 2015-04-27 09:09:02,813 1.2.0.SNAP  INFO DeploymentSupervisor-0 zk.ContainerListener - Path cache event: type=INITIALIZED 2015-04-27 09:09:02,845 1.2.0.SNAP  INFO main admin.AdminServerApplication - Started AdminServerApplication in 6.777 seconds (JVM running for 14.213) 2015-04-27 09:09:04,010 1.2.0.SNAP  INFO DeploymentSupervisor-0 zk.ContainerListener - Path cache event: path=/containers/dc7692b1-979b-4fa3-a11a-3f35cdedc319, type=CHILD_ADDED 2015-04-27 09:09:04,017 1.2.0.SNAP  INFO DeploymentSupervisor-0 zk.ContainerListener - Container arrived: Container{name='dc7692b1-979b-4fa3-a11a-3f35cdedc319', attributes={groups=, host=Glenns-MacBook-Pro.local, id=dc7692b1-979b-4fa3-a11a-3f35cdedc319, managementPort=9395, ip=172.31.99.83, pid=99669}} 2015-04-27 09:09:04,018 1.2.0.SNAP  INFO DeploymentSupervisor-0 zk.ContainerListener - Scheduling deployments to new container(s) in 15000 ms 2015-04-27 09:10:35,003 1.2.0.SNAP  INFO DeploymentSupervisor-0 zk.ZKJobDeploymentHandler - Deployment status for job 'tfphj4ffb45d5-0d6c-4f22-b407-c313ce82b449': DeploymentStatus{state=failed,error(s)=org.springframework.beans.factory.BeanDefinitionStoreException: Invalid bean definition with name 'objectNameProperties' defined in null: Could not resolve placeholder 'xd.stream.name' in string value """"""""${xd.stream.name}""""""""; nested exception is java.lang.IllegalArgumentException: Could not resolve placeholder 'xd.stream.name' in string value """"""""${xd.stream.name}""""""""  at org.springframework.beans.factory.config.PlaceholderConfigurerSupport.doProcessProperties(PlaceholderConfigurerSupport.java:211)  at org.springframework.context.support.PropertySourcesPlaceholderConfigurer.processProperties(PropertySourcesPlaceholderConfigurer.java:180)  at org.springframework.context.support.PropertySourcesPlaceholderConfigurer.postProcessBeanFactory(PropertySourcesPlaceholderConfigurer.java:155)  at org.springframework.context.support.PostProcessorRegistrationDelegate.invokeBeanFactoryPostProcessors(PostProcessorRegistrationDelegate.java:265)  at org.springframework.context.support.PostProcessorRegistrationDelegate.invokeBeanFactoryPostProcessors(PostProcessorRegistrationDelegate.java:162)  at org.springframework.context.support.AbstractApplicationContext.invokeBeanFactoryPostProcessors(AbstractApplicationContext.java:606)  at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:462)  at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:691)  at org.springframework.boot.SpringApplication.run(SpringApplication.java:321)  at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:139)  at org.springframework.xd.module.core.SimpleModule.initialize(SimpleModule.java:214)  at org.springframework.xd.dirt.module.ModuleDeployer.doDeploy(ModuleDeployer.java:217)  at org.springframework.xd.dirt.module.ModuleDeployer.deploy(ModuleDeployer.java:200)  at org.springframework.xd.dirt.server.container.DeploymentListener.deployModule(DeploymentListener.java:365)  at org.springframework.xd.dirt.server.container.DeploymentListener.deployJobModule(DeploymentListener.java:291)  at org.springframework.xd.dirt.server.container.DeploymentListener.onChildAdded(DeploymentListener.java:181)  at org.springframework.xd.dirt.server.container.DeploymentListener.childEvent(DeploymentListener.java:149)  at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:509)  at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:503)  at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)  at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297)  at org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:83)  at org.apache.curator.framework.recipes.cache.PathChildrenCache.callListeners(PathChildrenCache.java:500)  at org.apache.curator.framework.recipes.cache.EventOperation.invoke(EventOperation.java:35)  at org.apache.curator.framework.recipes.cache.PathChildrenCache$10.run(PathChildrenCache.java:762)  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)  at java.util.concurrent.FutureTask.run(FutureTask.java:262)  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)  at java.util.concurrent.FutureTask.run(FutureTask.java:262)  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)  at java.lang.Thread.run(Thread.java:745) Caused by: java.lang.IllegalArgumentException: Could not resolve placeholder 'xd.stream.name' in string value """"""""${xd.stream.name}""""""""  at org.springframework.util.PropertyPlaceholderHelper.parseStringValue(PropertyPlaceholderHelper.java:174)  at org.springframework.util.PropertyPlaceholderHelper.replacePlaceholders(PropertyPlaceholderHelper.java:126)  at org.springframework.core.env.AbstractPropertyResolver.doResolvePlaceholders(AbstractPropertyResolver.java:204)  at org.springframework.core.env.AbstractPropertyResolver.resolveRequiredPlaceholders(AbstractPropertyResolver.java:178)  at org.springframework.context.support.PropertySourcesPlaceholderConfigurer$2.resolveStringValue(PropertySourcesPlaceholderConfigurer.java:175)  at org.springframework.beans.factory.config.BeanDefinitionVisitor.resolveStringValue(BeanDefinitionVisitor.java:282)  at org.springframework.beans.factory.config.BeanDefinitionVisitor.resolveValue(BeanDefinitionVisitor.java:204)  at org.springframework.beans.factory.config.BeanDefinitionVisitor.visitMap(BeanDefinitionVisitor.java:262)  at org.springframework.beans.factory.config.BeanDefinitionVisitor.resolveValue(BeanDefinitionVisitor.java:198)  at org.springframework.beans.factory.config.BeanDefinitionVisitor.visitPropertyValues(BeanDefinitionVisitor.java:141)  at org.springframework.beans.factory.config.BeanDefinitionVisitor.visitBeanDefinition(BeanDefinitionVisitor.java:82)  at org.springframework.beans.factory.config.PlaceholderConfigurerSupport.doProcessProperties(PlaceholderConfigurerSupport.java:208)  ... 31 more } """,1.0
1,XD-2994,FEATURE,Done,MEDIUM,"""Verify module count works on 10+ Containers""","""This tests should prove that Kafka can handle a module count greater than one with the proper concurrency settings. load-generator should be used as the foundation for this test with the following settings: module.load-generator.count=10,module.throughput.consumer.concurrency=10  An environment should be provisioned to support the containers, Zookeeper and Kafka. ""","""""""This tests should prove that Kafka can handle a module count greater than one with the proper concurrency settings. load-generator should be used as the foundation for this test with the following settings: module.load-generator.count=10,module.throughput.consumer.concurrency=10  An environment should be provisioned to support the containers, Zookeeper and Kafka. """"""",,3.0
1,XD-2993,FEATURE,Done,MEDIUM,"""Add a new variation of DSL parser for Flo""","""As a Flo developer, I'd like to have a new DSL parser, so I can easily  detect incorrect module/option values when supplied from the Flo UI.  Example: MyStream = mail | log tap:stream:MyStream.bar > log  If parsed separately (which Flo UI does), the current parser endpoint will barf on the second stream because it doesnt know about the first stream (MyStream). ""","""""""As a Flo developer, I'd like to have a new DSL parser, so I can easily  detect incorrect module/option values when supplied from the Flo UI.  Example: MyStream = mail | log tap:stream:MyStream.bar > log  If parsed separately (which Flo UI does), the current parser endpoint will barf on the second stream because it doesnt know about the first stream (MyStream). """"""",,8.0
1,XD-2992,FEATURE,Done,MEDIUM,"""Add support for multiple topics in Kafka source""","""As a user, I'd like to consume multiple topic-partitions, so I can have the option to consume from multiple data endpoints and still be able to serve the data via single queue.""","""""""As a user, I'd like to consume multiple topic-partitions, so I can have the option to consume from multiple data endpoints and still be able to serve the data via single queue.""""""",,3.0
1,XD-2987,FEATURE,Done,URGENT,"""Update acceptance test to use new JMX Module name format""","""the update to the JMX was introduced in XD-2941.   Also noticed that we should have been checking source and not sink .  This was also resolved.""","""""""the update to the JMX was introduced in XD-2941.   Also noticed that we should have been checking source and not sink .  This was also resolved.""""""",,2.0
1,XD-2986,FEATURE,Done,MEDIUM,"""Experiment with re-parsing of streams when needed""","""As a follow up to XD-2877, experiment with the removal of the list of modules from BaseDefinition and reparse as needed.  Branch is here: https://github.com/<USER>spring-xd/tree/deploy-refactor-2""","""""""As a follow up to XD-2877, experiment with the removal of the list of modules from BaseDefinition and reparse as needed.  Branch is here: https://github.com/<USER>spring-xd/tree/deploy-refactor-2""""""",,8.0
1,XD-2985,FEATURE,Done,MEDIUM,"""Update spring-data-hadoop to version 2.1.2.RELEASE""","""Update spring-data-hadoop to version 2.1.2.RELEASE""","""Update spring-data-hadoop to version 2.1.2.RELEASE""",,2.0
1,XD-2984,BUG,Done,URGENT,"""xd-admin script fails when providing --hadoopDistro option""","""XD-2837 added back the --hadoopDistro option for xd-admin scripts. However, if I try to use it I get an error message saying: """"--hadoopDistro"""" is not a valid option ""","""""""XD-2837 added back the --hadoopDistro option for xd-admin scripts. However, if I try to use it I get an error message saying: """"""""--hadoopDistro"""""""" is not a valid option """"""",,3.0
1,XD-2980,FEATURE,Done,MEDIUM,"""Create Boot based ModuleRunner (phase 2)""","""As a user, I'd like to use Boot-based {{ModuleRunner}} for use in container-managed environments, so I can run XD without _xd-containers_.  Scope: * Complete the remaining deployment properties work""","""""""As a user, I'd like to use Boot-based {{ModuleRunner}} for use in container-managed environments, so I can run XD without _xd-containers_.  Scope: * Complete the remaining deployment properties work""""""",,8.0
1,XD-2979,FEATURE,Done,MEDIUM,"""Submit java receptor client for CF incubation""","""As a user, I'd like to use the Java receptor client, so I can interact with Diego runtime using the Java receptor REST APIs.""","""""""As a user, I'd like to use the Java receptor client, so I can interact with Diego runtime using the Java receptor REST APIs.""""""",,8.0
1,XD-2973,FEATURE,Done,MEDIUM,"""Run the sqoop job against secured cluster""","""As a user, I'd like to run the sqoop jobs against secured hdfs cluster, so I can restrict access to only authorized users. ""","""""""As a user, I'd like to run the sqoop jobs against secured hdfs cluster, so I can restrict access to only authorized users. """"""",,3.0
1,XD-2972,BUG,Done,HIGH,"""STS - Spring XD Imported with Compilation Error""","""Cannot import Spring XD into STS without compilation errors in class:  *org.springframework.xd.dirt.rest.ModulesController#list*  Error is in:  {code} return assembler.toResource(page, detailed ? detailedAssembler: simpleAssembler); {code}  {code} The method toResource(Page<ModuleDefinition>, Link) in the type PagedResourcesAssembler<ModuleDefinition> is not applicable for the arguments (Page<ModuleDefinition>, (detailed ? detailedAssembler : simpleAssembler)) {code}  Seems to be an STS specific issue.""","""""""Cannot import Spring XD into STS without compilation errors in class:  *org.springframework.xd.dirt.rest.ModulesController#list*  Error is in:      Seems to be an STS specific issue.""""""",""" return assembler.toResource(page, detailed ? detailedAssembler: simpleAssembler);  The method toResource(Page<ModuleDefinition>, Link) in the type PagedResourcesAssembler<ModuleDefinition> is not applicable for the arguments (Page<ModuleDefinition>, (detailed ? detailedAssembler : simpleAssembler)) """,3.0
1,XD-2971,FEATURE,Done,MEDIUM,"""Document the use of properties file as deployment manifest""","""As a user, I'd like to refer to the documentation to configure the properties file, so I can use it as recommended to represent the deployment manifest.""","""""""As a user, I'd like to refer to the documentation to configure the properties file, so I can use it as recommended to represent the deployment manifest.""""""",,1.0
1,XD-2970,FEATURE,Done,HIGH,"""Standardize XD logging to align with Spring Boot""","""In XD today we use  commons-logging or slf4j  APIs bound to log4j at runtime (configured with log4j.properties).    Boot uses slf4j APIs backed by logback. This causes some build incompatibilities building a component that depends on spring-xd-dirt and spring-boot, requiring specific dependency exclusions.  In order to simplify building and troubleshooting log dependencies, XD should standardize on  slf4j APIs (replace any commons-logging Loggers with Slf4j). This is internal only, and would not impact users who are used to seeing log4j.properties. An additional step is to replace log4j with logback. This change would be visible to end users but will provide us greater affinity with boot and improve the developer experience. If we make this change it should go into 1.2 GA.  ""","""""""In XD today we use  commons-logging or slf4j  APIs bound to log4j at runtime (configured with log4j.properties).    Boot uses slf4j APIs backed by logback. This causes some build incompatibilities building a component that depends on spring-xd-dirt and spring-boot, requiring specific dependency exclusions.  In order to simplify building and troubleshooting log dependencies, XD should standardize on  slf4j APIs (replace any commons-logging Loggers with Slf4j). This is internal only, and would not impact users who are used to seeing log4j.properties. An additional step is to replace log4j with logback. This change would be visible to end users but will provide us greater affinity with boot and improve the developer experience. If we make this change it should go into 1.2 GA.  """"""",,8.0
1,XD-2964,FEATURE,Done,MEDIUM,"""Spike: Come up with a design for stateful stream processing""","""As a developer, I'd like to study the state management requirements, so I can brainstorm and identify the design to natively add _stateful_ stream processing support in XD. ""","""""""As a developer, I'd like to study the state management requirements, so I can brainstorm and identify the design to natively add _stateful_ stream processing support in XD. """"""",,8.0
1,XD-2962,FEATURE,Done,MEDIUM,"""Document performance benchmark results""","""As a developer, I'd like to document performance benchmark results along with the infrastructure specifics, so I can publish the blog for customers/users to use it as a reference while setting up Spring XD cluster.""","""""""As a developer, I'd like to document performance benchmark results along with the infrastructure specifics, so I can publish the blog for customers/users to use it as a reference while setting up Spring XD cluster.""""""",,8.0
1,XD-2961,FEATURE,Done,MEDIUM,"""Benchmark against Kafka 0.8.2 release""","""As a developer, I'd like to rerun _baseline_, _Tuple_, and _Serialized_ payloads, so I can compare the differences in performance between 0.8.1 and 0.8.2 Kafka releases.   Note: 1.1.1 > Benched against 0.8.1  1.2 > Benched against 0.8.2   ""","""""""As a developer, I'd like to rerun _baseline_, _Tuple_, and _Serialized_ payloads, so I can compare the differences in performance between 0.8.1 and 0.8.2 Kafka releases.   Note: 1.1.1 > Benched against 0.8.1  1.2 > Benched against 0.8.2   """"""",,8.0
1,XD-2958,FEATURE,Done,MEDIUM,"""Upgrade to Kafka 0.8.2""","""As a developer, I'd like to upgrade to Kafka 0.8.2, so I can leverage the latest features in order to test the performance characteristics.""","""""""As a developer, I'd like to upgrade to Kafka 0.8.2, so I can leverage the latest features in order to test the performance characteristics.""""""",,8.0
1,XD-2957,FEATURE,Done,MEDIUM,"""Create samples and document Kryo optimization guidelines""","""As a developer, I'd like to document the Kryo optimization guidelines, so the end-users can refer to it while tuning to improve performance.""","""""""As a developer, I'd like to document the Kryo optimization guidelines, so the end-users can refer to it while tuning to improve performance.""""""",,3.0
1,XD-2956,FEATURE,Done,MEDIUM,"""Revisit the requirement for ID and Timestamp attributes in Tuple""","""As a developer, I'd like revisit the design to determine the necessity for _ID_ and _TimeStamp_ attributes in {{Tuple}}, so I can refactor in order to improve performance throughput.""","""""""As a developer, I'd like revisit the design to determine the necessity for _ID_ and _TimeStamp_ attributes in {{Tuple}}, so I can refactor in order to improve performance throughput.""""""",,1.0
1,XD-2955,FEATURE,Done,MEDIUM,"""Refactor MessageBus to avoid unnecessary use of MessageBuilder  ""","""As a developer, I'd like to refactor the programmatic means by which the MessageBus transforms the Message so throughput performance can be optimized. ""","""""""As a developer, I'd like to refactor the programmatic means by which the MessageBus transforms the Message so throughput performance can be optimized. """"""",,5.0
1,XD-2953,FEATURE,Done,MEDIUM,"""Get rid of SparkStreamingDriverModule""","""Code that is in there could be moved to the SparkStreamingModule.  Then, as part of a later refactoring, that plugin should be made part of the module (and loaded by the module classloader)""","""""""Code that is in there could be moved to the SparkStreamingModule.  Then, as part of a later refactoring, that plugin should be made part of the module (and loaded by the module classloader)""""""",,3.0
1,XD-2949,FEATURE,Done,MEDIUM,"""Error Message for """"Missing Job Description"""" needs to be updated""","""When using the rest interface to create a Job with an empty description, used to generate the following exception, """"Definition can not be empty"""".   Now generates """"XD112E:(pos 0): Unexpectedly ran out of input^"""".  The correct error should be, """"definition cannot be blank or null""""  ""","""""""When using the rest interface to create a Job with an empty description, used to generate the following exception, """"""""Definition can not be empty"""""""".   Now generates """"""""XD112E:(pos 0): Unexpectedly ran out of input^"""""""".  The correct error should be, """"""""definition cannot be blank or null""""""""  """"""",,2.0
1,XD-2948,IMPROVEMENT,Done,MEDIUM,"""Document how to specify custom-modules location via Environment variable.""","""It is possible to specify the location of custom modules via the environment variable {{XD_CUSTOMMODULE_HOME}} which is provided by Spring Boot property key derivation mechanism (in this case derived from {{xd.customModule.home}}).  This allows a user to specify a custom modules location that survives a complete wipe of spring-xd installations.""","""""""It is possible to specify the location of custom modules via the environment variable {{XD_CUSTOMMODULE_HOME}} which is provided by Spring Boot property key derivation mechanism (in this case derived from {{xd.customModule.home}}).  This allows a user to specify a custom modules location that survives a complete wipe of spring-xd installations.""""""",,1.0
1,XD-2947,FEATURE,Done,MEDIUM,"""Add support for expressions and dynamically evaluate at runtime""","""As a user, I'd like to have the ability to use expressions, so I can dynamically name directories/files based on the timestamp or other intermediate data point.""","""""""As a user, I'd like to have the ability to use expressions, so I can dynamically name directories/files based on the timestamp or other intermediate data point.""""""",,5.0
1,XD-2942,IMPROVEMENT,Done,MEDIUM,"""Add ftp source to default source modules""","""It would be nice to have a simple ftp source. I have to do it for one of my projects. Same as XD-2139 but for source modules.""","""""""It would be nice to have a simple ftp source. I have to do it for one of my projects. Same as XD-2139 but for source modules.""""""",,2.0
1,XD-2941,BUG,Done,HIGH,"""Failure to get message rates for modules with labels.""","""Start XD distributed XD with specified management port and xd:   messageRateMonitoring:     enabled: true in servers.yml to gather stats.  Create stream {{file | log}}, deploy it, navigate to Containers tab in Admin UI. Rates are shown correctly. Create stream {{MYFILE: file | log}}, deploy it, navigate to Containers tab in Admin UI - none of the message rates are shown. Open browser dev tools console and note 500 error response.  spring-xd-dirt -> ContainersController lines 109-112 creates request to get message rates for modules.  Typical request: {{http://192.168.0.10:9292/management/jolokia/read/xd.str4:module=log.*,component=*,name=input/MeanSendRate}}  Typical response: {code:json} {""""request"""":{""""mbean"""":""""xd.str4:component=*,module=log.1,name=input"""",""""attribute"""":""""MeanSendRate"""",""""type"""":""""read""""},""""value"""":{""""xd.str4:component=MessageChannel,module=log.1,name=input"""":{""""MeanSendRate"""":0.0}},""""timestamp"""":1428675070,""""status"""":200} {code}  For file module with label `MYFILE` the request is: {{http://192.168.0.10:9292/management/jolokia/read/xd.str4:module=MYFILE.*,component=*,name=output/MeanSendRate}}  Response: {code:json} {""""mbean"""":""""xd.str4:component=*,module=MYFILE.1,name=output"""",""""attribute"""":""""MeanSendRate"""",""""type"""":""""read""""},""""stacktrace"""":""""javax.management.InstanceNotFoundException: No MBean with pattern xd.str4:module=MYFILE.1,component=*,name=output found for reading attributes\n\tat org.jolokia.handler.ReadHandler.searchMBeans(ReadHandler.java:160)\n\tat org.jolokia.handler.ReadHandler.fetchAttributesForMBeanPattern(ReadHandler.java:126)\n\tat org.jolokia.handler.ReadHandler.doHandleRequest(ReadHandler.java:116)\n\tat org.jolokia.handler.ReadHandler.doHandleRequest(ReadHandler.java:37)\n\tat org.jolokia.handler.JsonRequestHandler.handleRequest(JsonRequestHandler.java:160)\n\tat org.jolokia.backend.MBeanServerHandler.dispatchRequest(MBeanServerHandler.java:97)\n\tat org.jolokia.backend.LocalRequestDispatcher.dispatchRequest(LocalRequestDispatcher.java:98)\n\tat org.jolokia.backend.BackendManager.callRequestDispatcher(BackendManager.java:411)\n\tat org.jolokia.backend.BackendManager.handleRequest(BackendManager.java:158)\n\tat org.jolokia.http.HttpRequestHandler.executeRequest(HttpRequestHandler.java:197)\n\tat org.jolokia.http.HttpRequestHandler.handleGetRequest(HttpRequestHandler.java:86)\n\tat org.jolokia.http.AgentServlet$4.handleRequest(AgentServlet.java:435)\n\tat org.jolokia.http.AgentServlet.handleSecurely(AgentServlet.java:320)\n\tat org.jolokia.http.AgentServlet.handle(AgentServlet.java:291)\n\tat org.jolokia.http.AgentServlet.doGet(AgentServlet.java:252)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:735)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:848)\n\tat org.springframework.web.servlet.mvc.ServletWrappingController.handleRequestInternal(ServletWrappingController.java:158)\n\tat org.springframework.web.servlet.mvc.AbstractController.handleRequest(AbstractController.java:146)\n\tat org.springframework.boot.actuate.endpoint.mvc.JolokiaMvcEndpoint.handle(JolokiaMvcEndpoint.java:130)\n\tat sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:483)\n\tat org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:221)\n\tat org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:137)\n\tat org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:110)\n\tat org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandleMethod(RequestMappingHandlerAdapter.java:777)\n\tat org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:706)\n\tat org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:85)\n\tat org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:943)\n\tat org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:877)\n\tat org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:966)\n\tat org.springframework.web.servlet.FrameworkServlet.doGet(FrameworkServlet.java:857)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:735)\n\tat org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:842)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:848)\n\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:684)\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1496)\n\tat org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:186)\n\tat org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:160)\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)\n\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:499)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:137)\n\tat org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:557)\n\tat org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:231)\n\tat org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1086)\n\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:428)\n\tat org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:193)\n\tat org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1020)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)\n\tat org.eclipse.jetty.server.Server.handle(Server.java:370)\n\tat org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:494)\n\tat org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:971)\n\tat org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:1033)\n\tat org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:644)\n\tat org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235)\n\tat org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82)\n\tat org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:667)\n\tat org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:52)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)\n\tat java.lang.Thread.run(Thread.java:744)\n"""",""""error_type"""":""""javax.management.InstanceNotFoundException"""",""""error"""":""""javax.management.InstanceNotFoundException : No MBean with pattern xd.str4:module=MYFILE.1,component=*,name=output found for reading attributes"""",""""status"""":404} {code}  This reponse results in JSONException in the ContainersController because it's missing 'value' property.  The module id is somewhat problematic in the request: {{xd.str4:module=log.*}} index is {{\*}} but should be index within the stream, also node type (source/sink/processor) is missing. Therefore, stream {{mail | mail}} is suffering from the same problem.  Would be nice to have some sort of a bulk request to query more than one module for input/output message rates, such that I could get all message rates for modules in the stream.""","""""""Start XD distributed XD with specified management port and xd:   messageRateMonitoring:     enabled: true in servers.yml to gather stats.  Create stream {{file | log}}, deploy it, navigate to Containers tab in Admin UI. Rates are shown correctly. Create stream {{MYFILE: file | log}}, deploy it, navigate to Containers tab in Admin UI - none of the message rates are shown. Open browser dev tools console and note 500 error response.  spring-xd-dirt -> ContainersController lines 109-112 creates request to get message rates for modules.  Typical request: {{http://192.168.0.10:9292/management/jolokia/read/xd.str4:module=log.*,component=*,name=input/MeanSendRate}}  Typical response: {code:json} {""""""""request"""""""":{""""""""mbean"""""""":""""""""xd.str4:component=*,module=log.1,name=input"""""""",""""""""attribute"""""""":""""""""MeanSendRate"""""""",""""""""type"""""""":""""""""read""""""""},""""""""value"""""""":{""""""""xd.str4:component=MessageChannel,module=log.1,name=input"""""""":{""""""""MeanSendRate"""""""":0.0}},""""""""timestamp"""""""":1428675070,""""""""status"""""""":200}   This reponse results in JSONException in the ContainersController because it's missing 'value' property.  The module id is somewhat problematic in the request: {{xd.str4:module=log.*}} index is {{\*}} but should be index within the stream, also node type (source/sink/processor) is missing. Therefore, stream {{mail | mail}} is suffering from the same problem.  Would be nice to have some sort of a bulk request to query more than one module for input/output message rates, such that I could get all message rates for modules in the stream.""""""",""" {""""""""request"""""""":{""""""""mbean"""""""":""""""""xd.str4:component=*,module=log.1,name=input"""""""",""""""""attribute"""""""":""""""""MeanSendRate"""""""",""""""""type"""""""":""""""""read""""""""},""""""""value"""""""":{""""""""xd.str4:component=MessageChannel,module=log.1,name=input"""""""":{""""""""MeanSendRate"""""""":0.0}},""""""""timestamp"""""""":1428675070,""""""""status"""""""":200}  {""""""""mbean"""""""":""""""""xd.str4:component=*,module=MYFILE.1,name=output"""""""",""""""""attribute"""""""":""""""""MeanSendRate"""""""",""""""""type"""""""":""""""""read""""""""},""""""""stacktrace"""""""":""""""""javax.management.InstanceNotFoundException: No MBean with pattern xd.str4:module=MYFILE.1,component=*,name=output found for reading attributes\n\tat org.jolokia.handler.ReadHandler.searchMBeans(ReadHandler.java:160)\n\tat org.jolokia.handler.ReadHandler.fetchAttributesForMBeanPattern(ReadHandler.java:126)\n\tat org.jolokia.handler.ReadHandler.doHandleRequest(ReadHandler.java:116)\n\tat org.jolokia.handler.ReadHandler.doHandleRequest(ReadHandler.java:37)\n\tat org.jolokia.handler.JsonRequestHandler.handleRequest(JsonRequestHandler.java:160)\n\tat org.jolokia.backend.MBeanServerHandler.dispatchRequest(MBeanServerHandler.java:97)\n\tat org.jolokia.backend.LocalRequestDispatcher.dispatchRequest(LocalRequestDispatcher.java:98)\n\tat org.jolokia.backend.BackendManager.callRequestDispatcher(BackendManager.java:411)\n\tat org.jolokia.backend.BackendManager.handleRequest(BackendManager.java:158)\n\tat org.jolokia.http.HttpRequestHandler.executeRequest(HttpRequestHandler.java:197)\n\tat org.jolokia.http.HttpRequestHandler.handleGetRequest(HttpRequestHandler.java:86)\n\tat org.jolokia.http.AgentServlet$4.handleRequest(AgentServlet.java:435)\n\tat org.jolokia.http.AgentServlet.handleSecurely(AgentServlet.java:320)\n\tat org.jolokia.http.AgentServlet.handle(AgentServlet.java:291)\n\tat org.jolokia.http.AgentServlet.doGet(AgentServlet.java:252)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:735)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:848)\n\tat org.springframework.web.servlet.mvc.ServletWrappingController.handleRequestInternal(ServletWrappingController.java:158)\n\tat org.springframework.web.servlet.mvc.AbstractController.handleRequest(AbstractController.java:146)\n\tat org.springframework.boot.actuate.endpoint.mvc.JolokiaMvcEndpoint.handle(JolokiaMvcEndpoint.java:130)\n\tat sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:483)\n\tat org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:221)\n\tat org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:137)\n\tat org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:110)\n\tat org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandleMethod(RequestMappingHandlerAdapter.java:777)\n\tat org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:706)\n\tat org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:85)\n\tat org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:943)\n\tat org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:877)\n\tat org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:966)\n\tat org.springframework.web.servlet.FrameworkServlet.doGet(FrameworkServlet.java:857)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:735)\n\tat org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:842)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:848)\n\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:684)\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1496)\n\tat org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:186)\n\tat org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:160)\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)\n\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:499)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:137)\n\tat org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:557)\n\tat org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:231)\n\tat org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1086)\n\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:428)\n\tat org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:193)\n\tat org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1020)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)\n\tat org.eclipse.jetty.server.Server.handle(Server.java:370)\n\tat org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:494)\n\tat org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:971)\n\tat org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:1033)\n\tat org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:644)\n\tat org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235)\n\tat org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82)\n\tat org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:667)\n\tat org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:52)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)\n\tat java.lang.Thread.run(Thread.java:744)\n"""""""",""""""""error_type"""""""":""""""""javax.management.InstanceNotFoundException"""""""",""""""""error"""""""":""""""""javax.management.InstanceNotFoundException : No MBean with pattern xd.str4:module=MYFILE.1,component=*,name=output found for reading attributes"""""""",""""""""status"""""""":404} """,3.0
1,XD-2939,BUG,Done,HIGH,"""All Modules are undeployed on Zookeeper Connection Loss / GC Pause""","""We are currently running single node mode and experiencing the same problem as described here: http://stackoverflow.com/questions/28170864/spring-xd-jobs-automatic-undeployment-on-zookeeper-time-out-in-xd-singlenode-mo  I've turned on GC logs and can see that there is a 29.7 second GC pause around the time when this happens. We've already set the Zookeeper timeouts (as suggested in the stackoverflow question) - without effect - we can just see, that after the configured timeout the ConnectionLoss errors start to appear.  Sorry for the priorization - for us this currently is a major issue since we are running in singlenode mode (as a starter) and our system goes down once a day. Would this behavior change if we switch to distributed mode ?  I know that a GC pause of 29 secs is really long, however, I've already seen such pauses for batch systems pretty often. Long running jobs tend to move objects to older generations and sometimes there isn't much of a chance to do something against it. So I guess it's worth considering this in the behavior of XD ?""","""""""We are currently running single node mode and experiencing the same problem as described here: http://stackoverflow.com/questions/28170864/spring-xd-jobs-automatic-undeployment-on-zookeeper-time-out-in-xd-singlenode-mo  I've turned on GC logs and can see that there is a 29.7 second GC pause around the time when this happens. We've already set the Zookeeper timeouts (as suggested in the stackoverflow question) - without effect - we can just see, that after the configured timeout the ConnectionLoss errors start to appear.  Sorry for the priorization - for us this currently is a major issue since we are running in singlenode mode (as a starter) and our system goes down once a day. Would this behavior change if we switch to distributed mode ?  I know that a GC pause of 29 secs is really long, however, I've already seen such pauses for batch systems pretty often. Long running jobs tend to move objects to older generations and sometimes there isn't much of a chance to do something against it. So I guess it's worth considering this in the behavior of XD ?""""""",,0.0
1,XD-2938,MAINTENANCE,Done,HIGH,"""Sqoop - Unable to create job using MERGE command""","""As a user, I need to use XD Sqoop module to support the merge command. Currently, the SqoopRunner createFinalArguments method forces the requirement for connect, username and password options which are not valid for the merge option. A check of the module type to not force these options being assigned to sqoop arg list would be preferred""","""""""As a user, I need to use XD Sqoop module to support the merge command. Currently, the SqoopRunner createFinalArguments method forces the requirement for connect, username and password options which are not valid for the merge option. A check of the module type to not force these options being assigned to sqoop arg list would be preferred""""""",,5.0
1,XD-2935,FEATURE,Done,MEDIUM,"""Parameterize Merge Options""","""As a user, I'd like to parameterize Merge Options, so I can incrementally consume the delta with the help of megastore.  ""","""""""As a user, I'd like to parameterize Merge Options, so I can incrementally consume the delta with the help of megastore.  """"""",,5.0
1,XD-2934,FEATURE,Done,MEDIUM,"""Parameterize codegen options""","""As a user, I'd like to parameterize CodeGen Options, so I can generate code on the fly as needed.   ""","""""""As a user, I'd like to parameterize CodeGen Options, so I can generate code on the fly as needed.   """"""",,5.0
1,XD-2933,FEATURE,Done,MEDIUM,"""Parameterize import options for Sqoop""","""As a user, I'd like to parameterize all Import Options, so I can eliminate the need for {{args}} option since it gets confusing.""","""""""As a user, I'd like to parameterize all Import Options, so I can eliminate the need for {{args}} option since it gets confusing.""""""",,2.0
1,XD-2931,BUG,Done,URGENT,"""Login page is missing style info when secured""","""If the admin UI is secured, the login page is displayed without any styles.""","""""""If the admin UI is secured, the login page is displayed without any styles.""""""",,1.0
1,XD-2929,FEATURE,Done,MEDIUM,"""Document the use of nested jobs with example""","""As a developer, I'd like to document how to nest batch jobs and workflows in XD, so it will be easy for end-users to use it as reference. ""","""""""As a developer, I'd like to document how to nest batch jobs and workflows in XD, so it will be easy for end-users to use it as reference. """"""",,1.0
1,XD-2928,FEATURE,Done,HIGH,"""Sqoop module SQL generation issue""","""The Sqoop module is generating a SQL statement for --table argument that is not correct for Oracle source.  The Job definition is:  job create sqoop_lookup --definition """"sqoop --command=import --args='--connect=jdbc:oracle:thin:@XXXXXXXXX --driver=oracle.jdbc.OracleDriver --direct --username=********* --password=********* --table=W_LOOKUP_D  --target-dir=/user/zeybeb/ingest/gdw/masterdata/lookup_d --num-mappers=1'"""" --deploy   the --table=W_LOOKUP_D results in Sqoop Object generation:  13:22:59,798 INFO main manager.SqlManager - Executing SQL statement: SELECT t.* FROM W_LOOKUP_D AS t WHERE 1=0 13:22:59,861 ERROR main manager.SqlManager - Error executing statement: java.sql.SQLSyntaxErrorException: ORA-00933: SQL command not properly ended  the SQL shoudl be generate with '<table_name> t' instead of '<table_name> AS t'  The --table argument does not except a schema name. User should be able to provide schema.table_name syntax.  ""","""""""The Sqoop module is generating a SQL statement for --table argument that is not correct for Oracle source.  The Job definition is:  job create sqoop_lookup --definition """"""""sqoop --command=import --args='--connect=jdbc:oracle:thin:@XXXXXXXXX --driver=oracle.jdbc.OracleDriver --direct --username=********* --password=********* --table=W_LOOKUP_D  --target-dir=/user/zeybeb/ingest/gdw/masterdata/lookup_d --num-mappers=1'"""""""" --deploy   the --table=W_LOOKUP_D results in Sqoop Object generation:  13:22:59,798 INFO main manager.SqlManager - Executing SQL statement: SELECT t.* FROM W_LOOKUP_D AS t WHERE 1=0 13:22:59,861 ERROR main manager.SqlManager - Error executing statement: java.sql.SQLSyntaxErrorException: ORA-00933: SQL command not properly ended  the SQL shoudl be generate with '<table_name> t' instead of '<table_name> AS t'  The --table argument does not except a schema name. User should be able to provide schema.table_name syntax.  """"""",,0.0
1,XD-2924,BUG,Done,MEDIUM,"""Ability to tap spark streaming processor output""","""We need to support adding a tap stream that connects to spark streaming processor module's output channel.""","""""""We need to support adding a tap stream that connects to spark streaming processor module's output channel.""""""",,3.0
1,XD-2923,BUG,Done,MEDIUM,"""Not able to connect a pubsub channel to spark streaming module""","""If a spark streaming module is setup to connect to a pub/sub channel (a topic or a tap channel), then it doesn't bind to it.  For instance, if I have a stream """"ingest"""" with a definition """"http | log"""" and want to create another stream as, """"tap:stream:ingest > spark-processor | count"""" then this stream doesn't work.""","""""""If a spark streaming module is setup to connect to a pub/sub channel (a topic or a tap channel), then it doesn't bind to it.  For instance, if I have a stream """"""""ingest"""""""" with a definition """"""""http | log"""""""" and want to create another stream as, """"""""tap:stream:ingest > spark-processor | count"""""""" then this stream doesn't work.""""""",,3.0
1,XD-2922,FEATURE,Done,MEDIUM,"""Upgrade Spark version to 1.3.1""","""As a Spring XD user, I'd like to create streaming pipelines, so I can take advantage of latest specs from both XD and Spark/Spark Streaming.""","""""""As a Spring XD user, I'd like to create streaming pipelines, so I can take advantage of latest specs from both XD and Spark/Spark Streaming.""""""",,3.0
1,XD-2921,FEATURE,Done,MEDIUM,"""Clarify the use of escape quotes for properties in the Sqoop job""","""As a developer, I'd like to add documentation on escape quotes, so when someone using Sqoop job can double escape {{\\\\N}} instead of sending quotes {{'\N'}} to successfully submit the job.""","""""""As a developer, I'd like to add documentation on escape quotes, so when someone using Sqoop job can double escape {{\\\\N}} instead of sending quotes {{'\N'}} to successfully submit the job.""""""",,1.0
1,XD-2920,IMPROVEMENT,Done,MEDIUM,"""Dynamic router should allow to discard messages""","""Currently dynamic router sink has to return a valid queue name. This is problematic when the message should be discarded as part of the routing process. In this case one have to define a stream with {{filter | router}} steps where part of the SpEL is duplicated between {{filter}} and {{router}} modules.  Instead the dynamic router should allow to return null to discard the message and stop further processing. Spring Integration is already providing {{resolution-required}} attribute on {{<router/>}}. ""","""""""Currently dynamic router sink has to return a valid queue name. This is problematic when the message should be discarded as part of the routing process. In this case one have to define a stream with {{filter | router}} steps where part of the SpEL is duplicated between {{filter}} and {{router}} modules.  Instead the dynamic router should allow to return null to discard the message and stop further processing. Spring Integration is already providing {{resolution-required}} attribute on {{<router/>}}. """"""",,2.0
1,XD-2919,FEATURE,Done,MEDIUM,"""Create persistent stream repository""","""As a developer, I'd like to create persistent repository for streams, so I could leverage the persisted metadata and reestablish the streaming pipe under failure conditions.""","""""""As a developer, I'd like to create persistent repository for streams, so I could leverage the persisted metadata and reestablish the streaming pipe under failure conditions.""""""",,5.0
1,XD-2918,FEATURE,Done,MEDIUM,"""Define pluggable runtime SPI""","""As a developer, I'd like to define pluggable runtime SPI, so I have the option to choose the implementation based on deployment targets such as CF, on-prem, Mesos etc.""","""""""As a developer, I'd like to define pluggable runtime SPI, so I have the option to choose the implementation based on deployment targets such as CF, on-prem, Mesos etc.""""""",,5.0
1,XD-2916,FEATURE,Done,MEDIUM,"""Create a Java client for Receptor""","""As a developer, I'd like to create a [java client|https://github.com/markfisher/receptor-client] for Receptor, so I can interact with Diego runtime via Receptor API calls from XD. ""","""""""As a developer, I'd like to create a [java client|https://github.com/markfisher/receptor-client] for Receptor, so I can interact with Diego runtime via Receptor API calls from XD. """"""",,8.0
1,XD-2915,FEATURE,Done,MEDIUM,"""Create Boot based ModuleRunner""","""As a developer, I'd like to build isolated Boot-based {{ModuleRunner}} for use in container-managed environments, so I can run XD without the hard requirement for running _xd-containers_.""","""""""As a developer, I'd like to build isolated Boot-based {{ModuleRunner}} for use in container-managed environments, so I can run XD without the hard requirement for running _xd-containers_.""""""",,8.0
1,XD-2914,FEATURE,Done,MEDIUM,"""Create a pluggable runtime SPI""","""As a developer, I'd like to migrate module deployment from the """"repository"""" abstraction (used for stream/job definitions), so I can create it as a pluggable runtime SPI.""","""""""As a developer, I'd like to migrate module deployment from the """"""""repository"""""""" abstraction (used for stream/job definitions), so I can create it as a pluggable runtime SPI.""""""",,8.0
1,XD-2912,BUG,Done,HIGH,"""Sqoop Module not running""","""Can  not get any jobs created using Scoop Module to run. Have tried both the direct implementation using module definition and job create from shell.  Job creates , deploys and goes into execution mode. Runs endlessly no return. Attaching container log and job definition:  job create sqoop96 --definition """"sqoop --command=import --args='--table INV.MTL_CATEGORY_SETS_B -m=1 --connect jdbc:oracle:thin:@xxxxx.xxx.xxx --username ****** --password *********'"""" --deploy  --Container Log 2015-04-06 15:38:23,720 INFO [main] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: Created MRAppMaster for application appattempt_1428324095255_0006_000001 2015-04-06 15:38:24,041 WARN [main] org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable 2015-04-06 15:38:24,058 INFO [main] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: Executing with tokens: 2015-04-06 15:38:24,058 INFO [main] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: Kind: YARN_AM_RM_TOKEN, Service: , Ident: (appAttemptId { application_id { id: 6 cluster_timestamp: 1428324095255 } attemptId: 1 } keyId: 723784990) 2015-04-06 15:38:24,213 INFO [main] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: Using mapred newApiCommitter. 2015-04-06 15:38:24,811 INFO [main] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: OutputCommitter set in config null 2015-04-06 15:38:24,864 INFO [main] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter 2015-04-06 15:38:24,895 INFO [main] org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.mapreduce.jobhistory.EventType for class org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler 2015-04-06 15:38:24,897 INFO [main] org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.mapreduce.v2.app.job.event.JobEventType for class org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher 2015-04-06 15:38:24,898 INFO [main] org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.mapreduce.v2.app.job.event.TaskEventType for class org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskEventDispatcher 2015-04-06 15:38:24,899 INFO [main] org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.mapreduce.v2.app.job.event.TaskAttemptEventType for class org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher 2015-04-06 15:38:24,900 INFO [main] org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventType for class org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler 2015-04-06 15:38:24,906 INFO [main] org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.mapreduce.v2.app.speculate.Speculator$EventType for class org.apache.hadoop.mapreduce.v2.app.MRAppMaster$SpeculatorEventDispatcher 2015-04-06 15:38:24,906 INFO [main] org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.mapreduce.v2.app.rm.ContainerAllocator$EventType for class org.apache.hadoop.mapreduce.v2.app.MRAppMaster$ContainerAllocatorRouter 2015-04-06 15:38:24,907 INFO [main] org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncher$EventType for class org.apache.hadoop.mapreduce.v2.app.MRAppMaster$ContainerLauncherRouter 2015-04-06 15:38:24,926 INFO [main] org.apache.hadoop.mapreduce.v2.jobhistory.JobHistoryUtils: Default file system is set solely by core-default.xml therefore -  ignoring 2015-04-06 15:38:24,941 INFO [main] org.apache.hadoop.mapreduce.v2.jobhistory.JobHistoryUtils: Default file system is set solely by core-default.xml therefore -  ignoring 2015-04-06 15:38:24,960 INFO [main] org.apache.hadoop.mapreduce.v2.jobhistory.JobHistoryUtils: Default file system is set solely by core-default.xml therefore -  ignoring 2015-04-06 15:38:24,979 INFO [main] org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: Emitting job history data to the timeline server is not enabled 2015-04-06 15:38:25,032 INFO [main] org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.mapreduce.v2.app.job.event.JobFinishEvent$Type for class org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobFinishEventHandler 2015-04-06 15:38:25,295 INFO [main] org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties 2015-04-06 15:38:25,353 INFO [main] org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s). 2015-04-06 15:38:25,353 INFO [main] org.apache.hadoop.metrics2.impl.MetricsSystemImpl: MRAppMaster metrics system started 2015-04-06 15:38:25,362 INFO [main] org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: Adding job token for job_1428324095255_0006 to jobTokenSecretManager 2015-04-06 15:38:25,476 INFO [main] org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: Not uberizing job_1428324095255_0006 because: not enabled; 2015-04-06 15:38:25,489 INFO [main] org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: Input size for job job_1428324095255_0006 = 0. Number of splits = 1 2015-04-06 15:38:25,489 INFO [main] org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: Number of reduces for job job_1428324095255_0006 = 0 2015-04-06 15:38:25,489 INFO [main] org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: job_1428324095255_0006Job Transitioned from NEW to INITED 2015-04-06 15:38:25,490 INFO [main] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: MRAppMaster launching normal, non-uberized, multi-container job job_1428324095255_0006. 2015-04-06 15:38:25,515 INFO [main] org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue 2015-04-06 15:38:25,522 INFO [Socket Reader #1 for port 51164] org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 51164 2015-04-06 15:38:25,543 INFO [main] org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.mapreduce.v2.api.MRClientProtocolPB to the server 2015-04-06 15:38:25,543 INFO [IPC Server Responder] org.apache.hadoop.ipc.Server: IPC Server Responder: starting 2015-04-06 15:38:25,543 INFO [IPC Server listener on 51164] org.apache.hadoop.ipc.Server: IPC Server listener on 51164: starting 2015-04-06 15:38:25,544 INFO [main] org.apache.hadoop.mapreduce.v2.app.client.MRClientService: Instantiated MRClientService at sandbox.hortonworks.com/192.168.3.128:51164 2015-04-06 15:38:25,598 INFO [main] org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog 2015-04-06 15:38:25,601 INFO [main] org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.mapreduce is not defined 2015-04-06 15:38:25,609 INFO [main] org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter) 2015-04-06 15:38:25,614 INFO [main] org.apache.hadoop.http.HttpServer2: Added filter AM_PROXY_FILTER (class=org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter) to context mapreduce 2015-04-06 15:38:25,614 INFO [main] org.apache.hadoop.http.HttpServer2: Added filter AM_PROXY_FILTER (class=org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter) to context static 2015-04-06 15:38:25,616 INFO [main] org.apache.hadoop.http.HttpServer2: adding path spec: /mapreduce/* 2015-04-06 15:38:25,616 INFO [main] org.apache.hadoop.http.HttpServer2: adding path spec: /ws/* 2015-04-06 15:38:25,624 INFO [main] org.apache.hadoop.http.HttpServer2: Jetty bound to port 38343 2015-04-06 15:38:25,624 INFO [main] org.mortbay.log: jetty-6.1.26.hwx 2015-04-06 15:38:25,662 INFO [main] org.mortbay.log: Extract jar:file:/usr/hdp/2.2.0.0-2041/hadoop-yarn/hadoop-yarn-common-2.6.0.2.2.0.0-2041.jar!/webapps/mapreduce to /tmp/Jetty_0_0_0_0_38343_mapreduce____86ppbg/webapp 2015-04-06 15:38:25,901 INFO [main] org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:38343 2015-04-06 15:38:25,901 INFO [main] org.apache.hadoop.yarn.webapp.WebApps: Web app /mapreduce started at 38343 2015-04-06 15:38:26,261 INFO [main] org.apache.hadoop.yarn.webapp.WebApps: Registered webapp guice modules 2015-04-06 15:38:26,265 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator: JOB_CREATE job_1428324095255_0006 2015-04-06 15:38:26,267 INFO [main] org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue 2015-04-06 15:38:26,267 INFO [Socket Reader #1 for port 58807] org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 58807 2015-04-06 15:38:26,272 INFO [IPC Server Responder] org.apache.hadoop.ipc.Server: IPC Server Responder: starting 2015-04-06 15:38:26,272 INFO [IPC Server listener on 58807] org.apache.hadoop.ipc.Server: IPC Server listener on 58807: starting 2015-04-06 15:38:26,300 INFO [main] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: nodeBlacklistingEnabled:true 2015-04-06 15:38:26,300 INFO [main] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: maxTaskFailuresPerNode is 3 2015-04-06 15:38:26,300 INFO [main] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: blacklistDisablePercent is 33 2015-04-06 15:38:26,343 INFO [main] org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8030 2015-04-06 15:38:26,410 INFO [main] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: maxContainerCapability: <memory:2250, vCores:32> 2015-04-06 15:38:26,410 INFO [main] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: queue: default 2015-04-06 15:38:26,415 INFO [main] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Upper limit on the thread pool size is 500 2015-04-06 15:38:26,417 INFO [main] org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy: yarn.client.max-cached-nodemanagers-proxies : 0 2015-04-06 15:38:26,428 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: job_1428324095255_0006Job Transitioned from INITED to SETUP 2015-04-06 15:38:26,430 INFO [CommitterEvent Processor #0] org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler: Processing the event EventType: JOB_SETUP 2015-04-06 15:38:26,442 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: job_1428324095255_0006Job Transitioned from SETUP to RUNNING 2015-04-06 15:38:26,457 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1428324095255_0006_m_000000 Task Transitioned from NEW to SCHEDULED 2015-04-06 15:38:26,459 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1428324095255_0006_m_000000_0 TaskAttempt Transitioned from NEW to UNASSIGNED 2015-04-06 15:38:26,459 INFO [Thread-50] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: mapResourceRequest:<memory:1024, vCores:1> 2015-04-06 15:38:26,531 INFO [eventHandlingThread] org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: Event Writer setup for JobId: job_1428324095255_0006, File: hdfs://sandbox:8020/tmp/hadoop-yarn/staging/root/.staging/job_1428324095255_0006/job_1428324095255_0006_1.jhist 2015-04-06 15:38:27,413 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Before Scheduling: PendingReds:0 ScheduledMaps:1 ScheduledReds:0 AssignedMaps:0 AssignedReds:0 CompletedMaps:0 CompletedReds:0 ContAlloc:0 ContRel:0 HostLocal:0 RackLocal:0 2015-04-06 15:38:27,466 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: getResources() for application_1428324095255_0006: ask=1 release= 0 newContainers=0 finishedContainers=0 resourcelimit=<memory:500, vCores:0> knownNMs=1""","""""""Can  not get any jobs created using Scoop Module to run. Have tried both the direct implementation using module definition and job create from shell.  Job creates , deploys and goes into execution mode. Runs endlessly no return. Attaching container log and job definition:  job create sqoop96 --definition """"""""sqoop --command=import --args='--table INV.MTL_CATEGORY_SETS_B -m=1 --connect jdbc:oracle:thin:@xxxxx.xxx.xxx --username ****** --password *********'"""""""" --deploy  --Container Log 2015-04-06 15:38:23,720 INFO [main] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: Created MRAppMaster for application appattempt_1428324095255_0006_000001 2015-04-06 15:38:24,041 WARN [main] org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable 2015-04-06 15:38:24,058 INFO [main] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: Executing with tokens: 2015-04-06 15:38:24,058 INFO [main] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: Kind: YARN_AM_RM_TOKEN, Service: , Ident: (appAttemptId { application_id { id: 6 cluster_timestamp: 1428324095255 } attemptId: 1 } keyId: 723784990) 2015-04-06 15:38:24,213 INFO [main] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: Using mapred newApiCommitter. 2015-04-06 15:38:24,811 INFO [main] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: OutputCommitter set in config null 2015-04-06 15:38:24,864 INFO [main] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter 2015-04-06 15:38:24,895 INFO [main] org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.mapreduce.jobhistory.EventType for class org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler 2015-04-06 15:38:24,897 INFO [main] org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.mapreduce.v2.app.job.event.JobEventType for class org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher 2015-04-06 15:38:24,898 INFO [main] org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.mapreduce.v2.app.job.event.TaskEventType for class org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskEventDispatcher 2015-04-06 15:38:24,899 INFO [main] org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.mapreduce.v2.app.job.event.TaskAttemptEventType for class org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher 2015-04-06 15:38:24,900 INFO [main] org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventType for class org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler 2015-04-06 15:38:24,906 INFO [main] org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.mapreduce.v2.app.speculate.Speculator$EventType for class org.apache.hadoop.mapreduce.v2.app.MRAppMaster$SpeculatorEventDispatcher 2015-04-06 15:38:24,906 INFO [main] org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.mapreduce.v2.app.rm.ContainerAllocator$EventType for class org.apache.hadoop.mapreduce.v2.app.MRAppMaster$ContainerAllocatorRouter 2015-04-06 15:38:24,907 INFO [main] org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncher$EventType for class org.apache.hadoop.mapreduce.v2.app.MRAppMaster$ContainerLauncherRouter 2015-04-06 15:38:24,926 INFO [main] org.apache.hadoop.mapreduce.v2.jobhistory.JobHistoryUtils: Default file system is set solely by core-default.xml therefore -  ignoring 2015-04-06 15:38:24,941 INFO [main] org.apache.hadoop.mapreduce.v2.jobhistory.JobHistoryUtils: Default file system is set solely by core-default.xml therefore -  ignoring 2015-04-06 15:38:24,960 INFO [main] org.apache.hadoop.mapreduce.v2.jobhistory.JobHistoryUtils: Default file system is set solely by core-default.xml therefore -  ignoring 2015-04-06 15:38:24,979 INFO [main] org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: Emitting job history data to the timeline server is not enabled 2015-04-06 15:38:25,032 INFO [main] org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.mapreduce.v2.app.job.event.JobFinishEvent$Type for class org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobFinishEventHandler 2015-04-06 15:38:25,295 INFO [main] org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties 2015-04-06 15:38:25,353 INFO [main] org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s). 2015-04-06 15:38:25,353 INFO [main] org.apache.hadoop.metrics2.impl.MetricsSystemImpl: MRAppMaster metrics system started 2015-04-06 15:38:25,362 INFO [main] org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: Adding job token for job_1428324095255_0006 to jobTokenSecretManager 2015-04-06 15:38:25,476 INFO [main] org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: Not uberizing job_1428324095255_0006 because: not enabled; 2015-04-06 15:38:25,489 INFO [main] org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: Input size for job job_1428324095255_0006 = 0. Number of splits = 1 2015-04-06 15:38:25,489 INFO [main] org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: Number of reduces for job job_1428324095255_0006 = 0 2015-04-06 15:38:25,489 INFO [main] org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: job_1428324095255_0006Job Transitioned from NEW to INITED 2015-04-06 15:38:25,490 INFO [main] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: MRAppMaster launching normal, non-uberized, multi-container job job_1428324095255_0006. 2015-04-06 15:38:25,515 INFO [main] org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue 2015-04-06 15:38:25,522 INFO [Socket Reader #1 for port 51164] org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 51164 2015-04-06 15:38:25,543 INFO [main] org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.mapreduce.v2.api.MRClientProtocolPB to the server 2015-04-06 15:38:25,543 INFO [IPC Server Responder] org.apache.hadoop.ipc.Server: IPC Server Responder: starting 2015-04-06 15:38:25,543 INFO [IPC Server listener on 51164] org.apache.hadoop.ipc.Server: IPC Server listener on 51164: starting 2015-04-06 15:38:25,544 INFO [main] org.apache.hadoop.mapreduce.v2.app.client.MRClientService: Instantiated MRClientService at sandbox.hortonworks.com/192.168.3.128:51164 2015-04-06 15:38:25,598 INFO [main] org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog 2015-04-06 15:38:25,601 INFO [main] org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.mapreduce is not defined 2015-04-06 15:38:25,609 INFO [main] org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter) 2015-04-06 15:38:25,614 INFO [main] org.apache.hadoop.http.HttpServer2: Added filter AM_PROXY_FILTER (class=org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter) to context mapreduce 2015-04-06 15:38:25,614 INFO [main] org.apache.hadoop.http.HttpServer2: Added filter AM_PROXY_FILTER (class=org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter) to context static 2015-04-06 15:38:25,616 INFO [main] org.apache.hadoop.http.HttpServer2: adding path spec: /mapreduce/* 2015-04-06 15:38:25,616 INFO [main] org.apache.hadoop.http.HttpServer2: adding path spec: /ws/* 2015-04-06 15:38:25,624 INFO [main] org.apache.hadoop.http.HttpServer2: Jetty bound to port 38343 2015-04-06 15:38:25,624 INFO [main] org.mortbay.log: jetty-6.1.26.hwx 2015-04-06 15:38:25,662 INFO [main] org.mortbay.log: Extract jar:file:/usr/hdp/2.2.0.0-2041/hadoop-yarn/hadoop-yarn-common-2.6.0.2.2.0.0-2041.jar!/webapps/mapreduce to /tmp/Jetty_0_0_0_0_38343_mapreduce____86ppbg/webapp 2015-04-06 15:38:25,901 INFO [main] org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:38343 2015-04-06 15:38:25,901 INFO [main] org.apache.hadoop.yarn.webapp.WebApps: Web app /mapreduce started at 38343 2015-04-06 15:38:26,261 INFO [main] org.apache.hadoop.yarn.webapp.WebApps: Registered webapp guice modules 2015-04-06 15:38:26,265 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator: JOB_CREATE job_1428324095255_0006 2015-04-06 15:38:26,267 INFO [main] org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue 2015-04-06 15:38:26,267 INFO [Socket Reader #1 for port 58807] org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 58807 2015-04-06 15:38:26,272 INFO [IPC Server Responder] org.apache.hadoop.ipc.Server: IPC Server Responder: starting 2015-04-06 15:38:26,272 INFO [IPC Server listener on 58807] org.apache.hadoop.ipc.Server: IPC Server listener on 58807: starting 2015-04-06 15:38:26,300 INFO [main] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: nodeBlacklistingEnabled:true 2015-04-06 15:38:26,300 INFO [main] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: maxTaskFailuresPerNode is 3 2015-04-06 15:38:26,300 INFO [main] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: blacklistDisablePercent is 33 2015-04-06 15:38:26,343 INFO [main] org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8030 2015-04-06 15:38:26,410 INFO [main] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: maxContainerCapability: <memory:2250, vCores:32> 2015-04-06 15:38:26,410 INFO [main] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: queue: default 2015-04-06 15:38:26,415 INFO [main] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Upper limit on the thread pool size is 500 2015-04-06 15:38:26,417 INFO [main] org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy: yarn.client.max-cached-nodemanagers-proxies : 0 2015-04-06 15:38:26,428 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: job_1428324095255_0006Job Transitioned from INITED to SETUP 2015-04-06 15:38:26,430 INFO [CommitterEvent Processor #0] org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler: Processing the event EventType: JOB_SETUP 2015-04-06 15:38:26,442 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: job_1428324095255_0006Job Transitioned from SETUP to RUNNING 2015-04-06 15:38:26,457 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1428324095255_0006_m_000000 Task Transitioned from NEW to SCHEDULED 2015-04-06 15:38:26,459 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1428324095255_0006_m_000000_0 TaskAttempt Transitioned from NEW to UNASSIGNED 2015-04-06 15:38:26,459 INFO [Thread-50] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: mapResourceRequest:<memory:1024, vCores:1> 2015-04-06 15:38:26,531 INFO [eventHandlingThread] org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: Event Writer setup for JobId: job_1428324095255_0006, File: hdfs://sandbox:8020/tmp/hadoop-yarn/staging/root/.staging/job_1428324095255_0006/job_1428324095255_0006_1.jhist 2015-04-06 15:38:27,413 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Before Scheduling: PendingReds:0 ScheduledMaps:1 ScheduledReds:0 AssignedMaps:0 AssignedReds:0 CompletedMaps:0 CompletedReds:0 ContAlloc:0 ContRel:0 HostLocal:0 RackLocal:0 2015-04-06 15:38:27,466 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: getResources() for application_1428324095255_0006: ask=1 release= 0 newContainers=0 finishedContainers=0 resourcelimit=<memory:500, vCores:0> knownNMs=1""""""",,0.0
1,XD-2911,FEATURE,Done,MEDIUM,"""Improve performance of TupleBuilder ""","""As a developer, I'd like to bench test cases around {{TupleBuilder}}, so I can identify the bottlenecks and tune for performance optimizations. ""","""""""As a developer, I'd like to bench test cases around {{TupleBuilder}}, so I can identify the bottlenecks and tune for performance optimizations. """"""",,8.0
1,XD-2910,FEATURE,Done,MEDIUM,"""Revisit benchmark matrix for Sqoop vs. jdbshdfs jobs""","""As a developer, I'd like to revisit performance benchmarks with new improvements, so I can verify the optimizations around _jdbchdfs_.""","""""""As a developer, I'd like to revisit performance benchmarks with new improvements, so I can verify the optimizations around _jdbchdfs_.""""""",,1.0
1,XD-2909,FEATURE,Done,MEDIUM,"""Produce Kafka Baseline numbers on Rackspace""","""Produce Kafka Baseline numbers on Rackspace""","""Produce Kafka Baseline numbers on Rackspace""",,5.0
1,XD-2908,FEATURE,Done,HIGH,"""Acceptance Tests needs to wait for JobDefinitionResources to be populated ""","""After the Introduction to XD-2861 the acquisition of JobResources takes more time.  We have to introduce a pause to wait for getJobDefinitionResource to be populated. ""","""""""After the Introduction to XD-2861 the acquisition of JobResources takes more time.  We have to introduce a pause to wait for getJobDefinitionResource to be populated. """"""",,5.0
1,XD-2906,FEATURE,Done,MEDIUM,"""Create a new CI build to verify 'install' target""","""As a developer, I'd like to add a new CI build to include _install_ target, so I can verify the target expectations, as it is often time consuming to verify it in the development environment.""","""""""As a developer, I'd like to add a new CI build to include _install_ target, so I can verify the target expectations, as it is often time consuming to verify it in the development environment.""""""",,2.0
1,XD-2905,FEATURE,Done,MEDIUM,"""Complete remaining work for the DEBS challenge""","""As a developer, I'd like to complete the remaining work with DEBS challenge, so I can submit by the deadline.""","""""""As a developer, I'd like to complete the remaining work with DEBS challenge, so I can submit by the deadline.""""""",,2.0
1,XD-2904,FEATURE,Done,MEDIUM,"""Upgrade to Boot 1.2.3 release""","""As a user, I'd like to upgrade to Spring Boot 1.2.3 release, do I can leverage the latest improvements and bug fixes.  We should also sync-up the following dependency updates to [synchronize with Boot|https://github.com/spring-projects/spring-boot/blob/master/spring-boot-dependencies/pom.xml]: {code} <logback.version>1.1.3</logback.version> <jackson.version>2.5.1</jackson.version> <gemfire.version>8.0.0</gemfire.version> <h2.version>1.4.185</h2.version> <javax-mail.version>1.5.3</javax-mail.version> <undertow.version>1.2.3.Final</undertow.version> <joda-time.version>2.7</joda-time.version> <nekohtml.version>1.9.21</nekohtml.version> <activemq.version>5.11.1</activemq.version> <antlr2.version>2.7.7</antlr2.version> <commons-dbcp2.version>2.0.1</commons-dbcp2.version> <tomcat.version>8.0.21</tomcat.version> <aspectj.version>1.8.5</aspectj.version> <groovy.version>2.4.3</groovy.version> <crashub.version>1.3.1</crashub.version> <jetty.version>9.2.9.v20150224</jetty.version> <elasticsearch.version>1.4.4</elasticsearch.version> <flyway.version>3.2.1</flyway.version> <freemarker.version>2.3.22</freemarker.version> <jdom2.version>2.0.6</jdom2.version> <liquibase.version>3.3.2</liquibase.version> <mockito.version>1.10.19</mockito.version> mongodb.version>2.13.0</mongodb.version> <slf4j.version>1.7.11</slf4j.version> <spring-cloud-connectors.version>1.1.1.RELEASE</spring-cloud-connectors.version> <spring-security.version>4.0.1.RELEASE</spring-security.version> <jedis.version>2.6.2</jedis.version> <spring-ws.version>2.2.1.RELEASE</spring-ws.version> {code}""","""""""As a user, I'd like to upgrade to Spring Boot 1.2.3 release, do I can leverage the latest improvements and bug fixes.  We should also sync-up the following dependency updates to [synchronize with Boot|https://github.com/spring-projects/spring-boot/blob/master/spring-boot-dependencies/pom.xml]: """"""",""" <logback.version>1.1.3</logback.version> <jackson.version>2.5.1</jackson.version> <gemfire.version>8.0.0</gemfire.version> <h2.version>1.4.185</h2.version> <javax-mail.version>1.5.3</javax-mail.version> <undertow.version>1.2.3.Final</undertow.version> <joda-time.version>2.7</joda-time.version> <nekohtml.version>1.9.21</nekohtml.version> <activemq.version>5.11.1</activemq.version> <antlr2.version>2.7.7</antlr2.version> <commons-dbcp2.version>2.0.1</commons-dbcp2.version> <tomcat.version>8.0.21</tomcat.version> <aspectj.version>1.8.5</aspectj.version> <groovy.version>2.4.3</groovy.version> <crashub.version>1.3.1</crashub.version> <jetty.version>9.2.9.v20150224</jetty.version> <elasticsearch.version>1.4.4</elasticsearch.version> <flyway.version>3.2.1</flyway.version> <freemarker.version>2.3.22</freemarker.version> <jdom2.version>2.0.6</jdom2.version> <liquibase.version>3.3.2</liquibase.version> <mockito.version>1.10.19</mockito.version> mongodb.version>2.13.0</mongodb.version> <slf4j.version>1.7.11</slf4j.version> <spring-cloud-connectors.version>1.1.1.RELEASE</spring-cloud-connectors.version> <spring-security.version>4.0.1.RELEASE</spring-security.version> <jedis.version>2.6.2</jedis.version> <spring-ws.version>2.2.1.RELEASE</spring-ws.version> """,1.0
1,XD-2903,FEATURE,Done,MEDIUM,"""Upgrade to 1.1.2 SIK release""","""As a developer, I'd like to upgrade to SI Kafka release, so I can synchronize with latest improvements and bug fixes.  ""","""""""As a developer, I'd like to upgrade to SI Kafka release, so I can synchronize with latest improvements and bug fixes.  """"""",,1.0
1,XD-2902,FEATURE,Done,MEDIUM,"""Upgrade to latest SI release ""","""As a developer, I'd like to upgrade to SI milestone/GA release, so I can synchronize with JMX improvements.    This is dependent on SI Milestone and GA release timelines.""","""""""As a developer, I'd like to upgrade to SI milestone/GA release, so I can synchronize with JMX improvements.    This is dependent on SI Milestone and GA release timelines.""""""",,1.0
1,XD-2899,FEATURE,Done,MEDIUM,"""Improve HA support for Rabbit""","""As a developer, I would like to connect to the broker that hosts the Rabbit queue, so I can connect to a Rabbit cluster that's setup for HA/FT. Perhaps consider having this feature natively supported in spring amqp itself.""","""""""As a developer, I would like to connect to the broker that hosts the Rabbit queue, so I can connect to a Rabbit cluster that's setup for HA/FT. Perhaps consider having this feature natively supported in spring amqp itself.""""""",,3.0
1,XD-2896,FEATURE,Done,MEDIUM,"""Add support to capture errors/stacktrace via DLQ""","""As a user, I'd like to have the configuration option to use an alternative DLQ, so I can publish the message this time with additional headers, including one that contains the exception (and stack trace). ""","""""""As a user, I'd like to have the configuration option to use an alternative DLQ, so I can publish the message this time with additional headers, including one that contains the exception (and stack trace). """"""",,3.0
1,XD-2895,FEATURE,Done,HIGH,"""Password for Sqoop Job definition is in the open""","""While creation sqoop and providing the password for the sqoop jobs the guid does not mask the password with a '*********'.""","""""""While creation sqoop and providing the password for the sqoop jobs the guid does not mask the password with a '*********'.""""""",,5.0
1,XD-2893,FEATURE,Done,MEDIUM,"""Properly render defaults for """"module info"""" that use \n \t etc.""","""Characters line \t, \n, etc. should be either escaped, or rendered as human readable variants in module info (eg <newline>)""","""""""Characters line \t, \n, etc. should be either escaped, or rendered as human readable variants in module info (eg <newline>)""""""",,2.0
1,XD-2892,FEATURE,Done,MEDIUM,"""Add support for PHD 3.0 ""","""As a developer, I'd like to certify Spring XD against PHD 3.0, so I can synchronize with the latest ODP based bits. ""","""""""As a developer, I'd like to certify Spring XD against PHD 3.0, so I can synchronize with the latest ODP based bits. """"""",,3.0
1,XD-2891,FEATURE,Done,MEDIUM,"""Provide an --override option to the module upload command""","""When uploading a new version of a module the admin container if there is already an existing module, the behavior should be to delete the existing contents of the module directory and replace it with that of the new upload jar.  This would be an optional parameter.""","""""""When uploading a new version of a module the admin container if there is already an existing module, the behavior should be to delete the existing contents of the module directory and replace it with that of the new upload jar.  This would be an optional parameter.""""""",,3.0
1,XD-2890,MAINTENANCE,Done,MEDIUM,"""Add support to ready files line by line""","""As a user, I'd like to have the option to read the file line by line, so I get the optional OOTB optimum file reading experience.""","""""""As a user, I'd like to have the option to read the file line by line, so I get the optional OOTB optimum file reading experience.""""""",,8.0
1,XD-2887,FEATURE,Done,MEDIUM,"""Have a version of GET /modules that returns full info""","""Similar to the DetailedModuleDefinitionResource that is returned when querying a single module, but would be returned when listing (provided a ?full flag has been turned on)""","""""""Similar to the DetailedModuleDefinitionResource that is returned when querying a single module, but would be returned when listing (provided a ?full flag has been turned on)""""""",,3.0
1,XD-2885,FEATURE,Done,MEDIUM,"""Only ship relevant modules files""","""The current build ships everything that is found in the modules directory, including build artifacts such as build/ or IDEA *.iml files.  Restrict the build to only include config/, lib/ at the moment.""","""""""The current build ships everything that is found in the modules directory, including build artifacts such as build/ or IDEA *.iml files.  Restrict the build to only include config/, lib/ at the moment.""""""",,1.0
1,XD-2884,FEATURE,Done,MEDIUM,"""Document dynamic classpath feature""","""Document dynamic classpath feature""","""Document dynamic classpath feature""",,3.0
1,XD-2882,FEATURE,Done,MEDIUM,"""Provide an option for hdfs sink to use """"Syncable"""" writes""","""As a user, I'd like to have an option to have the hdfs sink use """"Syncable"""" writes to provide better resiliency in the case of sink/container failures. I'm willing to accept the performance penalty if I choose this option. ""","""""""As a user, I'd like to have an option to have the hdfs sink use """"""""Syncable"""""""" writes to provide better resiliency in the case of sink/container failures. I'm willing to accept the performance penalty if I choose this option. """"""",,3.0
1,XD-2879,FEATURE,Done,MEDIUM,"""Add support for explicit partition count configuration for Kafka bus""","""As a developer, I'd like to add support for explicit partition count configuration, so I can use this option to cleverly route the payload to the intended consumer (module).""","""""""As a developer, I'd like to add support for explicit partition count configuration, so I can use this option to cleverly route the payload to the intended consumer (module).""""""",,5.0
1,XD-2877,IMPROVEMENT,Done,MEDIUM,"""Refactor deployment interfaces/class hierarchy""","""As a pre-requisite for XD-2835 and a continuation of XD-2671, split apart the concepts of repository and deployment. This will affect the {{ResourceDeployer}} interface and the classes that implement it.""","""""""As a pre-requisite for XD-2835 and a continuation of XD-2671, split apart the concepts of repository and deployment. This will affect the {{ResourceDeployer}} interface and the classes that implement it.""""""",,8.0
1,XD-2876,IMPROVEMENT,Done,MEDIUM,"""Update Documentation Link""","""{code}  _____                           __   _______ /  ___|          (-)             \ \ / /  _  \ \ `--. _ __  _ __ _ _ __   __ _   \ V /| | | |  `--. \ '_ \| '__| | '_ \ / _` |  / ^ \| | | | /\__/ / |_) | |  | | | | | (_| | / / \ \ |/ / \____/| .__/|_|  |_|_| |_|\__, | \/   \/___/       | |                  __/ |       |_|                 |___/ 1.1.1.RELEASE                    eXtreme Data   Started : ContainerServerApplication Documentation: https://github.com/spring-projects/spring-xd/wiki {code}  This should probably be changed to:  Documentation: http://docs.spring.io/spring-xd/docs/current/reference/html/  ""","""""""  This should probably be changed to:  Documentation: http://docs.spring.io/spring-xd/docs/current/reference/html/  """"""","""  _____                           __   _______ /  ___|          (-)             \ \ / /  _  \ \ `--. _ __  _ __ _ _ __   __ _   \ V /| | | |  `--. \ '_ \| '__| | '_ \ / _` |  / ^ \| | | | /\__/ / |_) | |  | | | | | (_| | / / \ \ |/ / \____/| .__/|_|  |_|_| |_|\__, | \/   \/___/       | |                  __/ |       |_|                 |___/ 1.1.1.RELEASE                    eXtreme Data   Started : ContainerServerApplication Documentation: https://github.com/spring-projects/spring-xd/wiki """,1.0
1,XD-2873,BUG,Done,URGENT,"""Creating Streams sporadically using Kafka as a message bus throws TopicNotFound exception""","""XD Version Spring XD 1.1.1.Release 1 Admin on own (on-metal) Rackspace machine 2 Containers each having own (on-metal) rackspace machine 1 zookeeper node collocated with admin  While executing XD performance testing on Rackspace using Kafka as a transport we occasionally get the following exception: {noformat} 2015-03-26 18:36:30,677 1.1.1.RELEASE  INFO DeploymentsPathChildrenCache-0 server.DeploymentListener - Path cache event: path=/deployments/modules/allocated/4c3c9ccf-44db-4772-87c2-70c63b82c3aa/foo3.sink.throughput.1, type=CHILD_ADDED 2015-03-26 18:36:30,685 1.1.1.RELEASE  INFO DeploymentsPathChildrenCache-0 server.DeploymentListener - Deploying module 'throughput' for stream 'foo3' 2015-03-26 18:36:30,820 1.1.1.RELEASE  INFO DeploymentsPathChildrenCache-0 server.DeploymentListener - Deploying module [ModuleDescriptor@19f0b0a6 moduleName = 'throughput', moduleLabel = 'throughput', group = 'foo3', sourceChannelName = [null], sinkChannelName = [null], index = 1, type = sink, parameters = map[[empty]], children = list[[empty]]] 2015-03-26 18:36:31,372 1.1.1.RELEASE ERROR DeploymentsPathChildrenCache-0 server.DeploymentListener - Exception deploying module org.springframework.integration.kafka.core.TopicNotFoundException: No topic named 'foo3.0' found  at org.springframework.integration.kafka.core.DefaultConnectionFactory.getPartitions(DefaultConnectionFactory.java:209)  at org.springframework.xd.dirt.integration.kafka.KafkaMessageBus.createKafkaConsumer(KafkaMessageBus.java:640)  at org.springframework.xd.dirt.integration.kafka.KafkaMessageBus.bindConsumer(KafkaMessageBus.java:454)  at org.springframework.xd.dirt.plugins.AbstractMessageBusBinderPlugin.bindMessageConsumer(AbstractMessageBusBinderPlugin.java:275)  at org.springframework.xd.dirt.plugins.AbstractMessageBusBinderPlugin.bindConsumerAndProducers(AbstractMessageBusBinderPlugin.java:158)  at org.springframework.xd.dirt.plugins.stream.StreamPlugin.postProcessModule(StreamPlugin.java:73)  at org.springframework.xd.dirt.module.ModuleDeployer.postProcessModule(ModuleDeployer.java:238)  at org.springframework.xd.dirt.module.ModuleDeployer.doDeploy(ModuleDeployer.java:218)  at org.springframework.xd.dirt.module.ModuleDeployer.deploy(ModuleDeployer.java:200)  at org.springframework.xd.dirt.server.DeploymentListener.deployModule(DeploymentListener.java:363)  at org.springframework.xd.dirt.server.DeploymentListener.deployStreamModule(DeploymentListener.java:332)  at org.springframework.xd.dirt.server.DeploymentListener.onChildAdded(DeploymentListener.java:179)  at org.springframework.xd.dirt.server.DeploymentListener.childEvent(DeploymentListener.java:147)  at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:509)  at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:503)  at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)  at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297)  at org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:83)  at org.apache.curator.framework.recipes.cache.PathChildrenCache.callListeners(PathChildrenCache.java:500)  at org.apache.curator.framework.recipes.cache.EventOperation.invoke(EventOperation.java:35)  at org.apache.curator.framework.recipes.cache.PathChildrenCache$10.run(PathChildrenCache.java:762)  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)  at java.util.concurrent.FutureTask.run(FutureTask.java:262)  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)  at java.util.concurrent.FutureTask.run(FutureTask.java:262)  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)  at java.lang.Thread.run(Thread.java:745) {noformat}  stream used to create the exception: stream create foo4 --definition """"load-generator --messageSize=1000 --messageCount=10000000 | throughput"""" --deploy  After failed deployment.  I destroy the stream and recreate it and it works fine.""","""""""XD Version Spring XD 1.1.1.Release 1 Admin on own (on-metal) Rackspace machine 2 Containers each having own (on-metal) rackspace machine 1 zookeeper node collocated with admin  While executing XD performance testing on Rackspace using Kafka as a transport we occasionally get the following exception:   stream used to create the exception: stream create foo4 --definition """"""""load-generator --messageSize=1000 --messageCount=10000000 | throughput"""""""" --deploy  After failed deployment.  I destroy the stream and recreate it and it works fine.""""""",""" 2015-03-26 18:36:30,677 1.1.1.RELEASE  INFO DeploymentsPathChildrenCache-0 server.DeploymentListener - Path cache event: path=/deployments/modules/allocated/4c3c9ccf-44db-4772-87c2-70c63b82c3aa/foo3.sink.throughput.1, type=CHILD_ADDED 2015-03-26 18:36:30,685 1.1.1.RELEASE  INFO DeploymentsPathChildrenCache-0 server.DeploymentListener - Deploying module 'throughput' for stream 'foo3' 2015-03-26 18:36:30,820 1.1.1.RELEASE  INFO DeploymentsPathChildrenCache-0 server.DeploymentListener - Deploying module [ModuleDescriptor@19f0b0a6 moduleName = 'throughput', moduleLabel = 'throughput', group = 'foo3', sourceChannelName = [null], sinkChannelName = [null], index = 1, type = sink, parameters = map[[empty]], children = list[[empty]]] 2015-03-26 18:36:31,372 1.1.1.RELEASE ERROR DeploymentsPathChildrenCache-0 server.DeploymentListener - Exception deploying module org.springframework.integration.kafka.core.TopicNotFoundException: No topic named 'foo3.0' found  at org.springframework.integration.kafka.core.DefaultConnectionFactory.getPartitions(DefaultConnectionFactory.java:209)  at org.springframework.xd.dirt.integration.kafka.KafkaMessageBus.createKafkaConsumer(KafkaMessageBus.java:640)  at org.springframework.xd.dirt.integration.kafka.KafkaMessageBus.bindConsumer(KafkaMessageBus.java:454)  at org.springframework.xd.dirt.plugins.AbstractMessageBusBinderPlugin.bindMessageConsumer(AbstractMessageBusBinderPlugin.java:275)  at org.springframework.xd.dirt.plugins.AbstractMessageBusBinderPlugin.bindConsumerAndProducers(AbstractMessageBusBinderPlugin.java:158)  at org.springframework.xd.dirt.plugins.stream.StreamPlugin.postProcessModule(StreamPlugin.java:73)  at org.springframework.xd.dirt.module.ModuleDeployer.postProcessModule(ModuleDeployer.java:238)  at org.springframework.xd.dirt.module.ModuleDeployer.doDeploy(ModuleDeployer.java:218)  at org.springframework.xd.dirt.module.ModuleDeployer.deploy(ModuleDeployer.java:200)  at org.springframework.xd.dirt.server.DeploymentListener.deployModule(DeploymentListener.java:363)  at org.springframework.xd.dirt.server.DeploymentListener.deployStreamModule(DeploymentListener.java:332)  at org.springframework.xd.dirt.server.DeploymentListener.onChildAdded(DeploymentListener.java:179)  at org.springframework.xd.dirt.server.DeploymentListener.childEvent(DeploymentListener.java:147)  at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:509)  at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:503)  at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)  at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297)  at org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:83)  at org.apache.curator.framework.recipes.cache.PathChildrenCache.callListeners(PathChildrenCache.java:500)  at org.apache.curator.framework.recipes.cache.EventOperation.invoke(EventOperation.java:35)  at org.apache.curator.framework.recipes.cache.PathChildrenCache$10.run(PathChildrenCache.java:762)  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)  at java.util.concurrent.FutureTask.run(FutureTask.java:262)  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)  at java.util.concurrent.FutureTask.run(FutureTask.java:262)  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)  at java.lang.Thread.run(Thread.java:745) """,3.0
1,XD-2872,BUG,Done,URGENT,"""Able to bypass authorization checks by appending """".json"""" or """".xml""""""","""How to reproduce:  1) Enable security 2) Use a user that has the following role only: """"ROLE_CREATE"""" 3) Make a normal REST call:  {code} http://localhost:9393/runtime/containers {code}  yields the *desired response*:  {code}     {        """"timestamp"""": """"2015-03-26T16:51:17.010Z"""",        """"status"""": 403,        """"error"""": """"Forbidden"""",        """"message"""": """"Access is denied"""",        """"path"""": """"/runtime/containers""""     } {code}  Now try:  {code} http://localhost:9393/runtime/containers.json {code}  This produces:  {code}       {        """"links"""":        [            {                """"rel"""": """"self"""",                """"href"""": """"http://localhost:9393/runtime/containers{?page,size,sort}""""            }        ],        """"content"""":        [            {                """"containerId"""": """"86eea5aa-b18e-41c5-a3f5-42dfa10713c1"""",                """"groups"""": """""""",                """"deploymentSize"""": 0,                """"deployedModules"""":                [                ],                """"messageRates"""": null,                """"attributes"""":                {                    """"ip"""": """"10.0.1.119"""",                    """"host"""": """"INTEGRATION.local"""",                    """"groups"""": """""""",                    """"pid"""": """"52686"""",                    """"id"""": """"86eea5aa-b18e-41c5-a3f5-42dfa10713c1""""                },                """"links"""":                [                    {                        """"rel"""": """"self"""",                        """"href"""": """"http://localhost:9393/runtime/containers/86eea5aa-b18e-41c5-a3f5-42dfa10713c1""""                    }                ]            }        ],        """"page"""":        {            """"size"""": 20,            """"totalElements"""": 1,            """"totalPages"""": 1,            """"number"""": 0        }     } {code}""","""""""How to reproduce:  1) Enable security 2) Use a user that has the following role only: """"""""ROLE_CREATE"""""""" 3) Make a normal REST call:    yields the *desired response*:    Now try:    This produces:  """"""",""" http://localhost:9393/runtime/containers      {        """"""""timestamp"""""""": """"""""2015-03-26T16:51:17.010Z"""""""",        """"""""status"""""""": 403,        """"""""error"""""""": """"""""Forbidden"""""""",        """"""""message"""""""": """"""""Access is denied"""""""",        """"""""path"""""""": """"""""/runtime/containers""""""""     }  http://localhost:9393/runtime/containers.json        {        """"""""links"""""""":        [            {                """"""""rel"""""""": """"""""self"""""""",                """"""""href"""""""": """"""""http://localhost:9393/runtime/containers{?page,size,sort}""""""""            }        ],        """"""""content"""""""":        [            {                """"""""containerId"""""""": """"""""86eea5aa-b18e-41c5-a3f5-42dfa10713c1"""""""",                """"""""groups"""""""": """""""""""""""",                """"""""deploymentSize"""""""": 0,                """"""""deployedModules"""""""":                [                ],                """"""""messageRates"""""""": null,                """"""""attributes"""""""":                {                    """"""""ip"""""""": """"""""10.0.1.119"""""""",                    """"""""host"""""""": """"""""INTEGRATION.local"""""""",                    """"""""groups"""""""": """""""""""""""",                    """"""""pid"""""""": """"""""52686"""""""",                    """"""""id"""""""": """"""""86eea5aa-b18e-41c5-a3f5-42dfa10713c1""""""""                },                """"""""links"""""""":                [                    {                        """"""""rel"""""""": """"""""self"""""""",                        """"""""href"""""""": """"""""http://localhost:9393/runtime/containers/86eea5aa-b18e-41c5-a3f5-42dfa10713c1""""""""                    }                ]            }        ],        """"""""page"""""""":        {            """"""""size"""""""": 20,            """"""""totalElements"""""""": 1,            """"""""totalPages"""""""": 1,            """"""""number"""""""": 0        }     } """,3.0
1,XD-2869,FEATURE,Done,MEDIUM,"""Error when creating job from UI with security""","""As a user, I logged in with ROLE_CREATE and I get an error while trying job creation from admin_ui. I can create job from the shell successfully. Trying the same workflow with ROLE_ADMIN results with the same error as well. I don't see anything in the admin/container logs about the error itself. ""","""""""As a user, I logged in with ROLE_CREATE and I get an error while trying job creation from admin_ui. I can create job from the shell successfully. Trying the same workflow with ROLE_ADMIN results with the same error as well. I don't see anything in the admin/container logs about the error itself. """"""",,1.0
1,XD-2868,FEATURE,Done,MEDIUM,"""Support Partitioned Batch Jobs with a LocalMessageBus""","""Initial support for partitioned batch jobs (initially tested with a local bus) had an {{ExecutorChannel}} in the job context to enable multiple partitions to run. Otherwise, with a local bus, only one partition would run at a time.  When further work was done to support other buses, this was removed and the bus was used to control partition concurrency.  The {{LocalMessageBus}} was changed to use an unbounded task executor; this was wrong because now all partitions ran at once.  Further changes to the local bus changed the task executor to be pooled, but with default properties that mean only one thread is used.  Further, the pool configuration is bus-wide so you can't use that configuration to select the concurrency for an individual job.  The bottom line is that the local bus is not suitable for partitioned batch jobs; it was not anticipated that it would be used for this scenario. With 1.0.x too many partitions run (all); with 1.1.x only one thread runs (by default).  In the local bus, we need to use a configurable, dedicated, bounded task executor for each batch job. ""","""""""Initial support for partitioned batch jobs (initially tested with a local bus) had an {{ExecutorChannel}} in the job context to enable multiple partitions to run. Otherwise, with a local bus, only one partition would run at a time.  When further work was done to support other buses, this was removed and the bus was used to control partition concurrency.  The {{LocalMessageBus}} was changed to use an unbounded task executor; this was wrong because now all partitions ran at once.  Further changes to the local bus changed the task executor to be pooled, but with default properties that mean only one thread is used.  Further, the pool configuration is bus-wide so you can't use that configuration to select the concurrency for an individual job.  The bottom line is that the local bus is not suitable for partitioned batch jobs; it was not anticipated that it would be used for this scenario. With 1.0.x too many partitions run (all); with 1.1.x only one thread runs (by default).  In the local bus, we need to use a configurable, dedicated, bounded task executor for each batch job. """"""",,5.0
1,XD-2865,BUG,Done,MEDIUM,"""Message Bus: Shut down Kafka Consumers completely before unbinding""","""This causes the following exception to be thrown in the log (without functional adverse effects)  org.springframework.messaging.MessageDeliveryException: Dispatcher has no subscribers for channel 'unknown.channel.name'.; nested exception is org.springframework.integration.MessageDispatchingException: Dispatcher has no subscribers  at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:81)  at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:277)  at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:239)  at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:115)  at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:45)  at org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:95)  at org.springframework.integration.endpoint.MessageProducerSupport.sendMessage(MessageProducerSupport.java:101)  at org.springframework.integration.kafka.inbound.KafkaMessageDrivenChannelAdapter.access$300(KafkaMessageDrivenChannelAdapter.java:43)  at org.springframework.integration.kafka.inbound.KafkaMessageDrivenChannelAdapter$AutoAcknowledgingChannelForwardingMessageListener.doOnMessage(KafkaMessageDrivenChannelAdapter.java:172)  at org.springframework.integration.kafka.listener.AbstractDecodingMessageListener.onMessage(AbstractDecodingMessageListener.java:50)  at org.springframework.integration.kafka.listener.QueueingMessageListenerInvoker.run(QueueingMessageListenerInvoker.java:121)  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)  at java.lang.Thread.run(Thread.java:745) Caused by: org.springframework.integration.MessageDispatchingException: Dispatcher has no subscribers  at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:107)  at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)  at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)  ... 13 more  ""","""""""This causes the following exception to be thrown in the log (without functional adverse effects)  org.springframework.messaging.MessageDeliveryException: Dispatcher has no subscribers for channel 'unknown.channel.name'.; nested exception is org.springframework.integration.MessageDispatchingException: Dispatcher has no subscribers  at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:81)  at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:277)  at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:239)  at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:115)  at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:45)  at org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:95)  at org.springframework.integration.endpoint.MessageProducerSupport.sendMessage(MessageProducerSupport.java:101)  at org.springframework.integration.kafka.inbound.KafkaMessageDrivenChannelAdapter.access$300(KafkaMessageDrivenChannelAdapter.java:43)  at org.springframework.integration.kafka.inbound.KafkaMessageDrivenChannelAdapter$AutoAcknowledgingChannelForwardingMessageListener.doOnMessage(KafkaMessageDrivenChannelAdapter.java:172)  at org.springframework.integration.kafka.listener.AbstractDecodingMessageListener.onMessage(AbstractDecodingMessageListener.java:50)  at org.springframework.integration.kafka.listener.QueueingMessageListenerInvoker.run(QueueingMessageListenerInvoker.java:121)  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)  at java.lang.Thread.run(Thread.java:745) Caused by: org.springframework.integration.MessageDispatchingException: Dispatcher has no subscribers  at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:107)  at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)  at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)  ... 13 more  """"""",,2.0
1,XD-2864,FEATURE,Done,MEDIUM,"""JavaConfiguredModule should throw an exception when no @Configuration class is present ""","""I had a custom module with a typo: base_packages=base_packages=com.acme.config  The module deploys without error but the stream hangs since the channels, etc. are not found in the stream plugin. Very hard to debug. ""","""""""I had a custom module with a typo: base_packages=base_packages=com.acme.config  The module deploys without error but the stream hangs since the channels, etc. are not found in the stream plugin. Very hard to debug. """"""",,2.0
1,XD-2861,BUG,Done,MEDIUM,"""Admin leader election issue when using different management port""","""When the admin is started with the different management port (default is the same as that of admin http port), then the leadership is requested when the management context is setup. The leadership election should happen only using the Admin server application context.  With this, the following exception is thrown when deployment requests are handled:  2015-03-24 21:48:26,340 1.2.0.SNAP ERROR DeploymentSupervisor-0 queue.DistributedQueue - Exception processing queue item: queue-0000000004 org.springframework.xd.dirt.server.admin.deployment.DeploymentException: testStream  at org.springframework.xd.dirt.stream.AbstractInstancePersistingDeployer.undeployResource(AbstractInstancePersistingDeployer.java:164)  at org.springframework.xd.dirt.stream.AbstractInstancePersistingDeployer.undeploy(AbstractInstancePersistingDeployer.java:83)  at org.springframework.xd.dirt.stream.AbstractInstancePersistingDeployer.undeployAll(AbstractInstancePersistingDeployer.java:109)  at org.springframework.xd.dirt.stream.AbstractInstancePersistingDeployer.deleteAll(AbstractInstancePersistingDeployer.java:117)  at org.springframework.xd.dirt.server.admin.deployment.zk.DeploymentMessageConsumer.processDeploymentMessage(DeploymentMessageConsumer.java:115)  at org.springframework.xd.dirt.server.admin.deployment.zk.DeploymentMessageConsumer.consumeMessage(DeploymentMessageConsumer.java:73)  at org.springframework.xd.dirt.server.admin.deployment.zk.DeploymentMessageConsumer.consumeMessage(DeploymentMessageConsumer.java:43)  at org.apache.curator.framework.recipes.queue.DistributedQueue.processMessageBytes(DistributedQueue.java:678)  at org.apache.curator.framework.recipes.queue.DistributedQueue.processNormally(DistributedQueue.java:712)  at org.apache.curator.framework.recipes.queue.DistributedQueue.access$300(DistributedQueue.java:65)  at org.apache.curator.framework.recipes.queue.DistributedQueue$5.run(DistributedQueue.java:629)  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)  at java.util.concurrent.FutureTask.run(FutureTask.java:262)  at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)  at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)  at java.lang.Thread.run(Thread.java:745) Caused by: java.lang.IllegalArgumentException: Module deployment request path cache shouldn't be null.  at org.springframework.util.Assert.notNull(Assert.java:112)  at org.springframework.xd.dirt.server.admin.deployment.zk.ZKDeploymentHandler.undeploy(ZKDeploymentHandler.java:81)  at org.springframework.xd.dirt.stream.AbstractInstancePersistingDeployer.undeployResource(AbstractInstancePersistingDeployer.java:161)  ... 17 more""","""""""When the admin is started with the different management port (default is the same as that of admin http port), then the leadership is requested when the management context is setup. The leadership election should happen only using the Admin server application context.  With this, the following exception is thrown when deployment requests are handled:  2015-03-24 21:48:26,340 1.2.0.SNAP ERROR DeploymentSupervisor-0 queue.DistributedQueue - Exception processing queue item: queue-0000000004 org.springframework.xd.dirt.server.admin.deployment.DeploymentException: testStream  at org.springframework.xd.dirt.stream.AbstractInstancePersistingDeployer.undeployResource(AbstractInstancePersistingDeployer.java:164)  at org.springframework.xd.dirt.stream.AbstractInstancePersistingDeployer.undeploy(AbstractInstancePersistingDeployer.java:83)  at org.springframework.xd.dirt.stream.AbstractInstancePersistingDeployer.undeployAll(AbstractInstancePersistingDeployer.java:109)  at org.springframework.xd.dirt.stream.AbstractInstancePersistingDeployer.deleteAll(AbstractInstancePersistingDeployer.java:117)  at org.springframework.xd.dirt.server.admin.deployment.zk.DeploymentMessageConsumer.processDeploymentMessage(DeploymentMessageConsumer.java:115)  at org.springframework.xd.dirt.server.admin.deployment.zk.DeploymentMessageConsumer.consumeMessage(DeploymentMessageConsumer.java:73)  at org.springframework.xd.dirt.server.admin.deployment.zk.DeploymentMessageConsumer.consumeMessage(DeploymentMessageConsumer.java:43)  at org.apache.curator.framework.recipes.queue.DistributedQueue.processMessageBytes(DistributedQueue.java:678)  at org.apache.curator.framework.recipes.queue.DistributedQueue.processNormally(DistributedQueue.java:712)  at org.apache.curator.framework.recipes.queue.DistributedQueue.access$300(DistributedQueue.java:65)  at org.apache.curator.framework.recipes.queue.DistributedQueue$5.run(DistributedQueue.java:629)  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)  at java.util.concurrent.FutureTask.run(FutureTask.java:262)  at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)  at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)  at java.lang.Thread.run(Thread.java:745) Caused by: java.lang.IllegalArgumentException: Module deployment request path cache shouldn't be null.  at org.springframework.util.Assert.notNull(Assert.java:112)  at org.springframework.xd.dirt.server.admin.deployment.zk.ZKDeploymentHandler.undeploy(ZKDeploymentHandler.java:81)  at org.springframework.xd.dirt.stream.AbstractInstancePersistingDeployer.undeployResource(AbstractInstancePersistingDeployer.java:161)  ... 17 more""""""",,1.0
1,XD-2860,IMPROVEMENT,Done,MEDIUM,"""remove ModuleDefinitions.dummy()""","""This method should be replaced with a utility method in a test support class so that it is only available in a testing context. ""","""""""This method should be replaced with a utility method in a test support class so that it is only available in a testing context. """"""",,2.0
1,XD-2859,FEATURE,Done,MEDIUM,"""UI: Deploy Stream - Return key does not submit form""","""*http://localhost:9393/admin-ui/#/streams/definitions/test/deploy*""","""""""*http://localhost:9393/admin-ui/#/streams/definitions/test/deploy*""""""",,1.0
1,XD-2858,FEATURE,Done,MEDIUM,"""Add dynamic classpath support for modules""","""As a developer, I'd like to add support for dynamic classpath for modules, so we can have the flexibility to load the right dependencies either based on module options (0) or via other properties such as including the dependencies from a specific location (1).   (0): {code} /lib/*.jar:lib/${distro}/*.jar {code}  (1): {code} ${xd.home}/lib/hadoop/${distro}/*.jar {code}  *Example:* {code} http | hdfs --distro=PHD22  http | myCustomModule --classpath=/my/funky/dir  http | jpa --provider=eclipse  jpa: /config/ /lib/something-that-is-common.jar     /eclipse/eclipse-link-3.2.jar     /hibernate/hibernate-core-5.0.jar  module.classpath = /lib/*.jar:/lib/${provider}/*.jar {code}""","""""""As a developer, I'd like to add support for dynamic classpath for modules, so we can have the flexibility to load the right dependencies either based on module options (0) or via other properties such as including the dependencies from a specific location (1).   (0):   (1):   *Example:* """"""",""" /lib/*.jar:lib/${distro}/*.jar  ${xd.home}/lib/hadoop/${distro}/*.jar  http | hdfs --distro=PHD22  http | myCustomModule --classpath=/my/funky/dir  http | jpa --provider=eclipse  jpa: /config/ /lib/something-that-is-common.jar     /eclipse/eclipse-link-3.2.jar     /hibernate/hibernate-core-5.0.jar  module.classpath = /lib/*.jar:/lib/${provider}/*.jar """,5.0
1,XD-2855,IMPROVEMENT,Done,HIGH,"""Basic security makes xd-shell throw 403 Forbidden error""","""After enabling admin endpoint security in servers.yml using basic authentication and single user {code} spring:   profiles: admin security:   basic:     enabled: true # false to disable security settings (default)     realm: SpringXD   user: # valid only if security.basic.enabled=true     name: myadmin     password: myadmin {code}  Spring XD UI is secured however xd-shell commands are resulting in a 403 error:  {code} server-unknown:>admin config server --uri http://localhost:9393 --username myadmin --password myadmin Successfully targeted http://localhost:9393 xd:>admin config info   -------------  -------------------------------------------   Credentials    [username='myadmin, password=****']   Result         Successfully targeted http://localhost:9393   Target         http://localhost:9393   Timezone used  Greenwich Mean Time (UTC 0:00)   -------------  ------------------------------------------- xd:>stream list Command failed org.springframework.web.client.HttpClientErrorException: 403 Forbidden xd:>stream create --name """"t1"""" --definition """"time | log"""" Command failed org.springframework.web.client.HttpClientErrorException: 403 Forbidden {code}  This can be fixed by adding configuration explained in """"File based authentication"""" docs section:  {code} xd:   security:     authentication:       file:         enabled: true         users:             myadmin: myadmin, ROLE_VIEW, ROLE_ADMIN, ROLE_CREATE {code}  Following is the problem: # Configuration explained in """"Single user authentication"""" chapter should work out of the box without additional role setup # Docs should be more clear on authorization""","""""""After enabling admin endpoint security in servers.yml using basic authentication and single user   Spring XD UI is secured however xd-shell commands are resulting in a 403 error:    This can be fixed by adding configuration explained in """"""""File based authentication"""""""" docs section:    Following is the problem: # Configuration explained in """"""""Single user authentication"""""""" chapter should work out of the box without additional role setup # Docs should be more clear on authorization""""""",""" spring:   profiles: admin security:   basic:     enabled: true # false to disable security settings (default)     realm: SpringXD   user: # valid only if security.basic.enabled=true     name: myadmin     password: myadmin  server-unknown:>admin config server --uri http://localhost:9393 --username myadmin --password myadmin Successfully targeted http://localhost:9393 xd:>admin config info   -------------  -------------------------------------------   Credentials    [username='myadmin, password=****']   Result         Successfully targeted http://localhost:9393   Target         http://localhost:9393   Timezone used  Greenwich Mean Time (UTC 0:00)   -------------  ------------------------------------------- xd:>stream list Command failed org.springframework.web.client.HttpClientErrorException: 403 Forbidden xd:>stream create --name """"""""t1"""""""" --definition """"""""time | log"""""""" Command failed org.springframework.web.client.HttpClientErrorException: 403 Forbidden  xd:   security:     authentication:       file:         enabled: true         users:             myadmin: myadmin, ROLE_VIEW, ROLE_ADMIN, ROLE_CREATE """,2.0
1,XD-2854,BUG,Done,HIGH,"""Launching XD admin fails with ZK holding existing stream data""","""Following exception is thrown when starting XD admin withe ZK holding the stream data:  2015-03-23 17:21:13,831 1.2.0.SNAP ERROR LeaderSelector-0 leader.LeaderSelector - The leader threw an exception java.lang.NullPointerException at com.fasterxml.jackson.core.JsonFactory.createParser(JsonFactory.java:822) at com.fasterxml.jackson.databind.ObjectReader.readValue(ObjectReader.java:896) at org.springframework.xd.dirt.stream.zookeeper.ZooKeeperStreamDefinitionRepository.findOne(ZooKeeperStreamDefinitionRepository.java:157) at org.springframework.xd.dirt.stream.zookeeper.ZooKeeperStreamDefinitionRepository.findOne(ZooKeeperStreamDefinitionRepository.java:56) at org.springframework.xd.dirt.stream.dsl.StreamConfigParser.lookupStream(StreamConfigParser.java:654) at org.springframework.xd.dirt.stream.dsl.ChannelNode.resolve(ChannelNode.java:144) at org.springframework.xd.dirt.stream.dsl.SourceChannelNode.resolve(SourceChannelNode.java:54) at org.springframework.xd.dirt.stream.dsl.StreamNode.resolve(StreamNode.java:125) at org.springframework.xd.dirt.stream.dsl.StreamConfigParser.parse(StreamConfigParser.java:110) at org.springframework.xd.dirt.stream.XDStreamParser.parse(XDStreamParser.java:121) at org.springframework.xd.dirt.stream.StreamFactory.createStream(StreamFactory.java:84) at org.springframework.xd.dirt.server.admin.deployment.zk.DeploymentLoader.loadStream(DeploymentLoader.java:101) at org.springframework.xd.dirt.server.admin.deployment.zk.DefaultDeploymentStateRecalculator.recalculateStreamStates(DefaultDeploymentStateRecalculator.java:96) at org.springframework.xd.dirt.server.admin.deployment.zk.DefaultDeploymentStateRecalculator.onSupervisorElected(DefaultDeploymentStateRecalculator.java:182) at org.springframework.xd.dirt.server.admin.deployment.zk.DeploymentSupervisor$LeaderListener.takeLeadership(DeploymentSupervisor.java:468) at org.apache.curator.framework.recipes.leader.LeaderSelector$WrappedListener.takeLeadership(LeaderSelector.java:536) at org.apache.curator.framework.recipes.leader.LeaderSelector.doWork(LeaderSelector.java:398) at org.apache.curator.framework.recipes.leader.LeaderSelector.doWorkLoop(LeaderSelector.java:443) at org.apache.curator.framework.recipes.leader.LeaderSelector.access$100(LeaderSelector.java:63) at org.apache.curator.framework.recipes.leader.LeaderSelector$2.call(LeaderSelector.java:244) at org.apache.curator.framework.recipes.leader.LeaderSelector$2.call(LeaderSelector.java:238) at java.util.concurrent.FutureTask.run(FutureTask.java:262) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) at java.util.concurrent.FutureTask.run(FutureTask.java:262) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:745)""","""""""Following exception is thrown when starting XD admin withe ZK holding the stream data:  2015-03-23 17:21:13,831 1.2.0.SNAP ERROR LeaderSelector-0 leader.LeaderSelector - The leader threw an exception java.lang.NullPointerException at com.fasterxml.jackson.core.JsonFactory.createParser(JsonFactory.java:822) at com.fasterxml.jackson.databind.ObjectReader.readValue(ObjectReader.java:896) at org.springframework.xd.dirt.stream.zookeeper.ZooKeeperStreamDefinitionRepository.findOne(ZooKeeperStreamDefinitionRepository.java:157) at org.springframework.xd.dirt.stream.zookeeper.ZooKeeperStreamDefinitionRepository.findOne(ZooKeeperStreamDefinitionRepository.java:56) at org.springframework.xd.dirt.stream.dsl.StreamConfigParser.lookupStream(StreamConfigParser.java:654) at org.springframework.xd.dirt.stream.dsl.ChannelNode.resolve(ChannelNode.java:144) at org.springframework.xd.dirt.stream.dsl.SourceChannelNode.resolve(SourceChannelNode.java:54) at org.springframework.xd.dirt.stream.dsl.StreamNode.resolve(StreamNode.java:125) at org.springframework.xd.dirt.stream.dsl.StreamConfigParser.parse(StreamConfigParser.java:110) at org.springframework.xd.dirt.stream.XDStreamParser.parse(XDStreamParser.java:121) at org.springframework.xd.dirt.stream.StreamFactory.createStream(StreamFactory.java:84) at org.springframework.xd.dirt.server.admin.deployment.zk.DeploymentLoader.loadStream(DeploymentLoader.java:101) at org.springframework.xd.dirt.server.admin.deployment.zk.DefaultDeploymentStateRecalculator.recalculateStreamStates(DefaultDeploymentStateRecalculator.java:96) at org.springframework.xd.dirt.server.admin.deployment.zk.DefaultDeploymentStateRecalculator.onSupervisorElected(DefaultDeploymentStateRecalculator.java:182) at org.springframework.xd.dirt.server.admin.deployment.zk.DeploymentSupervisor$LeaderListener.takeLeadership(DeploymentSupervisor.java:468) at org.apache.curator.framework.recipes.leader.LeaderSelector$WrappedListener.takeLeadership(LeaderSelector.java:536) at org.apache.curator.framework.recipes.leader.LeaderSelector.doWork(LeaderSelector.java:398) at org.apache.curator.framework.recipes.leader.LeaderSelector.doWorkLoop(LeaderSelector.java:443) at org.apache.curator.framework.recipes.leader.LeaderSelector.access$100(LeaderSelector.java:63) at org.apache.curator.framework.recipes.leader.LeaderSelector$2.call(LeaderSelector.java:244) at org.apache.curator.framework.recipes.leader.LeaderSelector$2.call(LeaderSelector.java:238) at java.util.concurrent.FutureTask.run(FutureTask.java:262) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) at java.util.concurrent.FutureTask.run(FutureTask.java:262) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:745)""""""",,5.0
1,XD-2853,BUG,Done,MEDIUM,"""XD admin ZK distributed queue consumer initialization issue""","""The ZK distributed queue consumer is initialized even before the module, stream, job deployment requests path cache are started. This could lead to issue when the consumer start processing the requests before the cache are initialized.  On such scenario, the following exception could be thrown:  2015-03-23 21:00:25,919 1.2.0.SNAP ERROR DeploymentSupervisor-0 queue.DistributedQueue - Exception processing queue item: queue-0000000002 org.springframework.xd.dirt.server.admin.deployment.DeploymentException: dataSender         at org.springframework.xd.dirt.stream.AbstractInstancePersistingDeployer.undeployResource(AbstractInstancePersistingDeployer.java:164)         at org.springframework.xd.dirt.stream.AbstractInstancePersistingDeployer.undeploy(AbstractInstancePersistingDeployer.java:83)         at org.springframework.xd.dirt.stream.AbstractInstancePersistingDeployer.undeployAll(AbstractInstancePersistingDeployer.java:109)         at org.springframework.xd.dirt.stream.AbstractInstancePersistingDeployer.deleteAll(AbstractInstancePersistingDeployer.java:117)         at org.springframework.xd.dirt.server.admin.deployment.zk.DeploymentMessageConsumer.processDeploymentMessage(DeploymentMessageConsumer.java:115)         at org.springframework.xd.dirt.server.admin.deployment.zk.DeploymentMessageConsumer.consumeMessage(DeploymentMessageConsumer.java:70)         at org.springframework.xd.dirt.server.admin.deployment.zk.DeploymentMessageConsumer.consumeMessage(DeploymentMessageConsumer.java:43)         at org.apache.curator.framework.recipes.queue.DistributedQueue.processMessageBytes(DistributedQueue.java:678)         at org.apache.curator.framework.recipes.queue.DistributedQueue.processNormally(DistributedQueue.java:712)         at org.apache.curator.framework.recipes.queue.DistributedQueue.access$300(DistributedQueue.java:65)         at org.apache.curator.framework.recipes.queue.DistributedQueue$5.run(DistributedQueue.java:629)         at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)         at java.util.concurrent.FutureTask.run(FutureTask.java:262)         at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)         at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)         at java.lang.Thread.run(Thread.java:745) Caused by: java.lang.IllegalArgumentException: Module deployment request path cache shouldn't be null.         at org.springframework.util.Assert.notNull(Assert.java:112)         at org.springframework.xd.dirt.server.admin.deployment.zk.ZKDeploymentHandler.undeploy(ZKDeploymentHandler.java:81)         at org.springframework.xd.dirt.stream.AbstractInstancePersistingDeployer.undeployResource(AbstractInstancePersistingDeployer.java:161)""","""""""The ZK distributed queue consumer is initialized even before the module, stream, job deployment requests path cache are started. This could lead to issue when the consumer start processing the requests before the cache are initialized.  On such scenario, the following exception could be thrown:  2015-03-23 21:00:25,919 1.2.0.SNAP ERROR DeploymentSupervisor-0 queue.DistributedQueue - Exception processing queue item: queue-0000000002 org.springframework.xd.dirt.server.admin.deployment.DeploymentException: dataSender         at org.springframework.xd.dirt.stream.AbstractInstancePersistingDeployer.undeployResource(AbstractInstancePersistingDeployer.java:164)         at org.springframework.xd.dirt.stream.AbstractInstancePersistingDeployer.undeploy(AbstractInstancePersistingDeployer.java:83)         at org.springframework.xd.dirt.stream.AbstractInstancePersistingDeployer.undeployAll(AbstractInstancePersistingDeployer.java:109)         at org.springframework.xd.dirt.stream.AbstractInstancePersistingDeployer.deleteAll(AbstractInstancePersistingDeployer.java:117)         at org.springframework.xd.dirt.server.admin.deployment.zk.DeploymentMessageConsumer.processDeploymentMessage(DeploymentMessageConsumer.java:115)         at org.springframework.xd.dirt.server.admin.deployment.zk.DeploymentMessageConsumer.consumeMessage(DeploymentMessageConsumer.java:70)         at org.springframework.xd.dirt.server.admin.deployment.zk.DeploymentMessageConsumer.consumeMessage(DeploymentMessageConsumer.java:43)         at org.apache.curator.framework.recipes.queue.DistributedQueue.processMessageBytes(DistributedQueue.java:678)         at org.apache.curator.framework.recipes.queue.DistributedQueue.processNormally(DistributedQueue.java:712)         at org.apache.curator.framework.recipes.queue.DistributedQueue.access$300(DistributedQueue.java:65)         at org.apache.curator.framework.recipes.queue.DistributedQueue$5.run(DistributedQueue.java:629)         at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)         at java.util.concurrent.FutureTask.run(FutureTask.java:262)         at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)         at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)         at java.lang.Thread.run(Thread.java:745) Caused by: java.lang.IllegalArgumentException: Module deployment request path cache shouldn't be null.         at org.springframework.util.Assert.notNull(Assert.java:112)         at org.springframework.xd.dirt.server.admin.deployment.zk.ZKDeploymentHandler.undeploy(ZKDeploymentHandler.java:81)         at org.springframework.xd.dirt.stream.AbstractInstancePersistingDeployer.undeployResource(AbstractInstancePersistingDeployer.java:161)""""""",,1.0
1,XD-2852,FEATURE,Done,MEDIUM,"""Create a gpload batch job""","""As a developer, I'd like to create a _gpload_ tasklet, so I can ingest data from various sources into GPDB in an efficient manner.""","""""""As a developer, I'd like to create a _gpload_ tasklet, so I can ingest data from various sources into GPDB in an efficient manner.""""""",,5.0
1,XD-2850,FEATURE,Done,MEDIUM,"""Create a File source to efficiently read files""","""As a developer, I'd like to use an efficient approach to read files, so I don't have to read line-by-line and keep it in-memory in order to consume/write the file content.   Would the _tasklet_ approach be better as opposed to transmitting data via message bus (as streams)? ""","""""""As a developer, I'd like to use an efficient approach to read files, so I don't have to read line-by-line and keep it in-memory in order to consume/write the file content.   Would the _tasklet_ approach be better as opposed to transmitting data via message bus (as streams)? """"""",,5.0
1,XD-2848,MAINTENANCE,Done,MEDIUM,"""Design and budget Perf Env for XD on RackSpace""","""Provide design for how we are going to run XD and Kafka on Rackspace.  This includes the base design for the Kafka Perf tests environment. This will be used to provide a budget for the cloud resources  for the performance environment.  ""","""""""Provide design for how we are going to run XD and Kafka on Rackspace.  This includes the base design for the Kafka Perf tests environment. This will be used to provide a budget for the cloud resources  for the performance environment.  """"""",,3.0
1,XD-2845,FEATURE,Done,MEDIUM,"""Add support for admin-ui and Flo integration ""","""As a developer, I'd like to setup UI infrastructure, so I can integrate admin_ui and Flo.""","""""""As a developer, I'd like to setup UI infrastructure, so I can integrate admin_ui and Flo.""""""",,5.0
1,XD-2844,FEATURE,Done,MEDIUM,"""Create a POC for gpfdist sink""","""As a user, I'd like to have the OOTB _gpfdist_ sink module, so I can use this module to do ultra fast data movement from various sources into GPDB/HAWQ.""","""""""As a user, I'd like to have the OOTB _gpfdist_ sink module, so I can use this module to do ultra fast data movement from various sources into GPDB/HAWQ.""""""",,8.0
1,XD-2842,BUG,Done,MEDIUM,"""Kafka source should not try to decode payloads as Strings""","""Currently, the Kafka source uses a StringDecoder by default - which is an invalid assumption if the payload is not the result of String conversion.   ""","""""""Currently, the Kafka source uses a StringDecoder by default - which is an invalid assumption if the payload is not the result of String conversion.   """"""",,2.0
1,XD-2840,MAINTENANCE,Done,MEDIUM,"""Research how to accommodate dynamic partitions when scaling containers""","""As a developer, I'd like to rebalance partitions as we scale the containers, so I don't have to bring down the running stream/job to reestablish dynamic partitions.""","""""""As a developer, I'd like to rebalance partitions as we scale the containers, so I don't have to bring down the running stream/job to reestablish dynamic partitions.""""""",,8.0
1,XD-2839,FEATURE,Done,MEDIUM,"""Add support to host/read python script from HDFS""","""As a developer, I'd like to host/read Python script (file) from HDFS, so I can use the shell processor in XD (on CF) to delegate data science functionality to Py runtime and receive the feedback back in XD.""","""""""As a developer, I'd like to host/read Python script (file) from HDFS, so I can use the shell processor in XD (on CF) to delegate data science functionality to Py runtime and receive the feedback back in XD.""""""",,8.0
1,XD-2838,FEATURE,Done,MEDIUM,"""Update all the module documentation to include """"shortDescription""""""","""As a developer, I'd like to update all the module docs to also include _shortDescription_ so that it's available for users to learn more about the module.""","""""""As a developer, I'd like to update all the module docs to also include _shortDescription_ so that it's available for users to learn more about the module.""""""",,2.0
1,XD-2837,BUG,Done,URGENT,"""XD-Admin fails to start""","""When starting xd-admin getting the following exception: {noformat} 2015-03-20 14:25:53,904 1.2.0.SNAP  WARN main annotation.AnnotationConfigApplicationContext - Exception encountered during context initialization - cancelling refresh attempt org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'moduleRegistry' defined in class path resource [META-INF/spring-xd/internal/repositories.xml]: Cannot create inner bean 'org.springframework.xd.dirt.module.CustomModuleRegistryFactoryBean#175e137b' of type [org.springframework.xd.dirt.module.CustomModuleRegistryFactoryBean] while setting constructor argument with key [1]; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'org.springframework.xd.dirt.module.CustomModuleRegistryFactoryBean#175e137b' defined in class path resource [META-INF/spring-xd/internal/repositories.xml]: Invocation of init method failed; nested exception is java.lang.NoClassDefFoundError: org/apache/hadoop/fs/FSDataInputStream  at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveInnerBean(BeanDefinitionValueResolver.java:313)  at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveValueIfNecessary(BeanDefinitionValueResolver.java:122)  at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveManagedList(BeanDefinitionValueResolver.java:382)  at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveValueIfNecessary(BeanDefinitionValueResolver.java:157)  at org.springframework.beans.factory.support.ConstructorResolver.resolveConstructorArguments(ConstructorResolver.java:648)  at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:140)  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1131)  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1034)  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:504)  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:476)  at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:303)  at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:230)  at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:299)  at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:194)  at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:762)  at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:757)  at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:480)  at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:691)  at org.springframework.boot.SpringApplication.run(SpringApplication.java:321)  at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:139)  at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:129)  at org.springframework.xd.dirt.server.AdminServerApplication.run(AdminServerApplication.java:89)  at org.springframework.xd.dirt.server.AdminServerApplication.main(AdminServerApplication.java:73) Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'org.springframework.xd.dirt.module.CustomModuleRegistryFactoryBean#175e137b' defined in class path resource [META-INF/spring-xd/internal/repositories.xml]: Invocation of init method failed; nested exception is java.lang.NoClassDefFoundError: org/apache/hadoop/fs/FSDataInputStream  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1566)  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:539)  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:476)  at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveInnerBean(BeanDefinitionValueResolver.java:299)  ... 22 more Caused by: java.lang.NoClassDefFoundError: org/apache/hadoop/fs/FSDataInputStream  at java.lang.Class.forName0(Native Method)  at java.lang.Class.forName(Class.java:190)  at org.springframework.xd.dirt.module.ExtendedResource.<clinit>(ExtendedResource.java:47)  at org.springframework.xd.dirt.module.WritableResourceModuleRegistry.afterPropertiesSet(WritableResourceModuleRegistry.java:123)  at org.springframework.xd.dirt.module.CustomModuleRegistryFactoryBean.afterPropertiesSet(CustomModuleRegistryFactoryBean.java:70)  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1625)  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1562)  ... 25 more Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.fs.FSDataInputStream  at java.net.URLClassLoader$1.run(URLClassLoader.java:366)  at java.net.URLClassLoader$1.run(URLClassLoader.java:355)  at java.security.AccessController.doPrivileged(Native Method)  at java.net.URLClassLoader.findClass(URLClassLoader.java:354)  at java.lang.ClassLoader.loadClass(ClassLoader.java:425)  at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)  at java.lang.ClassLoader.loadClass(ClassLoader.java:358)  ... 32 more 2015-03-20 14:25:53,911 1.2.0.SNAP ERROR main boot.SpringApplication - Application startup failed org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'moduleRegistry' defined in class path resource [META-INF/spring-xd/internal/repositories.xml]: Cannot create inner bean 'org.springframework.xd.dirt.module.CustomModuleRegistryFactoryBean#175e137b' of type [org.springframework.xd.dirt.module.CustomModuleRegistryFactoryBean] while setting constructor argument with key [1]; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'org.springframework.xd.dirt.module.CustomModuleRegistryFactoryBean#175e137b' defined in class path resource [META-INF/spring-xd/internal/repositories.xml]: Invocation of init method failed; nested exception is java.lang.NoClassDefFoundError: org/apache/hadoop/fs/FSDataInputStream  at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveInnerBean(BeanDefinitionValueResolver.java:313)  at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveValueIfNecessary(BeanDefinitionValueResolver.java:122)  at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveManagedList(BeanDefinitionValueResolver.java:382)  at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveValueIfNecessary(BeanDefinitionValueResolver.java:157)  at org.springframework.beans.factory.support.ConstructorResolver.resolveConstructorArguments(ConstructorResolver.java:648)  at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:140)  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1131)  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1034)  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:504)  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:476)  at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:303)  at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:230)  at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:299)  at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:194)  at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:762)  at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:757)  at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:480)  at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:691)  at org.springframework.boot.SpringApplication.run(SpringApplication.java:321)  at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:139)  at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:129)  at org.springframework.xd.dirt.server.AdminServerApplication.run(AdminServerApplication.java:89)  at org.springframework.xd.dirt.server.AdminServerApplication.main(AdminServerApplication.java:73) Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'org.springframework.xd.dirt.module.CustomModuleRegistryFactoryBean#175e137b' defined in class path resource [META-INF/spring-xd/internal/repositories.xml]: Invocation of init method failed; nested exception is java.lang.NoClassDefFoundError: org/apache/hadoop/fs/FSDataInputStream  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1566)  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:539)  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:476)  at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveInnerBean(BeanDefinitionValueResolver.java:299)  ... 22 more Caused by: java.lang.NoClassDefFoundError: org/apache/hadoop/fs/FSDataInputStream  at java.lang.Class.forName0(Native Method)  at java.lang.Class.forName(Class.java:190)  at org.springframework.xd.dirt.module.ExtendedResource.<clinit>(ExtendedResource.java:47)  at org.springframework.xd.dirt.module.WritableResourceModuleRegistry.afterPropertiesSet(WritableResourceModuleRegistry.java:123)  at org.springframework.xd.dirt.module.CustomModuleRegistryFactoryBean.afterPropertiesSet(CustomModuleRegistryFactoryBean.java:70)  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1625)  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1562)  ... 25 more Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.fs.FSDataInputStream  at java.net.URLClassLoader$1.run(URLClassLoader.java:366)  at java.net.URLClassLoader$1.run(URLClassLoader.java:355)  at java.security.AccessController.doPrivileged(Native Method)  at java.net.URLClassLoader.findClass(URLClassLoader.java:354)  at java.lang.ClassLoader.loadClass(ClassLoader.java:425)  at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)  at java.lang.ClassLoader.loadClass(ClassLoader.java:358)  ... 32 more 2015-03-20 14:25:53,915 1.2.0.SNAP ERROR main server.AdminServerApplication - Error creating bean with name 'moduleRegistry' defined in class path resource [META-INF/spring-xd/internal/repositories.xml]: Cannot create inner bean 'org.springframework.xd.dirt.module.CustomModuleRegistryFactoryBean#175e137b' of type [org.springframework.xd.dirt.module.CustomModuleRegistryFactoryBean] while setting constructor argument with key [1]; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'org.springframework.xd.dirt.module.CustomModuleRegistryFactoryBean#175e137b' defined in class path resource [META-INF/spring-xd/internal/repositories.xml]: Invocation of init method failed; nested exception is java.lang.NoClassDefFoundError: org/apache/hadoop/fs/FSDataInputStream {noformat}  Reproduced Locally (mac) and on EC2. xd-singlenode works fine. Commit: 4673b5ab97""","""""""When starting xd-admin getting the following exception:   Reproduced Locally (mac) and on EC2. xd-singlenode works fine. Commit: 4673b5ab97""""""",""" 2015-03-20 14:25:53,904 1.2.0.SNAP  WARN main annotation.AnnotationConfigApplicationContext - Exception encountered during context initialization - cancelling refresh attempt org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'moduleRegistry' defined in class path resource [META-INF/spring-xd/internal/repositories.xml]: Cannot create inner bean 'org.springframework.xd.dirt.module.CustomModuleRegistryFactoryBean#175e137b' of type [org.springframework.xd.dirt.module.CustomModuleRegistryFactoryBean] while setting constructor argument with key [1]; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'org.springframework.xd.dirt.module.CustomModuleRegistryFactoryBean#175e137b' defined in class path resource [META-INF/spring-xd/internal/repositories.xml]: Invocation of init method failed; nested exception is java.lang.NoClassDefFoundError: org/apache/hadoop/fs/FSDataInputStream  at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveInnerBean(BeanDefinitionValueResolver.java:313)  at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveValueIfNecessary(BeanDefinitionValueResolver.java:122)  at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveManagedList(BeanDefinitionValueResolver.java:382)  at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveValueIfNecessary(BeanDefinitionValueResolver.java:157)  at org.springframework.beans.factory.support.ConstructorResolver.resolveConstructorArguments(ConstructorResolver.java:648)  at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:140)  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1131)  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1034)  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:504)  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:476)  at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:303)  at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:230)  at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:299)  at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:194)  at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:762)  at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:757)  at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:480)  at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:691)  at org.springframework.boot.SpringApplication.run(SpringApplication.java:321)  at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:139)  at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:129)  at org.springframework.xd.dirt.server.AdminServerApplication.run(AdminServerApplication.java:89)  at org.springframework.xd.dirt.server.AdminServerApplication.main(AdminServerApplication.java:73) Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'org.springframework.xd.dirt.module.CustomModuleRegistryFactoryBean#175e137b' defined in class path resource [META-INF/spring-xd/internal/repositories.xml]: Invocation of init method failed; nested exception is java.lang.NoClassDefFoundError: org/apache/hadoop/fs/FSDataInputStream  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1566)  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:539)  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:476)  at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveInnerBean(BeanDefinitionValueResolver.java:299)  ... 22 more Caused by: java.lang.NoClassDefFoundError: org/apache/hadoop/fs/FSDataInputStream  at java.lang.Class.forName0(Native Method)  at java.lang.Class.forName(Class.java:190)  at org.springframework.xd.dirt.module.ExtendedResource.<clinit>(ExtendedResource.java:47)  at org.springframework.xd.dirt.module.WritableResourceModuleRegistry.afterPropertiesSet(WritableResourceModuleRegistry.java:123)  at org.springframework.xd.dirt.module.CustomModuleRegistryFactoryBean.afterPropertiesSet(CustomModuleRegistryFactoryBean.java:70)  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1625)  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1562)  ... 25 more Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.fs.FSDataInputStream  at java.net.URLClassLoader$1.run(URLClassLoader.java:366)  at java.net.URLClassLoader$1.run(URLClassLoader.java:355)  at java.security.AccessController.doPrivileged(Native Method)  at java.net.URLClassLoader.findClass(URLClassLoader.java:354)  at java.lang.ClassLoader.loadClass(ClassLoader.java:425)  at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)  at java.lang.ClassLoader.loadClass(ClassLoader.java:358)  ... 32 more 2015-03-20 14:25:53,911 1.2.0.SNAP ERROR main boot.SpringApplication - Application startup failed org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'moduleRegistry' defined in class path resource [META-INF/spring-xd/internal/repositories.xml]: Cannot create inner bean 'org.springframework.xd.dirt.module.CustomModuleRegistryFactoryBean#175e137b' of type [org.springframework.xd.dirt.module.CustomModuleRegistryFactoryBean] while setting constructor argument with key [1]; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'org.springframework.xd.dirt.module.CustomModuleRegistryFactoryBean#175e137b' defined in class path resource [META-INF/spring-xd/internal/repositories.xml]: Invocation of init method failed; nested exception is java.lang.NoClassDefFoundError: org/apache/hadoop/fs/FSDataInputStream  at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveInnerBean(BeanDefinitionValueResolver.java:313)  at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveValueIfNecessary(BeanDefinitionValueResolver.java:122)  at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveManagedList(BeanDefinitionValueResolver.java:382)  at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveValueIfNecessary(BeanDefinitionValueResolver.java:157)  at org.springframework.beans.factory.support.ConstructorResolver.resolveConstructorArguments(ConstructorResolver.java:648)  at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:140)  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1131)  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1034)  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:504)  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:476)  at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:303)  at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:230)  at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:299)  at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:194)  at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:762)  at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:757)  at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:480)  at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:691)  at org.springframework.boot.SpringApplication.run(SpringApplication.java:321)  at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:139)  at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:129)  at org.springframework.xd.dirt.server.AdminServerApplication.run(AdminServerApplication.java:89)  at org.springframework.xd.dirt.server.AdminServerApplication.main(AdminServerApplication.java:73) Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'org.springframework.xd.dirt.module.CustomModuleRegistryFactoryBean#175e137b' defined in class path resource [META-INF/spring-xd/internal/repositories.xml]: Invocation of init method failed; nested exception is java.lang.NoClassDefFoundError: org/apache/hadoop/fs/FSDataInputStream  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1566)  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:539)  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:476)  at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveInnerBean(BeanDefinitionValueResolver.java:299)  ... 22 more Caused by: java.lang.NoClassDefFoundError: org/apache/hadoop/fs/FSDataInputStream  at java.lang.Class.forName0(Native Method)  at java.lang.Class.forName(Class.java:190)  at org.springframework.xd.dirt.module.ExtendedResource.<clinit>(ExtendedResource.java:47)  at org.springframework.xd.dirt.module.WritableResourceModuleRegistry.afterPropertiesSet(WritableResourceModuleRegistry.java:123)  at org.springframework.xd.dirt.module.CustomModuleRegistryFactoryBean.afterPropertiesSet(CustomModuleRegistryFactoryBean.java:70)  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1625)  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1562)  ... 25 more Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.fs.FSDataInputStream  at java.net.URLClassLoader$1.run(URLClassLoader.java:366)  at java.net.URLClassLoader$1.run(URLClassLoader.java:355)  at java.security.AccessController.doPrivileged(Native Method)  at java.net.URLClassLoader.findClass(URLClassLoader.java:354)  at java.lang.ClassLoader.loadClass(ClassLoader.java:425)  at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)  at java.lang.ClassLoader.loadClass(ClassLoader.java:358)  ... 32 more 2015-03-20 14:25:53,915 1.2.0.SNAP ERROR main server.AdminServerApplication - Error creating bean with name 'moduleRegistry' defined in class path resource [META-INF/spring-xd/internal/repositories.xml]: Cannot create inner bean 'org.springframework.xd.dirt.module.CustomModuleRegistryFactoryBean#175e137b' of type [org.springframework.xd.dirt.module.CustomModuleRegistryFactoryBean] while setting constructor argument with key [1]; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'org.springframework.xd.dirt.module.CustomModuleRegistryFactoryBean#175e137b' defined in class path resource [META-INF/spring-xd/internal/repositories.xml]: Invocation of init method failed; nested exception is java.lang.NoClassDefFoundError: org/apache/hadoop/fs/FSDataInputStream """,3.0
1,XD-2836,FEATURE,Done,MEDIUM,"""xd-admin broken for HDFS module registry""","""xd-admin broken for HDFS module registry""","""xd-admin broken for HDFS module registry""",,1.0
1,XD-2835,FEATURE,Done,MEDIUM,"""XD on Lattice POC""","""As a developer, I'd like to continue XD-on-Lattice/Diego PoC, and will be focused on the design of a pluggable SPI, so it is more generally applicable than Lattice, with the Receptor API being just one implementation option. ""","""""""As a developer, I'd like to continue XD-on-Lattice/Diego PoC, and will be focused on the design of a pluggable SPI, so it is more generally applicable than Lattice, with the Receptor API being just one implementation option. """"""",,8.0
1,XD-2834,FEATURE,Done,MEDIUM,"""Make doc generation part of the standard build""","""Following the recent move of the doco to the main repo, it makes sense to have the doc generation be part of the """"main"""" build, at an early stage, as an incentive for developers to push doc changes as soon as they change the code.  ""","""""""Following the recent move of the doco to the main repo, it makes sense to have the doc generation be part of the """"""""main"""""""" build, at an early stage, as an incentive for developers to push doc changes as soon as they change the code.  """"""",,2.0
1,XD-2833,FEATURE,Done,MEDIUM,"""Document MongoDB source""","""Document MongoDB source""","""Document MongoDB source""",,1.0
1,XD-2832,FEATURE,Done,MEDIUM,"""Add support to create custom jobs using Java Config""","""As a developer, I'd like to create a custom job module using Java Config so that I don't have to deal with XML configurations. While deploying/launching the following job, I get the error attached below.  {code:xml} job create --name CDK_Global --definition """"customBatchJob"""" --deploy module upload --type job --name customBatchJob --file /Users/<USER>Documents/IntelliJWorkspace/CustomBatchModule/build/libs/CustomBatchModule-1.1.0.RELEASE.jar job launch --name CDK_Global {code}  *Error:* I'm getting an exception that the job doesn't exist asking if it's deployed""","""""""As a developer, I'd like to create a custom job module using Java Config so that I don't have to deal with XML configurations. While deploying/launching the following job, I get the error attached below.    *Error:* I'm getting an exception that the job doesn't exist asking if it's deployed""""""",""" job create --name CDK_Global --definition """"""""customBatchJob"""""""" --deploy module upload --type job --name customBatchJob --file /Users/mminella/Documents/IntelliJWorkspace/CustomBatchModule/build/libs/CustomBatchModule-1.1.0.RELEASE.jar job launch --name CDK_Global """,5.0
1,XD-2831,IMPROVEMENT,Done,MEDIUM,"""Refactor Deployment related classes in XD DIRT""","""This is an improvement ticket to address refactoring of XD dirt classes especially XD admin(zk, deployment related) and container.""","""""""This is an improvement ticket to address refactoring of XD dirt classes especially XD admin(zk, deployment related) and container.""""""",,8.0
1,XD-2830,FEATURE,Done,MEDIUM,"""Create gradle task to check that all projects have descriptions""","""This keeps coming up as an issue that prevents us from publishing to maven central.""","""""""This keeps coming up as an issue that prevents us from publishing to maven central.""""""",,2.0
1,XD-2829,FEATURE,Done,MEDIUM,"""Add the Dependencies Required to Use #xpath in Streams""","""Thanks to Gary I found this little gem of documentation to be able to use xpath expression in XD. Only hiccup is that I had to also add the spring-xml.jar to the classpath (otherwise it is missing XPathException class).   http://stackoverflow.com/questions/29110757/spring-xd-work-with-xml-payload""","""""""Thanks to Gary I found this little gem of documentation to be able to use xpath expression in XD. Only hiccup is that I had to also add the spring-xml.jar to the classpath (otherwise it is missing XPathException class).   http://stackoverflow.com/questions/29110757/spring-xd-work-with-xml-payload""""""",,1.0
1,XD-2828,BUG,Done,MEDIUM,"""XD distributed tests are broken""","""There are test failures running XD distributed tests.   It looks like all the test failures are related to NPE on DeploymentProperties format:  java.lang.NullPointerException  at org.springframework.xd.rest.domain.support.DeploymentPropertiesFormat.formatDeploymentProperties(DeploymentPropertiesFormat.java:72)  at org.springframework.xd.rest.client.impl.JobTemplate.deploy(JobTemplate.java:71)  at org.springframework.xd.distributed.test.JobStateTests.testJobStateTransition(JobStateTests.java:83)""","""""""There are test failures running XD distributed tests.   It looks like all the test failures are related to NPE on DeploymentProperties format:  java.lang.NullPointerException  at org.springframework.xd.rest.domain.support.DeploymentPropertiesFormat.formatDeploymentProperties(DeploymentPropertiesFormat.java:72)  at org.springframework.xd.rest.client.impl.JobTemplate.deploy(JobTemplate.java:71)  at org.springframework.xd.distributed.test.JobStateTests.testJobStateTransition(JobStateTests.java:83)""""""",,1.0
1,XD-2827,FEATURE,Done,MEDIUM,"""Enable @Value, etc in Module Options Metadata""","""A placeholder to investigate what can be done with Spring configuration in Module Options Metadata classes to simplify/enhance property configuration.  With @Configuration modules, these may now be beans in the module context. ""","""""""A placeholder to investigate what can be done with Spring configuration in Module Options Metadata classes to simplify/enhance property configuration.  With @Configuration modules, these may now be beans in the module context. """"""",,3.0
1,XD-2826,FEATURE,Done,MEDIUM,"""Remove Reactor Stream processor from ref docs to spring-xd-modules""","""Not going to integrate with Reactor for stream processing.""","""""""Not going to integrate with Reactor for stream processing.""""""",,1.0
1,XD-2825,FEATURE,Done,MEDIUM,"""SCS - Verify/Fix AbstractKryoMultitypeCodec implementation""","""This apparently is not tested or used internally, but I expect it to fail having tried a similar approach to derive the class of a generic type in a different situation. This method does not always work due to type erasure http://stackoverflow.com/questions/3403909/get-generic-type-of-class-at-runtime.  We need to verify if this is working, if not fix it. The API may require it, so possibly UnsupportedOperationException...   {code} /**    * Infers the type from this class's generic type argument    * @param kryo    * @param input    * @return  */ protected T doDeserialize(Kryo kryo, Input input) {  Class<T> type = (Class<T>) (     (ParameterizedType) this.getClass().getGenericSuperclass()).getActualTypeArguments()[0];   return doDeserialize(kryo, input, type); } {code}""","""""""This apparently is not tested or used internally, but I expect it to fail having tried a similar approach to derive the class of a generic type in a different situation. This method does not always work due to type erasure http://stackoverflow.com/questions/3403909/get-generic-type-of-class-at-runtime.  We need to verify if this is working, if not fix it. The API may require it, so possibly UnsupportedOperationException...   """"""",""" /**    * Infers the type from this class's generic type argument    * @param kryo    * @param input    * @return  */ protected T doDeserialize(Kryo kryo, Input input) {  Class<T> type = (Class<T>) (     (ParameterizedType) this.getClass().getGenericSuperclass()).getActualTypeArguments()[0];   return doDeserialize(kryo, input, type); } """,1.0
1,XD-2824,BUG,Done,HIGH,"""hdfs sink loses messages/data when container killed""","""Scenario running a """"rabbit | hdfs"""" stream and killing the xd-container while stream is running.  Looks like the messages get's acked before the data is flushed to hdfs.  This results in some data lost due to data either in tmp file or cached in the dfs client.  Reference: VESC-387""","""""""Scenario running a """"""""rabbit | hdfs"""""""" stream and killing the xd-container while stream is running.  Looks like the messages get's acked before the data is flushed to hdfs.  This results in some data lost due to data either in tmp file or cached in the dfs client.  Reference: VESC-387""""""",,5.0
1,XD-2823,FEATURE,Done,MEDIUM,"""Composite Modules should inherit """"xd.*"""" properties""","""Currently when modules are composed to a single application context, properties are not inherited.  https://github.com/spring-projects/spring-xd/wiki/Modules#placeholders-available-to-all-modules  ""","""""""Currently when modules are composed to a single application context, properties are not inherited.  https://github.com/spring-projects/spring-xd/wiki/Modules#placeholders-available-to-all-modules  """"""",,2.0
1,XD-2822,FEATURE,Done,MEDIUM,"""Improve ItemWriter in OOTB jdbchdfs to use DataStoreWriter""","""The current jdbchdfs job does not take advantage of all the features available to write into HDFS provided by Spring Hadoop's DataStoreWriter implementations, such as partitioning.  Update the jdbchdfs job to use <int-hadoop:store-writer/> (similar to the HDFS Sink) inside a new ItemWriter implementation  ""","""""""The current jdbchdfs job does not take advantage of all the features available to write into HDFS provided by Spring Hadoop's DataStoreWriter implementations, such as partitioning.  Update the jdbchdfs job to use <int-hadoop:store-writer/> (similar to the HDFS Sink) inside a new ItemWriter implementation  """"""",,3.0
1,XD-2821,FEATURE,Done,MEDIUM,"""Refactor job-launcher to not depend on execution context""","""As a developer, I'd like to decouple execution context from job launch lifecycle so that we can avoid CL and serialization errors.   This fix needs to be formally applied in Spring Batch itself. XD will upgrade to Batch release in order to inherit this functionality; hence, the current workaround with XD-2486 needs reafctored. ""","""""""As a developer, I'd like to decouple execution context from job launch lifecycle so that we can avoid CL and serialization errors.   This fix needs to be formally applied in Spring Batch itself. XD will upgrade to Batch release in order to inherit this functionality; hence, the current workaround with XD-2486 needs reafctored. """"""",,5.0
1,XD-2820,MAINTENANCE,Done,URGENT,"""Composing transformer and gemfire-json-server leads to FileNotFoundException during deployment""","""Composing """"transform"""" and """"gemfire-json-server"""" modules leads to FileNotFoundException during stream deployment when: - xd-admin and xd-container are started as system services (after installing from RPM).  - xd-singelonde is started outside of $XD_HOME/bin directory e.g. {code} $ cd """"$XD_HOME"""" $ bin/xd-singelonde {code}  but it's fully working and exception is *not* thrown when: - xd-singlenode script is started from within """"$XD_HOME/bin directory {code} $ cd """"$XD_HOME/bin"""" $ ./xd-singelonde {code}  Then using the XD Shell:  {code} $ xd-shell > module compose --name """"cm-gem-sink"""" --definition """"transform --outputType='application/json' | gemfire-json-server --regionName=timeRegion --keyExpression=payload.getField('location')"""" > stream create --name """"cm-test-gem"""" --definition """"tail --name='/tmp/time.json' | cm-gem-sink""""  > stream deploy --name """"cm-test-gem"""" {code}  Stream deployment will result in following exception  {code} [2015-03-11 16:38:10.918] boot - 17402  INFO [DeploymentsPathChildrenCache-0] --- DeploymentListener: Deploying module [ModuleDescriptor@2095e9f9 moduleName = 'tail', moduleLabel = 'tail', group = 'cm-test-gem', sourceChannelName = [null], sinkChannelName = [null], index = 0, type = source, parameters = map['name' -> '/tmp/time.json'], children = list[[empty]]] 2015-03-11 16:38:11,263 1.1.0.RELEASE  INFO DeploymentSupervisor-0 server.StreamDeploymentListener - Deployment status for stream 'cm-test-gem': DeploymentStatus{state=failed,error(s)=org.springframework.beans.factory.parsing.BeanDefinitionParsingException: Configuration problem: Error evaluating Groovy script: Configuration problem: Error evaluating Groovy script: ../modules/common/gemfire-sink.groovy (No such file or directory) Offending resource: URL [file:../modules/common/gemfire-sink.groovy]; nested exception is java.io.FileNotFoundException: ../modules/common/gemfire-sink.groovy (No such file or directory) Offending resource: file [/opt/spring-xd/spring-xd-1.1.0.RELEASE/xd/modules/sink/gemfire-json-server/config/gemfire-json-server.groovy]; nested exception is org.springframework.beans.factory.parsing.BeanDefinitionParsingException: Configuration problem: Error evaluating Groovy script: ../modules/common/gemfire-sink.groovy (No such file or directory) Offending resource: URL [file:../modules/common/gemfire-sink.groovy]; nested exception is java.io.FileNotFoundException: ../modules/common/gemfire-sink.groovy (No such file or directory)  at org.springframework.beans.factory.groovy.GroovyBeanDefinitionReader.loadBeanDefinitions(GroovyBeanDefinitionReader.java:247)  at org.springframework.beans.factory.groovy.GroovyBeanDefinitionReader.loadBeanDefinitions(GroovyBeanDefinitionReader.java:202)  at org.springframework.boot.BeanDefinitionLoader.load(BeanDefinitionLoader.java:178)  at org.springframework.boot.BeanDefinitionLoader.load(BeanDefinitionLoader.java:138)  at org.springframework.boot.BeanDefinitionLoader.load(BeanDefinitionLoader.java:127)  at org.springframework.boot.SpringApplication.load(SpringApplication.java:620)  at org.springframework.boot.SpringApplication.run(SpringApplication.java:315)  at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:139)  at org.springframework.xd.module.core.SimpleModule.initialize(SimpleModule.java:214)  at org.springframework.xd.module.core.CompositeModule.initialize(CompositeModule.java:105)  at org.springframework.xd.dirt.module.ModuleDeployer.doDeploy(ModuleDeployer.java:217)  at org.springframework.xd.dirt.module.ModuleDeployer.deploy(ModuleDeployer.java:200)  at org.springframework.xd.dirt.server.DeploymentListener.deployModule(DeploymentListener.java:363)  at org.springframework.xd.dirt.server.DeploymentListener.deployStreamModule(DeploymentListener.java:332)  at org.springframework.xd.dirt.server.DeploymentListener.onChildAdded(DeploymentListener.java:179)  at org.springframework.xd.dirt.server.DeploymentListener.childEvent(DeploymentListener.java:147)  at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:509)  at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:503)  at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)  at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297)  at org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:83)  at org.apache.curator.framework.recipes.cache.PathChildrenCache.callListeners(PathChildrenCache.java:500)  at org.apache.curator.framework.recipes.cache.EventOperation.invoke(EventOperation.java:35)  at org.apache.curator.framework.recipes.cache.PathChildrenCache$10.run(PathChildrenCache.java:762)  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)  at java.util.concurrent.FutureTask.run(FutureTask.java:262)  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)  at java.util.concurrent.FutureTask.run(FutureTask.java:262)  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)  at java.lang.Thread.run(Thread.java:745) Caused by: org.springframework.beans.factory.parsing.BeanDefinitionParsingException: Configuration problem: Error evaluating Groovy script: ../modules/common/gemfire-sink.groovy (No such file or directory) Offending resource: URL [file:../modules/common/gemfire-sink.groovy]; nested exception is java.io.FileNotFoundException: ../modules/common/gemfire-sink.groovy (No such file or directory)  at org.springframework.beans.factory.groovy.GroovyBeanDefinitionReader.loadBeanDefinitions(GroovyBeanDefinitionReader.java:247)  at org.springframework.beans.factory.groovy.GroovyBeanDefinitionReader.loadBeanDefinitions(GroovyBeanDefinitionReader.java:202)  at org.springframework.beans.factory.support.AbstractBeanDefinitionReader.loadBeanDefinitions(AbstractBeanDefinitionReader.java:181)  at org.springframework.beans.factory.support.AbstractBeanDefinitionReader.loadBeanDefinitions(AbstractBeanDefinitionReader.java:217)  at org.springframework.beans.factory.support.AbstractBeanDefinitionReader.loadBeanDefinitions(AbstractBeanDefinitionReader.java:188)  at org.springframework.beans.factory.groovy.GroovyBeanDefinitionReader.importBeans(GroovyBeanDefinitionReader.java:337)  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)  at java.lang.reflect.Method.invoke(Method.java:606)  at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:90)  at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:324)  at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:368)  at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1016)  at org.codehaus.groovy.runtime.callsite.PogoMetaClassSite.callCurrent(PogoMetaClassSite.java:66)  at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCallCurrent(CallSiteArray.java:49)  at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:133)  at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:141)  at beans$_run_closure1.doCall(beans:4)  at beans$_run_closure1.doCall(beans)  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)  at java.lang.reflect.Method.invoke(Method.java:606)  at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:90)  at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:324)  at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:278)  at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1016)  at groovy.lang.Closure.call(Closure.java:423)  at groovy.lang.Closure.call(Closure.java:417)  at org.springframework.beans.factory.groovy.GroovyBeanDefinitionReader.invokeBeanDefiningClosure(GroovyBeanDefinitionReader.java:426)  at org.springframework.beans.factory.groovy.GroovyBeanDefinitionReader$1.call(GroovyBeanDefinitionReader.java:223)  at groovy.lang.Closure.call(Closure.java:439)  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)  at java.lang.reflect.Method.invoke(Method.java:606)  at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:90)  at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:324)  at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1207)  at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1016)  at groovy.lang.MetaClassImpl.invokePropertyOrMissing(MetaClassImpl.java:1253)  at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1209)  at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1016)  at org.codehaus.groovy.runtime.callsite.PogoMetaClassSite.callCurrent(PogoMetaClassSite.java:66)  at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCallCurrent(CallSiteArray.java:49)  at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:133)  at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:141)  at beans.run(beans:1)  at groovy.lang.GroovyShell.evaluate(GroovyShell.java:649)  at org.springframework.beans.factory.groovy.GroovyBeanDefinitionReader.loadBeanDefinitions(GroovyBeanDefinitionReader.java:243)  ... 30 more Caused by: java.io.FileNotFoundException: ../modules/common/gemfire-sink.groovy (No such file or directory)  at java.io.FileInputStream.open(Native Method)  at java.io.FileInputStream.<init>(FileInputStream.java:146)  at java.io.FileInputStream.<init>(FileInputStream.java:101)  at sun.net.www.protocol.file.FileURLConnection.connect(FileURLConnection.java:90)  at sun.net.www.protocol.file.FileURLConnection.getInputStream(FileURLConnection.java:188)  at org.springframework.core.io.UrlResource.getInputStream(UrlResource.java:168)  at org.springframework.core.io.support.EncodedResource.getReader(EncodedResource.java:132)  at org.springframework.beans.factory.groovy.GroovyBeanDefinitionReader.loadBeanDefinitions(GroovyBeanDefinitionReader.java:243)  ... 80 more {code}  Exporting XD_HOME as a global variable seems to have no effect on this behavior.""","""""""Composing """"""""transform"""""""" and """"""""gemfire-json-server"""""""" modules leads to FileNotFoundException during stream deployment when: - xd-admin and xd-container are started as system services (after installing from RPM).  - xd-singelonde is started outside of $XD_HOME/bin directory e.g.   but it's fully working and exception is *not* thrown when: - xd-singlenode script is started from within """"""""$XD_HOME/bin directory   Then using the XD Shell:    Stream deployment will result in following exception    Exporting XD_HOME as a global variable seems to have no effect on this behavior.""""""",""" $ cd """"""""$XD_HOME"""""""" $ bin/xd-singelonde  $ cd """"""""$XD_HOME/bin"""""""" $ ./xd-singelonde  $ xd-shell > module compose --name """"""""cm-gem-sink"""""""" --definition """"""""transform --outputType='application/json' | gemfire-json-server --regionName=timeRegion --keyExpression=payload.getField('location')"""""""" > stream create --name """"""""cm-test-gem"""""""" --definition """"""""tail --name='/tmp/time.json' | cm-gem-sink""""""""  > stream deploy --name """"""""cm-test-gem""""""""  [2015-03-11 16:38:10.918] boot - 17402  INFO [DeploymentsPathChildrenCache-0] --- DeploymentListener: Deploying module [ModuleDescriptor@2095e9f9 moduleName = 'tail', moduleLabel = 'tail', group = 'cm-test-gem', sourceChannelName = [null], sinkChannelName = [null], index = 0, type = source, parameters = map['name' -> '/tmp/time.json'], children = list[[empty]]] 2015-03-11 16:38:11,263 1.1.0.RELEASE  INFO DeploymentSupervisor-0 server.StreamDeploymentListener - Deployment status for stream 'cm-test-gem': DeploymentStatus{state=failed,error(s)=org.springframework.beans.factory.parsing.BeanDefinitionParsingException: Configuration problem: Error evaluating Groovy script: Configuration problem: Error evaluating Groovy script: ../modules/common/gemfire-sink.groovy (No such file or directory) Offending resource: URL [file:../modules/common/gemfire-sink.groovy]; nested exception is java.io.FileNotFoundException: ../modules/common/gemfire-sink.groovy (No such file or directory) Offending resource: file [/opt/spring-xd/spring-xd-1.1.0.RELEASE/xd/modules/sink/gemfire-json-server/config/gemfire-json-server.groovy]; nested exception is org.springframework.beans.factory.parsing.BeanDefinitionParsingException: Configuration problem: Error evaluating Groovy script: ../modules/common/gemfire-sink.groovy (No such file or directory) Offending resource: URL [file:../modules/common/gemfire-sink.groovy]; nested exception is java.io.FileNotFoundException: ../modules/common/gemfire-sink.groovy (No such file or directory)  at org.springframework.beans.factory.groovy.GroovyBeanDefinitionReader.loadBeanDefinitions(GroovyBeanDefinitionReader.java:247)  at org.springframework.beans.factory.groovy.GroovyBeanDefinitionReader.loadBeanDefinitions(GroovyBeanDefinitionReader.java:202)  at org.springframework.boot.BeanDefinitionLoader.load(BeanDefinitionLoader.java:178)  at org.springframework.boot.BeanDefinitionLoader.load(BeanDefinitionLoader.java:138)  at org.springframework.boot.BeanDefinitionLoader.load(BeanDefinitionLoader.java:127)  at org.springframework.boot.SpringApplication.load(SpringApplication.java:620)  at org.springframework.boot.SpringApplication.run(SpringApplication.java:315)  at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:139)  at org.springframework.xd.module.core.SimpleModule.initialize(SimpleModule.java:214)  at org.springframework.xd.module.core.CompositeModule.initialize(CompositeModule.java:105)  at org.springframework.xd.dirt.module.ModuleDeployer.doDeploy(ModuleDeployer.java:217)  at org.springframework.xd.dirt.module.ModuleDeployer.deploy(ModuleDeployer.java:200)  at org.springframework.xd.dirt.server.DeploymentListener.deployModule(DeploymentListener.java:363)  at org.springframework.xd.dirt.server.DeploymentListener.deployStreamModule(DeploymentListener.java:332)  at org.springframework.xd.dirt.server.DeploymentListener.onChildAdded(DeploymentListener.java:179)  at org.springframework.xd.dirt.server.DeploymentListener.childEvent(DeploymentListener.java:147)  at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:509)  at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:503)  at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)  at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297)  at org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:83)  at org.apache.curator.framework.recipes.cache.PathChildrenCache.callListeners(PathChildrenCache.java:500)  at org.apache.curator.framework.recipes.cache.EventOperation.invoke(EventOperation.java:35)  at org.apache.curator.framework.recipes.cache.PathChildrenCache$10.run(PathChildrenCache.java:762)  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)  at java.util.concurrent.FutureTask.run(FutureTask.java:262)  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)  at java.util.concurrent.FutureTask.run(FutureTask.java:262)  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)  at java.lang.Thread.run(Thread.java:745) Caused by: org.springframework.beans.factory.parsing.BeanDefinitionParsingException: Configuration problem: Error evaluating Groovy script: ../modules/common/gemfire-sink.groovy (No such file or directory) Offending resource: URL [file:../modules/common/gemfire-sink.groovy]; nested exception is java.io.FileNotFoundException: ../modules/common/gemfire-sink.groovy (No such file or directory)  at org.springframework.beans.factory.groovy.GroovyBeanDefinitionReader.loadBeanDefinitions(GroovyBeanDefinitionReader.java:247)  at org.springframework.beans.factory.groovy.GroovyBeanDefinitionReader.loadBeanDefinitions(GroovyBeanDefinitionReader.java:202)  at org.springframework.beans.factory.support.AbstractBeanDefinitionReader.loadBeanDefinitions(AbstractBeanDefinitionReader.java:181)  at org.springframework.beans.factory.support.AbstractBeanDefinitionReader.loadBeanDefinitions(AbstractBeanDefinitionReader.java:217)  at org.springframework.beans.factory.support.AbstractBeanDefinitionReader.loadBeanDefinitions(AbstractBeanDefinitionReader.java:188)  at org.springframework.beans.factory.groovy.GroovyBeanDefinitionReader.importBeans(GroovyBeanDefinitionReader.java:337)  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)  at java.lang.reflect.Method.invoke(Method.java:606)  at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:90)  at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:324)  at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:368)  at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1016)  at org.codehaus.groovy.runtime.callsite.PogoMetaClassSite.callCurrent(PogoMetaClassSite.java:66)  at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCallCurrent(CallSiteArray.java:49)  at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:133)  at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:141)  at beans$_run_closure1.doCall(beans:4)  at beans$_run_closure1.doCall(beans)  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)  at java.lang.reflect.Method.invoke(Method.java:606)  at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:90)  at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:324)  at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:278)  at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1016)  at groovy.lang.Closure.call(Closure.java:423)  at groovy.lang.Closure.call(Closure.java:417)  at org.springframework.beans.factory.groovy.GroovyBeanDefinitionReader.invokeBeanDefiningClosure(GroovyBeanDefinitionReader.java:426)  at org.springframework.beans.factory.groovy.GroovyBeanDefinitionReader$1.call(GroovyBeanDefinitionReader.java:223)  at groovy.lang.Closure.call(Closure.java:439)  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)  at java.lang.reflect.Method.invoke(Method.java:606)  at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:90)  at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:324)  at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1207)  at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1016)  at groovy.lang.MetaClassImpl.invokePropertyOrMissing(MetaClassImpl.java:1253)  at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1209)  at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1016)  at org.codehaus.groovy.runtime.callsite.PogoMetaClassSite.callCurrent(PogoMetaClassSite.java:66)  at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCallCurrent(CallSiteArray.java:49)  at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:133)  at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:141)  at beans.run(beans:1)  at groovy.lang.GroovyShell.evaluate(GroovyShell.java:649)  at org.springframework.beans.factory.groovy.GroovyBeanDefinitionReader.loadBeanDefinitions(GroovyBeanDefinitionReader.java:243)  ... 30 more Caused by: java.io.FileNotFoundException: ../modules/common/gemfire-sink.groovy (No such file or directory)  at java.io.FileInputStream.open(Native Method)  at java.io.FileInputStream.<init>(FileInputStream.java:146)  at java.io.FileInputStream.<init>(FileInputStream.java:101)  at sun.net.www.protocol.file.FileURLConnection.connect(FileURLConnection.java:90)  at sun.net.www.protocol.file.FileURLConnection.getInputStream(FileURLConnection.java:188)  at org.springframework.core.io.UrlResource.getInputStream(UrlResource.java:168)  at org.springframework.core.io.support.EncodedResource.getReader(EncodedResource.java:132)  at org.springframework.beans.factory.groovy.GroovyBeanDefinitionReader.loadBeanDefinitions(GroovyBeanDefinitionReader.java:243)  ... 80 more """,2.0
1,XD-2819,MAINTENANCE,Done,MEDIUM,"""Broken """"Deployment"""" link in docs""","""Please see """"Deployment"""" link on http://docs.spring.io/autorepo/docs/spring-xd/1.1.0.RELEASE/reference/html/#_module_deployment page.   !broken-link-deployment.png!  The link is broken and redirects to http://docs.spring.io/autorepo/docs/spring-xd/1.1.0.RELEASE/reference/html/Deployment which is a 404.""","""""""Please see """"""""Deployment"""""""" link on http://docs.spring.io/autorepo/docs/spring-xd/1.1.0.RELEASE/reference/html/#_module_deployment page.   !broken-link-deployment.png!  The link is broken and redirects to http://docs.spring.io/autorepo/docs/spring-xd/1.1.0.RELEASE/reference/html/Deployment which is a 404.""""""",,1.0
1,XD-2818,FEATURE,Done,MEDIUM,"""upgrade to io.projectreactor breaks generated POMS""","""./gradlew install fails for spring-xd-extension-batch and spring-xd-extension-reactor. The first case is a simple update to gradle/build-extensions.gradle. The 2nd causes several compilation errors that are not trivial for a Reactor noob.  ""","""""""./gradlew install fails for spring-xd-extension-batch and spring-xd-extension-reactor. The first case is a simple update to gradle/build-extensions.gradle. The 2nd causes several compilation errors that are not trivial for a Reactor noob.  """"""",,2.0
1,XD-2817,BUG,Done,HIGH,"""Classpath issues with gemfire-json-server sink""","""The GemFire client for SpringXD is throwing java.lang.NoClassDefFoundError for the class com/gemstone/gemfire/cache/client/internal/PingOp after a Stream sinking to gemfire-json-server is destroyed.  Issue starts after destroying a stream, which makes me think we might be unloading the jar files from the classpath while still keeping a connection to the gemfire server.  Steps to reproduce:  1) Create a region in Gemfire to test  e.g.: gfsh>create region --name=Stocks --type=REPLICATE Member  | Status ------- | ------------------------------------- server1 | Region """"/Stocks"""" created on """"server1""""   2) Create a simple stream in Spring XD that writes to that region in gemfire-json-server. Deploy it for single node and let it run for a few seconds.   e.g.:  XD$ stream create streamx --definition """"trigger --fixedDelay=3 | http-client --url='''https://query.yahooapis.com/v1/public/yql?q=select * from yahoo.finance.quote where symbol in (\""""MSFT\"""")&format=json&env=store://datatables.org/alltableswithkeys''' --httpMethod=GET | splitter --expression=#jsonPath(payload,'$.query.results.quote') | gemfire-json-server --useLocator=true --host=localhost --port=10334 --regionName=Stocks --keyExpression=payload.getField('Symbol')"""" --deploy   3)  Destroy the stream  e.g.: XD$  stream destroy streamx  3)  Wait a few seconds and check the xd-singlenode output.. you'll see the exception as following:  [error 2015/03/13 11:04:52.437 PDT  <poolTimer-client-pool-14> tid=0x15a] Unexpected error in pool task <com.gemstone.gemfire.cache.client.internal.LiveServerPinger$PingTask@635c9341> java.lang.NoClassDefFoundError: com/gemstone/gemfire/cache/client/internal/PingOp  at com.gemstone.gemfire.cache.client.internal.LiveServerPinger$PingTask.run2(LiveServerPinger.java:83)  at com.gemstone.gemfire.cache.client.internal.PoolImpl$PoolTask.run(PoolImpl.java:1197)  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)  at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)  at com.gemstone.gemfire.internal.ScheduledThreadPoolExecutorWithKeepAlive$DelegatingScheduledFuture.run(ScheduledThreadPoolExecutorWithKeepAlive.java:252)  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)  at java.lang.Thread.run(Thread.java:745)""","""""""The GemFire client for SpringXD is throwing java.lang.NoClassDefFoundError for the class com/gemstone/gemfire/cache/client/internal/PingOp after a Stream sinking to gemfire-json-server is destroyed.  Issue starts after destroying a stream, which makes me think we might be unloading the jar files from the classpath while still keeping a connection to the gemfire server.  Steps to reproduce:  1) Create a region in Gemfire to test  e.g.: gfsh>create region --name=Stocks --type=REPLICATE Member  | Status ------- | ------------------------------------- server1 | Region """"""""/Stocks"""""""" created on """"""""server1""""""""   2) Create a simple stream in Spring XD that writes to that region in gemfire-json-server. Deploy it for single node and let it run for a few seconds.   e.g.:  XD$ stream create streamx --definition """"""""trigger --fixedDelay=3 | http-client --url='''https://query.yahooapis.com/v1/public/yql?q=select * from yahoo.finance.quote where symbol in (\""""""""MSFT\"""""""")&format=json&env=store://datatables.org/alltableswithkeys''' --httpMethod=GET | splitter --expression=#jsonPath(payload,'$.query.results.quote') | gemfire-json-server --useLocator=true --host=localhost --port=10334 --regionName=Stocks --keyExpression=payload.getField('Symbol')"""""""" --deploy   3)  Destroy the stream  e.g.: XD$  stream destroy streamx  3)  Wait a few seconds and check the xd-singlenode output.. you'll see the exception as following:  [error 2015/03/13 11:04:52.437 PDT  <poolTimer-client-pool-14> tid=0x15a] Unexpected error in pool task <com.gemstone.gemfire.cache.client.internal.LiveServerPinger$PingTask@635c9341> java.lang.NoClassDefFoundError: com/gemstone/gemfire/cache/client/internal/PingOp  at com.gemstone.gemfire.cache.client.internal.LiveServerPinger$PingTask.run2(LiveServerPinger.java:83)  at com.gemstone.gemfire.cache.client.internal.PoolImpl$PoolTask.run(PoolImpl.java:1197)  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)  at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)  at com.gemstone.gemfire.internal.ScheduledThreadPoolExecutorWithKeepAlive$DelegatingScheduledFuture.run(ScheduledThreadPoolExecutorWithKeepAlive.java:252)  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)  at java.lang.Thread.run(Thread.java:745)""""""",,3.0
1,XD-2816,FEATURE,Done,MEDIUM,"""Add 'about section' to module description.""","""It should be possible to configure a (short) description for a module that is display above the module options  via {{module info --name ....}}.  The description could contain a few lines describing the core functionality and potentially hyperlinks  to additional information for a module.  This information should be exposed via the REST interface as well.  Currently only the module options are printed.""","""""""It should be possible to configure a (short) description for a module that is display above the module options  via {{module info --name ....}}.  The description could contain a few lines describing the core functionality and potentially hyperlinks  to additional information for a module.  This information should be exposed via the REST interface as well.  Currently only the module options are printed.""""""",,5.0
1,XD-2815,FEATURE,Done,MEDIUM,"""Improve the performance of jdbchdfs batch job""","""As a user, I'd like to use a _jdbchdfs_ batch job as a passthrough (without chunk processing) so that I don't have to incur the batch read/write overhead.""","""""""As a user, I'd like to use a _jdbchdfs_ batch job as a passthrough (without chunk processing) so that I don't have to incur the batch read/write overhead.""""""",,2.0
1,XD-2814,FEATURE,Done,MEDIUM,"""RabbitMQ Dead Letter for TAP not deleted""","""If automatic binding of dead letter is enabled for rabbit mq and taps are deployed, anytime the tap is undeployed, the dead letter for that tap still remains.  The tap uses a unique name and the queue for that is automatically deleted, but the dead letter queue for it is not.  This problem becomes worse when containers are running in yarn and may not live for long periods of time.  Many dead letter queues for taps can become overwhelming.""","""""""If automatic binding of dead letter is enabled for rabbit mq and taps are deployed, anytime the tap is undeployed, the dead letter for that tap still remains.  The tap uses a unique name and the queue for that is automatically deleted, but the dead letter queue for it is not.  This problem becomes worse when containers are running in yarn and may not live for long periods of time.  Many dead letter queues for taps can become overwhelming.""""""",,3.0
1,XD-2813,FEATURE,Done,MEDIUM,"""Investigate running Camus as a batch job""","""Similar to Sqoop where we move data from RDBMS to HDFS we should look at integrating with Camus to load data from Kafka to HDFS.""","""""""Similar to Sqoop where we move data from RDBMS to HDFS we should look at integrating with Camus to load data from Kafka to HDFS.""""""",,5.0
1,XD-2812,BUG,Done,MEDIUM,"""Batch Job deployment screen only show 10 items...""","""The Batch Jobs Deployment screen  ('http://*****************:9393/admin-ui/#/jobs/deployments') UI screen is only showing 10 items without pagination and prohibiting users from launching their deployed jobs from the UI.  The jobs can only be launched via the RESTApi call  In release 1.0.3 the deployment page has not pagination but grows beyond 10 entries.""","""""""The Batch Jobs Deployment screen  ('http://*****************:9393/admin-ui/#/jobs/deployments') UI screen is only showing 10 items without pagination and prohibiting users from launching their deployed jobs from the UI.  The jobs can only be launched via the RESTApi call  In release 1.0.3 the deployment page has not pagination but grows beyond 10 entries.""""""",,5.0
1,XD-2811,BUG,Done,HIGH,"""deployed modules are not redeployed properly once the container come back online""","""Deployed component s are not deployed to the containers that failed and restarted.   We have 3 containers and 3 jobs where all jobs are deployed evenly, one job per container. However, when two of the containers fail and come back up, we end up with 3 jobs on 1 container.  See attached document for detail.  ""","""""""Deployed component s are not deployed to the containers that failed and restarted.   We have 3 containers and 3 jobs where all jobs are deployed evenly, one job per container. However, when two of the containers fail and come back up, we end up with 3 jobs on 1 container.  See attached document for detail.  """"""",,5.0
1,XD-2807,FEATURE,Done,MEDIUM,"""Fix gradle build inconsistencies and leftovers""","""The build has some inconsistencies that should be taken care of. Amongst the one I know:  * The UI project is always getting cleaned, for no apparent reason (there might have been one before), thus triggering a rebuild of everything downstream, most notably DIRT * The exec"""" task is not used anymore * Lots of projects are getting the boot plugin applied to them. I'm not sure 100% what that plugin does, but we don't need the repackage bit for example.""","""""""The build has some inconsistencies that should be taken care of. Amongst the one I know:  * The UI project is always getting cleaned, for no apparent reason (there might have been one before), thus triggering a rebuild of everything downstream, most notably DIRT * The exec"""""""" task is not used anymore * Lots of projects are getting the boot plugin applied to them. I'm not sure 100% what that plugin does, but we don't need the repackage bit for example.""""""",,3.0
1,XD-2806,BUG,Done,URGENT,"""Module count not respected when label is used""","""{code} xd:> stream create test --definition """"http | t1:transform --expression=payload | log"""" xd:>stream deploy test --properties module.t1.count=2 Deployed stream 'test' xd:>runtime modules   Module Id            Container Id                          Options                                                                                                                                                                                            Deployment Properties                                                                       Unit status   -------------------  ------------------------------------  -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------  ------------------------------------------------------------------------------------------  -----------   test.processor.t1.1  393d3af0-68e8-49b2-8601-da063cfbf98a  {valid=true, expression=payload}                                                                                                                                                                   {consumer.sequence=1, producer.next.module.count=1, count=1, consumer.count=1, sequence=1}  deployed   test.sink.log.1      f6bb3189-9c0e-44e8-962b-025e2288ffe3  {name=test, expression=payload, level=INFO}                                                                                                                                                        {consumer.sequence=1, count=1, consumer.count=1, sequence=1}                                deployed   test.source.http.1   f6bb3189-9c0e-44e8-962b-025e2288ffe3  {sslPropertiesLocation=classpath:httpSSL.properties, maxContentLength=1048576, port=9000, messageConverterClass=org.springframework.integration.x.http.NettyInboundMessageConverter, https=false}  {producer.next.module.count=1, count=1, sequence=1}                                         deployed {code}  ************************************* Works fine without the label: ************************************* {code} xd:>stream destroy test Destroyed stream 'test' xd:>stream create test --definition """"http | transform --expression=payload | log"""" Created new stream 'test' xd:>stream deploy test --properties module.transform.count=2 Deployed stream 'test' xd:>runtime modules   Module Id                   Container Id                          Options                                                                                                                                                                                            Deployment Properties                                                                       Unit status   --------------------------  ------------------------------------  -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------  ------------------------------------------------------------------------------------------  -----------   test.processor.transform.1  f6bb3189-9c0e-44e8-962b-025e2288ffe3  {valid=true, expression=payload}                                                                                                                                                                   {consumer.sequence=1, producer.next.module.count=1, count=2, consumer.count=2, sequence=1}  deployed   test.processor.transform.2  393d3af0-68e8-49b2-8601-da063cfbf98a  {valid=true, expression=payload}                                                                                                                                                                   {consumer.sequence=2, producer.next.module.count=1, count=2, consumer.count=2, sequence=2}  deployed   test.sink.log.1             393d3af0-68e8-49b2-8601-da063cfbf98a  {name=test, expression=payload, level=INFO}                                                                                                                                                        {consumer.sequence=1, count=1, consumer.count=1, sequence=1}                                deployed   test.source.http.1          f6bb3189-9c0e-44e8-962b-025e2288ffe3  {sslPropertiesLocation=classpath:httpSSL.properties, maxContentLength=1048576, port=9000, messageConverterClass=org.springframework.integration.x.http.NettyInboundMessageConverter, https=false}  {producer.next.module.count=2, count=1, sequence=1}                                         deployed {code}""","""""""  ************************************* Works fine without the label: ************************************* """"""",""" xd:> stream create test --definition """"""""http | t1:transform --expression=payload | log"""""""" xd:>stream deploy test --properties module.t1.count=2 Deployed stream 'test' xd:>runtime modules   Module Id            Container Id                          Options                                                                                                                                                                                            Deployment Properties                                                                       Unit status   -------------------  ------------------------------------  -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------  ------------------------------------------------------------------------------------------  -----------   test.processor.t1.1  393d3af0-68e8-49b2-8601-da063cfbf98a  {valid=true, expression=payload}                                                                                                                                                                   {consumer.sequence=1, producer.next.module.count=1, count=1, consumer.count=1, sequence=1}  deployed   test.sink.log.1      f6bb3189-9c0e-44e8-962b-025e2288ffe3  {name=test, expression=payload, level=INFO}                                                                                                                                                        {consumer.sequence=1, count=1, consumer.count=1, sequence=1}                                deployed   test.source.http.1   f6bb3189-9c0e-44e8-962b-025e2288ffe3  {sslPropertiesLocation=classpath:httpSSL.properties, maxContentLength=1048576, port=9000, messageConverterClass=org.springframework.integration.x.http.NettyInboundMessageConverter, https=false}  {producer.next.module.count=1, count=1, sequence=1}                                         deployed  xd:>stream destroy test Destroyed stream 'test' xd:>stream create test --definition """"""""http | transform --expression=payload | log"""""""" Created new stream 'test' xd:>stream deploy test --properties module.transform.count=2 Deployed stream 'test' xd:>runtime modules   Module Id                   Container Id                          Options                                                                                                                                                                                            Deployment Properties                                                                       Unit status   --------------------------  ------------------------------------  -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------  ------------------------------------------------------------------------------------------  -----------   test.processor.transform.1  f6bb3189-9c0e-44e8-962b-025e2288ffe3  {valid=true, expression=payload}                                                                                                                                                                   {consumer.sequence=1, producer.next.module.count=1, count=2, consumer.count=2, sequence=1}  deployed   test.processor.transform.2  393d3af0-68e8-49b2-8601-da063cfbf98a  {valid=true, expression=payload}                                                                                                                                                                   {consumer.sequence=2, producer.next.module.count=1, count=2, consumer.count=2, sequence=2}  deployed   test.sink.log.1             393d3af0-68e8-49b2-8601-da063cfbf98a  {name=test, expression=payload, level=INFO}                                                                                                                                                        {consumer.sequence=1, count=1, consumer.count=1, sequence=1}                                deployed   test.source.http.1          f6bb3189-9c0e-44e8-962b-025e2288ffe3  {sslPropertiesLocation=classpath:httpSSL.properties, maxContentLength=1048576, port=9000, messageConverterClass=org.springframework.integration.x.http.NettyInboundMessageConverter, https=false}  {producer.next.module.count=2, count=1, sequence=1}                                         deployed """,3.0
1,XD-2805,FEATURE,Done,MEDIUM,"""Add support to include """"namenode"""" address from a config file""","""As a user, I'd like to add the Hadoop _namenode_ specifics in a config file so that I don't have to incur the hassle of pointing to the _namenode_ location every time I open a new DSL session, but it is automatically configured. ""","""""""As a user, I'd like to add the Hadoop _namenode_ specifics in a config file so that I don't have to incur the hassle of pointing to the _namenode_ location every time I open a new DSL session, but it is automatically configured. """"""",,3.0
1,XD-2804,BUG,Done,HIGH,"""Module options are not trimmed""","""Spring XD 1.1 container will throw following exception:  {code} java.lang.IllegalStateException: Can't find class used for type of option 'myField': String   at org.springframework.xd.module.options.DefaultModuleOptionsMetadataResolver.makeSimpleModuleOptions(DefaultModuleOptionsMetadataResolver.java:147)  at org.springframework.xd.module.options.DefaultModuleOptionsMetadataResolver.resolveNormalMetadata(DefaultModuleOptionsMetadataResolver.java:202)  at org.springframework.xd.module.options.DefaultModuleOptionsMetadataResolver.resolve(DefaultModuleOptionsMetadataResolver.java:164)  at org.springframework.xd.module.options.DelegatingModuleOptionsMetadataResolver.resolve(DelegatingModuleOptionsMetadataResolver.java:44)  at org.springframework.xd.module.options.EnvironmentAwareModuleOptionsMetadataResolver.resolve(EnvironmentAwareModuleOptionsMetadataResolver.java:127)  at org.springframework.xd.dirt.stream.XDStreamParser.parse(XDStreamParser.java:174)  at org.springframework.xd.dirt.stream.AbstractDeployer.save(AbstractDeployer.java:96)  ... {code}  when module properties have a trailing whitespace character in type property (in example below there is a trailing space in options.myField.type value):  {code} options.myField.description = this is my field options.myField.type = String  {code}  Can the property values be trimmed before comparing to DefaultModuleOptionsMetadataResolver#SHORT_CLASSNAMES map  to avoid this problem?""","""""""Spring XD 1.1 container will throw following exception:    when module properties have a trailing whitespace character in type property (in example below there is a trailing space in options.myField.type value):    Can the property values be trimmed before comparing to DefaultModuleOptionsMetadataResolver#SHORT_CLASSNAMES map  to avoid this problem?""""""",""" java.lang.IllegalStateException: Can't find class used for type of option 'myField': String   at org.springframework.xd.module.options.DefaultModuleOptionsMetadataResolver.makeSimpleModuleOptions(DefaultModuleOptionsMetadataResolver.java:147)  at org.springframework.xd.module.options.DefaultModuleOptionsMetadataResolver.resolveNormalMetadata(DefaultModuleOptionsMetadataResolver.java:202)  at org.springframework.xd.module.options.DefaultModuleOptionsMetadataResolver.resolve(DefaultModuleOptionsMetadataResolver.java:164)  at org.springframework.xd.module.options.DelegatingModuleOptionsMetadataResolver.resolve(DelegatingModuleOptionsMetadataResolver.java:44)  at org.springframework.xd.module.options.EnvironmentAwareModuleOptionsMetadataResolver.resolve(EnvironmentAwareModuleOptionsMetadataResolver.java:127)  at org.springframework.xd.dirt.stream.XDStreamParser.parse(XDStreamParser.java:174)  at org.springframework.xd.dirt.stream.AbstractDeployer.save(AbstractDeployer.java:96)  ...  options.myField.description = this is my field options.myField.type = String  """,1.0
1,XD-2802,FEATURE,Done,MEDIUM,"""Migrate wiki documentation and the chores""","""As a developer, I'd like to migrate the wiki to project repo so that it can be tagged with the code and versioned etc. ""","""""""As a developer, I'd like to migrate the wiki to project repo so that it can be tagged with the code and versioned etc. """"""",,8.0
1,XD-2801,FEATURE,Done,MEDIUM,"""Updated XD-EC2 XD deployment for 1.2""","""Mask out all properties for XD-EC2""","""""""Mask out all properties for XD-EC2""""""",,3.0
1,XD-2800,MAINTENANCE,Done,MEDIUM,"""Add support for time/sequence-size windowed offset updates""","""Add support for time/sequence-size windowed offset updates""","""Add support for time/sequence-size windowed offset updates""",,2.0
1,XD-2799,MAINTENANCE,Done,MEDIUM,"""Expose bean settings as configuration options to the Kafka source and bus""","""Expose bean settings as configuration options to the Kafka source and bus""","""Expose bean settings as configuration options to the Kafka source and bus""",,5.0
1,XD-2798,MAINTENANCE,Done,URGENT,"""Redis and In-memory offset storage profiles for the kafka source have wrong definitions""","""Redis and In-memory offset storage profiles for the kafka source have wrong definitions""","""Redis and In-memory offset storage profiles for the kafka source have wrong definitions""",,1.0
1,XD-2797,FEATURE,Done,MEDIUM,"""Lattice Design Spike""","""As a developer, I'd like to continue Lattice/Diego POC so that I can identify the scope, risks, and the overall design for a pluggable SPI in XD runtime.""","""""""As a developer, I'd like to continue Lattice/Diego POC so that I can identify the scope, risks, and the overall design for a pluggable SPI in XD runtime.""""""",,5.0
1,XD-2795,FEATURE,Done,MEDIUM,"""Measure performance baseline for a simple stream""","""As a developer, I'd like to measure performance numbers for a simple stream so that I can characterize the overall throughput. ""","""""""As a developer, I'd like to measure performance numbers for a simple stream so that I can characterize the overall throughput. """"""",,8.0
1,XD-2794,IMPROVEMENT,Done,MEDIUM,"""Add a MongoDB source""","""As a developer, I'd like to add a mongodb source using an xml and a property file supporting mixing in of parameters so that I can use this module to ingest data from Mongo.""","""""""As a developer, I'd like to add a mongodb source using an xml and a property file supporting mixing in of parameters so that I can use this module to ingest data from Mongo.""""""",,5.0
1,XD-2793,FEATURE,Done,MEDIUM,"""Fix offset management for Kafka source""","""As a developer, I'd like to fix the offset management with Kafka _source_ module so that I can efficiently perform fetch operation from the given offsets.""","""""""As a developer, I'd like to fix the offset management with Kafka _source_ module so that I can efficiently perform fetch operation from the given offsets.""""""",,8.0
1,XD-2792,FEATURE,Done,MEDIUM,"""Automate provisioning story for XD""","""Automate provisioning story for XD""","""Automate provisioning story for XD""",,8.0
1,XD-2791,FEATURE,Done,MEDIUM,"""Complete CI setup for Windows""","""As a build manager, I'd like to schedule CI builds for windows so that I can verify XD runtime features/functionality.  The scope is to isolate the remaining test failures; perhaps, experiment with new AMI images until we have a solid infrastructure to fix the failing tests.""","""""""As a build manager, I'd like to schedule CI builds for windows so that I can verify XD runtime features/functionality.  The scope is to isolate the remaining test failures; perhaps, experiment with new AMI images until we have a solid infrastructure to fix the failing tests.""""""",,1.0
1,XD-2790,FEATURE,Done,MEDIUM,"""Rabbit source and sink mappedRequestHeaders should include all headers by default""","""Currently it is necessary to specify mappedRequestHeaders=*  on the rabbit sink, otherwise no headers are mapped to AMQP.  This should be the default behavior.""","""""""Currently it is necessary to specify mappedRequestHeaders=*  on the rabbit sink, otherwise no headers are mapped to AMQP.  This should be the default behavior.""""""",,1.0
1,XD-2789,BUG,Done,MEDIUM,"""module delete on windows throws exception""","""used module upload for processor:payload-conversion (from XD samples) All worked well until I tried to delete the module. customModule in servers.yml was set to: xd:   customModule:     home: file://c:/project/mymodulehome  StackTrace: {noformat} 2015-03-06 01:54:33,460 1.1.0.RELEASE ERROR qtp1891077689-37 rest.RestController Advice - Caught exception while handling a request java.lang.IllegalArgumentException: Could not delete module 'processor:payload-c onversion'         at org.springframework.util.Assert.isTrue(Assert.java:65)         at org.springframework.xd.dirt.module.ModuleDefinitionService.delete(Mod uleDefinitionService.java:121)         at org.springframework.xd.dirt.rest.ModulesController.delete(ModulesCont roller.java:155)         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)         at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)         at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)         at java.lang.reflect.Method.invoke(Unknown Source)         at org.springframework.web.method.support.InvocableHandlerMethod.doInvok e(InvocableHandlerMethod.java:221)         at org.springframework.web.method.support.InvocableHandlerMethod.invokeF orRequest(InvocableHandlerMethod.java:137)         at org.springframework.web.servlet.mvc.method.annotation.ServletInvocabl eHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:110)         at org.springframework.web.servlet.mvc.method.annotation.RequestMappingH andlerAdapter.invokeHandleMethod(RequestMappingHandlerAdapter.java:777)         at org.springframework.web.servlet.mvc.method.annotation.RequestMappingH andlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:706)         at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapt er.handle(AbstractHandlerMethodAdapter.java:85)         at org.springframework.web.servlet.DispatcherServlet.doDispatch(Dispatch erServlet.java:943)         at org.springframework.web.servlet.DispatcherServlet.doService(Dispatche rServlet.java:877)         at org.springframework.web.servlet.FrameworkServlet.processRequest(Frame workServlet.java:966)         at org.springframework.web.servlet.FrameworkServlet.doDelete(FrameworkSe rvlet.java:890)         at javax.servlet.http.HttpServlet.service(HttpServlet.java:761)         at org.springframework.web.servlet.FrameworkServlet.service(FrameworkSer vlet.java:842)         at javax.servlet.http.HttpServlet.service(HttpServlet.java:848)         at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:684 )         at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(Servlet Handler.java:1496)         at org.springframework.boot.actuate.autoconfigure.EndpointWebMvcAutoConf iguration$ApplicationContextHeaderFilter.doFilterInternal(EndpointWebMvcAutoConf iguration.java:291)         at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerR equestFilter.java:107)         at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(Servlet Handler.java:1467)         at org.springframework.web.filter.HiddenHttpMethodFilter.doFilterInterna l(HiddenHttpMethodFilter.java:77)         at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerR equestFilter.java:107)         at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(Servlet Handler.java:1467)         at org.springframework.web.filter.HttpPutFormContentFilter.doFilterInter nal(HttpPutFormContentFilter.java:87)         at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerR equestFilter.java:107)         at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(Servlet Handler.java:1467)         at org.springframework.boot.actuate.trace.WebRequestTraceFilter.doFilter Internal(WebRequestTraceFilter.java:100)         at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerR equestFilter.java:107)         at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(Servlet Handler.java:1467)         at org.springframework.security.web.FilterChainProxy.doFilterInternal(Fi lterChainProxy.java:186)         at org.springframework.security.web.FilterChainProxy.doFilter(FilterChai nProxy.java:160)         at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(Servlet Handler.java:1467)         at org.springframework.boot.actuate.autoconfigure.MetricFilterAutoConfig uration$MetricsFilter.doFilterInternal(MetricFilterAutoConfiguration.java:90)         at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerR equestFilter.java:107)         at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(Servlet Handler.java:1467)         at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java :499)         at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.j ava:137)         at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.jav a:557)         at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandl er.java:231)         at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandl er.java:1086)         at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java: 428)         at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandle r.java:193)         at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandle r.java:1020)         at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.j ava:135)         at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper .java:116)         at org.eclipse.jetty.server.Server.handle(Server.java:370)         at org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(Abstrac tHttpConnection.java:494)         at org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(Abstra ctHttpConnection.java:971)         at org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.header Complete(AbstractHttpConnection.java:1033)         at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:644)         at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235)          at org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnecti on.java:82)         at org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEn dPoint.java:667)         at org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEnd Point.java:52)         at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPoo l.java:608)         at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool .java:543)         at java.lang.Thread.run(Unknown Source)  {noformat}""","""""""used module upload for processor:payload-conversion (from XD samples) All worked well until I tried to delete the module. customModule in servers.yml was set to: xd:   customModule:     home: file://c:/project/mymodulehome  StackTrace: """"""",""" 2015-03-06 01:54:33,460 1.1.0.RELEASE ERROR qtp1891077689-37 rest.RestController Advice - Caught exception while handling a request java.lang.IllegalArgumentException: Could not delete module 'processor:payload-c onversion'         at org.springframework.util.Assert.isTrue(Assert.java:65)         at org.springframework.xd.dirt.module.ModuleDefinitionService.delete(Mod uleDefinitionService.java:121)         at org.springframework.xd.dirt.rest.ModulesController.delete(ModulesCont roller.java:155)         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)         at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)         at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)         at java.lang.reflect.Method.invoke(Unknown Source)         at org.springframework.web.method.support.InvocableHandlerMethod.doInvok e(InvocableHandlerMethod.java:221)         at org.springframework.web.method.support.InvocableHandlerMethod.invokeF orRequest(InvocableHandlerMethod.java:137)         at org.springframework.web.servlet.mvc.method.annotation.ServletInvocabl eHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:110)         at org.springframework.web.servlet.mvc.method.annotation.RequestMappingH andlerAdapter.invokeHandleMethod(RequestMappingHandlerAdapter.java:777)         at org.springframework.web.servlet.mvc.method.annotation.RequestMappingH andlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:706)         at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapt er.handle(AbstractHandlerMethodAdapter.java:85)         at org.springframework.web.servlet.DispatcherServlet.doDispatch(Dispatch erServlet.java:943)         at org.springframework.web.servlet.DispatcherServlet.doService(Dispatche rServlet.java:877)         at org.springframework.web.servlet.FrameworkServlet.processRequest(Frame workServlet.java:966)         at org.springframework.web.servlet.FrameworkServlet.doDelete(FrameworkSe rvlet.java:890)         at javax.servlet.http.HttpServlet.service(HttpServlet.java:761)         at org.springframework.web.servlet.FrameworkServlet.service(FrameworkSer vlet.java:842)         at javax.servlet.http.HttpServlet.service(HttpServlet.java:848)         at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:684 )         at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(Servlet Handler.java:1496)         at org.springframework.boot.actuate.autoconfigure.EndpointWebMvcAutoConf iguration$ApplicationContextHeaderFilter.doFilterInternal(EndpointWebMvcAutoConf iguration.java:291)         at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerR equestFilter.java:107)         at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(Servlet Handler.java:1467)         at org.springframework.web.filter.HiddenHttpMethodFilter.doFilterInterna l(HiddenHttpMethodFilter.java:77)         at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerR equestFilter.java:107)         at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(Servlet Handler.java:1467)         at org.springframework.web.filter.HttpPutFormContentFilter.doFilterInter nal(HttpPutFormContentFilter.java:87)         at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerR equestFilter.java:107)         at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(Servlet Handler.java:1467)         at org.springframework.boot.actuate.trace.WebRequestTraceFilter.doFilter Internal(WebRequestTraceFilter.java:100)         at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerR equestFilter.java:107)         at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(Servlet Handler.java:1467)         at org.springframework.security.web.FilterChainProxy.doFilterInternal(Fi lterChainProxy.java:186)         at org.springframework.security.web.FilterChainProxy.doFilter(FilterChai nProxy.java:160)         at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(Servlet Handler.java:1467)         at org.springframework.boot.actuate.autoconfigure.MetricFilterAutoConfig uration$MetricsFilter.doFilterInternal(MetricFilterAutoConfiguration.java:90)         at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerR equestFilter.java:107)         at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(Servlet Handler.java:1467)         at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java :499)         at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.j ava:137)         at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.jav a:557)         at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandl er.java:231)         at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandl er.java:1086)         at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java: 428)         at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandle r.java:193)         at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandle r.java:1020)         at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.j ava:135)         at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper .java:116)         at org.eclipse.jetty.server.Server.handle(Server.java:370)         at org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(Abstrac tHttpConnection.java:494)         at org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(Abstra ctHttpConnection.java:971)         at org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.header Complete(AbstractHttpConnection.java:1033)         at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:644)         at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235)          at org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnecti on.java:82)         at org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEn dPoint.java:667)         at org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEnd Point.java:52)         at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPoo l.java:608)         at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool .java:543)         at java.lang.Thread.run(Unknown Source)  """,3.0
1,XD-2788,FEATURE,Done,MEDIUM,"""Add throughput receiving sink""","""As a developer, I'd like to add load receiving _sink_ module so that I can measure received throughput""","""""""As a developer, I'd like to add load receiving _sink_ module so that I can measure received throughput""""""",,3.0
1,XD-2787,FEATURE,Done,MEDIUM,"""Add load generator source""","""As a developer, I'd like to add load generator _source_ module so that I could use it for performance testing use-cases.  ""","""""""As a developer, I'd like to add load generator _source_ module so that I could use it for performance testing use-cases.  """"""",,3.0
1,XD-2786,FEATURE,Done,MEDIUM,"""Create EC2 AMI image for performance testing""","""As a developer, I'd like to create EC2 AMI with the necessary packages so that I can run the Kafka Perf tests.""","""""""As a developer, I'd like to create EC2 AMI with the necessary packages so that I can run the Kafka Perf tests.""""""",,2.0
1,XD-2785,FEATURE,Done,MEDIUM,"""Identify the Kafka configuration for Kafka performance tests""","""As a developer, I'd like to identify the Kafka configurations so that I could setup infrastructure to perform performance testing. ""","""""""As a developer, I'd like to identify the Kafka configurations so that I could setup infrastructure to perform performance testing. """"""",,3.0
1,XD-2784,FEATURE,Done,MEDIUM,"""Research EC2 infrastructure required for Kafka performance tests""","""As a developer, I'd like to research and Identify the EC2 infrastructure required  so that I can run performance tests on Kafka. ""","""""""As a developer, I'd like to research and Identify the EC2 infrastructure required  so that I can run performance tests on Kafka. """"""",,3.0
1,XD-2783,BUG,Done,MEDIUM,"""RabbitMQ server.yml options ignored""","""{{p-spring-xd}} defines values for the following parameters in {{servers.yml}}. These values are not being retrieved, and hence have to be manually added to each stream definition. * {{addresses}} * {{username}} * {{password}} * {{virtual_host}}  (cf XD-2675, XD-2741)""","""""""{{p-spring-xd}} defines values for the following parameters in {{servers.yml}}. These values are not being retrieved, and hence have to be manually added to each stream definition. * {{addresses}} * {{username}} * {{password}} * {{virtual_host}}  (cf XD-2675, XD-2741)""""""",,2.0
1,XD-2782,BUG,Done,HIGH,"""PostgreSQL server.yml options ignored""","""{{p-spring-xd}} defines values for the following parameters in {{servers.yml}}. These values are not being retrieved, and hence have to be manually added to each stream definition.  * {{url}} * {{username}} * {{password}} * {{driverClassName}} * {{validationQuery}}  (cf XD-2675, XD-2741)""","""""""{{p-spring-xd}} defines values for the following parameters in {{servers.yml}}. These values are not being retrieved, and hence have to be manually added to each stream definition.  * {{url}} * {{username}} * {{password}} * {{driverClassName}} * {{validationQuery}}  (cf XD-2675, XD-2741)""""""",,2.0
1,XD-2781,BUG,Done,HIGH,"""Not possible to create a RabbitMQ to RabbitMQ stream""","""We get the following error when trying a rabbit to rabbit stream (definition as shown in the error):  {code} [STREAM TEST] WARN  Failure during end to end test org.springframework.xd.rest.client.impl.SpringXDException: XD143E:(pos 141): Label 'rabbit' should be unique but module 'rabbit' (at position 0) and module 'rabbit' (at position 1) both use it rabbit --queues=cxablknzdhvpotpo-source --addresses=10.85.30.129 --username=bef4412739e7d7fe929e --password=b8ace17e56456b7753a2 --vhost=/ | rabbit --exchange=cxablknzdhvpotpo-sink --addresses=10.85.30.129 --username=bef4412739e7d7fe929e --password=b8ace17e56456b7753a2 --vhost=/                                                                                                                                              ^ {code}""","""""""We get the following error when trying a rabbit to rabbit stream (definition as shown in the error):  """"""",""" [STREAM TEST] WARN  Failure during end to end test org.springframework.xd.rest.client.impl.SpringXDException: XD143E:(pos 141): Label 'rabbit' should be unique but module 'rabbit' (at position 0) and module 'rabbit' (at position 1) both use it rabbit --queues=cxablknzdhvpotpo-source --addresses=10.85.30.129 --username=bef4412739e7d7fe929e --password=b8ace17e56456b7753a2 --vhost=/ | rabbit --exchange=cxablknzdhvpotpo-sink --addresses=10.85.30.129 --username=bef4412739e7d7fe929e --password=b8ace17e56456b7753a2 --vhost=/                                                                                                                                              ^ """,3.0
1,XD-2780,BUG,Done,HIGH,"""Unable to define port on RabbitMQ streams""","""The documentation for RabbitMQ says to use the --address option to specify a port. We've tried --address=1.2.3.4:5678 (and also --port=5678 just in case) but have been unable to get a successful deployment. Running the same stream definition on a default port (i.e. without specifying a port) does work.""","""""""The documentation for RabbitMQ says to use the --address option to specify a port. We've tried --address=1.2.3.4:5678 (and also --port=5678 just in case) but have been unable to get a successful deployment. Running the same stream definition on a default port (i.e. without specifying a port) does work.""""""",,2.0
1,XD-2779,BUG,Done,MEDIUM,"""Fix error handling in jdbchdfs job ""","""The jdbchdfs job keeps the output stream open in case of error writing to HDFS. We should improve this and close it plus throw an exception.  We should also make sure the step is marked as failed instead of complete when an exception is thrown in the writer.""","""""""The jdbchdfs job keeps the output stream open in case of error writing to HDFS. We should improve this and close it plus throw an exception.  We should also make sure the step is marked as failed instead of complete when an exception is thrown in the writer.""""""",,3.0
1,XD-2778,FEATURE,Done,MEDIUM,"""Document the changes to Message Headers in 1.1""","""As a developer, I'd like to document the changes to message headers so that users can refer to the troubleshooting section if there are any serialization errors when reusing the 1.0 batch-jobs in 1.1 release.  Perhaps this could be part of [troubleshooting|https://github.com/spring-projects/spring-xd/wiki/Deployment#troubleshooting] section in our wiki.""","""""""As a developer, I'd like to document the changes to message headers so that users can refer to the troubleshooting section if there are any serialization errors when reusing the 1.0 batch-jobs in 1.1 release.  Perhaps this could be part of [troubleshooting|https://github.com/spring-projects/spring-xd/wiki/Deployment#troubleshooting] section in our wiki.""""""",,1.0
1,XD-2777,FEATURE,Done,MEDIUM,"""Document trigger source""","""Document trigger source""","""Document trigger source""",,1.0
1,XD-2776,FEATURE,Done,MEDIUM,"""Reproduce baseline numbers for Kafka""","""As a developer, I'd like to bench Kafka as message bus using in-built perf-testing producer/consumer utilities so that I can use that as a foundation to build XD use-cases and measure performance.   I'd like to reproduce baseline performance metrics as identified by the Kafka [engineering team|https://engineering.linkedin.com/kafka/benchmarking-apache-kafka-2-million-writes-second-three-cheap-machines].""","""""""As a developer, I'd like to bench Kafka as message bus using in-built perf-testing producer/consumer utilities so that I can use that as a foundation to build XD use-cases and measure performance.   I'd like to reproduce baseline performance metrics as identified by the Kafka [engineering team|https://engineering.linkedin.com/kafka/benchmarking-apache-kafka-2-million-writes-second-three-cheap-machines].""""""",,8.0
1,XD-2775,MAINTENANCE,Done,MEDIUM,"""Upgrade to SHDP 2.1.2 GA release""","""As a developer, I'd like to upgrade to SHDP 2.1.2 GA so that I can sync-up with latest features.""","""""""As a developer, I'd like to upgrade to SHDP 2.1.2 GA so that I can sync-up with latest features.""""""",,1.0
1,XD-2774,BUG,Done,URGENT,"""Update to SHDP 2.1.1 for fixing hdfs store writer to recover after error writing to hdfs""","""The hdfs sink doesn't recover after error writing to hdfs.  Steps to reproduce -  create a stream using hdfs sink with a small rollover:  {code} xd:>stream create --name errtest --definition """"time | hdfs --rollover=50"""" --deploy  {code}  stop the datanode(s) and wait for an exception like:  {code} 2015-03-03 10:41:57,832 1.1.0.RELEASE ERROR task-scheduler-3 handler.LoggingHandler - org.springframework.messaging.MessageHandlingException: failed to write Message payload to HDFS; nested exception is org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /xd/errtest/errtest-7.txt.tmp could only be replicated to 0 nodes instead of minReplication (=1).  There are 1 datanode(s) running and 1 node(s) are excluded in this operation.  at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1549)  at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3200)  at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:641)  at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:482)  at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)  at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)  at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:962)  at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2039)  at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2035)  at java.security.AccessController.doPrivileged(Native Method)  at javax.security.auth.Subject.doAs(Subject.java:415)  at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)  at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2033)   at org.springframework.xd.integration.hadoop.outbound.HdfsStoreMessageHandler.handleMessageInternal(HdfsStoreMessageHandler.java:129)  at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78)  at sun.reflect.GeneratedMethodAccessor86.invoke(Unknown Source)  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)  at java.lang.reflect.Method.invoke(Method.java:606)  at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)  at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)  at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)  at org.springframework.integration.monitor.SimpleMessageHandlerMetrics.handleMessage(SimpleMessageHandlerMetrics.java:107)  at org.springframework.integration.monitor.SimpleMessageHandlerMetrics.invoke(SimpleMessageHandlerMetrics.java:87)  at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)  at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)  at com.sun.proxy.$Proxy136.handleMessage(Unknown Source)  at org.springframework.integration.dispatcher.AbstractDispatcher.tryOptimizedDispatch(AbstractDispatcher.java:116)  at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:101)  at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)  at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)  at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:277)  at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:239)  at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)  at java.lang.reflect.Method.invoke(Method.java:606)  at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)  at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)  at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)  at org.springframework.integration.monitor.DirectChannelMetrics.monitorSend(DirectChannelMetrics.java:114)  at org.springframework.integration.monitor.DirectChannelMetrics.doInvoke(DirectChannelMetrics.java:98)  at org.springframework.integration.monitor.DirectChannelMetrics.invoke(DirectChannelMetrics.java:92)  at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)  at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)  at com.sun.proxy.$Proxy125.send(Unknown Source)  at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:115)  at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:45)  at org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:95)  at org.springframework.integration.handler.AbstractMessageProducingHandler.sendOutput(AbstractMessageProducingHandler.java:248)  at org.springframework.integration.handler.AbstractMessageProducingHandler.produceOutput(AbstractMessageProducingHandler.java:171)  at org.springframework.integration.handler.AbstractMessageProducingHandler.sendOutputs(AbstractMessageProducingHandler.java:119)  at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:105)  at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78)  at org.springframework.integration.dispatcher.AbstractDispatcher.tryOptimizedDispatch(AbstractDispatcher.java:116)  at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:101)  at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)  at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)  at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:277)  at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:239)  at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:115)  at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:45)  at org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:95)  at org.springframework.integration.handler.AbstractMessageProducingHandler.sendOutput(AbstractMessageProducingHandler.java:248)  at org.springframework.integration.handler.AbstractMessageProducingHandler.produceOutput(AbstractMessageProducingHandler.java:171)  at org.springframework.integration.handler.AbstractMessageProducingHandler.sendOutputs(AbstractMessageProducingHandler.java:119)  at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:105)  at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78)  at org.springframework.integration.dispatcher.AbstractDispatcher.tryOptimizedDispatch(AbstractDispatcher.java:116)  at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:101)  at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)  at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)  at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:277)  at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:239)  at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)  at java.lang.reflect.Method.invoke(Method.java:606)  at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)  at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)  at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)  at org.springframework.integration.monitor.DirectChannelMetrics.monitorSend(DirectChannelMetrics.java:114)  at org.springframework.integration.monitor.DirectChannelMetrics.doInvoke(DirectChannelMetrics.java:98)  at org.springframework.integration.monitor.DirectChannelMetrics.invoke(DirectChannelMetrics.java:92)  at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)  at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)  at com.sun.proxy.$Proxy137.send(Unknown Source)  at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:115)  at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:45)  at org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:95)  at org.springframework.integration.endpoint.SourcePollingChannelAdapter.handleMessage(SourcePollingChannelAdapter.java:130)  at org.springframework.integration.endpoint.AbstractPollingEndpoint.doPoll(AbstractPollingEndpoint.java:219)  at org.springframework.integration.endpoint.AbstractPollingEndpoint.access$000(AbstractPollingEndpoint.java:55)  at org.springframework.integration.endpoint.AbstractPollingEndpoint$1.call(AbstractPollingEndpoint.java:149)  at org.springframework.integration.endpoint.AbstractPollingEndpoint$1.call(AbstractPollingEndpoint.java:146)  at org.springframework.integration.endpoint.AbstractPollingEndpoint$Poller$1.run(AbstractPollingEndpoint.java:298)  at org.springframework.integration.util.ErrorHandlingTaskExecutor$1.run(ErrorHandlingTaskExecutor.java:52)  at org.springframework.core.task.SyncTaskExecutor.execute(SyncTaskExecutor.java:50)  at org.springframework.integration.util.ErrorHandlingTaskExecutor.execute(ErrorHandlingTaskExecutor.java:49)  at org.springframework.integration.endpoint.AbstractPollingEndpoint$Poller.run(AbstractPollingEndpoint.java:292)  at org.springframework.scheduling.support.DelegatingErrorHandlingRunnable.run(DelegatingErrorHandlingRunnable.java:54)  at org.springframework.scheduling.concurrent.ReschedulingRunnable.run(ReschedulingRunnable.java:81)  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)  at java.util.concurrent.FutureTask.run(FutureTask.java:262)  at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)  at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)  at java.lang.Thread.run(Thread.java:745) Caused by: org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /xd/errtest/errtest-7.txt.tmp could only be replicated to 0 nodes instead of minReplication (=1).  There are 1 datanode(s) running and 1 node(s) are excluded in this operation.  at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1549)  at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3200)  at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:641)  at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:482)  at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)  at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)  at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:962)  at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2039)  at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2035)  at java.security.AccessController.doPrivileged(Native Method)  at javax.security.auth.Subject.doAs(Subject.java:415)  at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)  at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2033)   at org.apache.hadoop.ipc.Client.call(Client.java:1468)  at org.apache.hadoop.ipc.Client.call(Client.java:1399)  at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)  at com.sun.proxy.$Proxy134.addBlock(Unknown Source)  at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:399)  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)  at java.lang.reflect.Method.invoke(Method.java:606)  at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)  at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)  at com.sun.proxy.$Proxy135.addBlock(Unknown Source)  at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1532)  at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1349)  at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:588) {code}  start the datanode(s) again, the sink never recovers and has to be undeployed and redeployed. ""","""""""The hdfs sink doesn't recover after error writing to hdfs.  Steps to reproduce -  create a stream using hdfs sink with a small rollover:    stop the datanode(s) and wait for an exception like:    start the datanode(s) again, the sink never recovers and has to be undeployed and redeployed. """"""",""" xd:>stream create --name errtest --definition """"""""time | hdfs --rollover=50"""""""" --deploy   2015-03-03 10:41:57,832 1.1.0.RELEASE ERROR task-scheduler-3 handler.LoggingHandler - org.springframework.messaging.MessageHandlingException: failed to write Message payload to HDFS; nested exception is org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /xd/errtest/errtest-7.txt.tmp could only be replicated to 0 nodes instead of minReplication (=1).  There are 1 datanode(s) running and 1 node(s) are excluded in this operation.  at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1549)  at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3200)  at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:641)  at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:482)  at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)  at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)  at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:962)  at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2039)  at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2035)  at java.security.AccessController.doPrivileged(Native Method)  at javax.security.auth.Subject.doAs(Subject.java:415)  at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)  at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2033)   at org.springframework.xd.integration.hadoop.outbound.HdfsStoreMessageHandler.handleMessageInternal(HdfsStoreMessageHandler.java:129)  at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78)  at sun.reflect.GeneratedMethodAccessor86.invoke(Unknown Source)  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)  at java.lang.reflect.Method.invoke(Method.java:606)  at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)  at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)  at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)  at org.springframework.integration.monitor.SimpleMessageHandlerMetrics.handleMessage(SimpleMessageHandlerMetrics.java:107)  at org.springframework.integration.monitor.SimpleMessageHandlerMetrics.invoke(SimpleMessageHandlerMetrics.java:87)  at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)  at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)  at com.sun.proxy.$Proxy136.handleMessage(Unknown Source)  at org.springframework.integration.dispatcher.AbstractDispatcher.tryOptimizedDispatch(AbstractDispatcher.java:116)  at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:101)  at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)  at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)  at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:277)  at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:239)  at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)  at java.lang.reflect.Method.invoke(Method.java:606)  at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)  at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)  at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)  at org.springframework.integration.monitor.DirectChannelMetrics.monitorSend(DirectChannelMetrics.java:114)  at org.springframework.integration.monitor.DirectChannelMetrics.doInvoke(DirectChannelMetrics.java:98)  at org.springframework.integration.monitor.DirectChannelMetrics.invoke(DirectChannelMetrics.java:92)  at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)  at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)  at com.sun.proxy.$Proxy125.send(Unknown Source)  at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:115)  at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:45)  at org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:95)  at org.springframework.integration.handler.AbstractMessageProducingHandler.sendOutput(AbstractMessageProducingHandler.java:248)  at org.springframework.integration.handler.AbstractMessageProducingHandler.produceOutput(AbstractMessageProducingHandler.java:171)  at org.springframework.integration.handler.AbstractMessageProducingHandler.sendOutputs(AbstractMessageProducingHandler.java:119)  at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:105)  at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78)  at org.springframework.integration.dispatcher.AbstractDispatcher.tryOptimizedDispatch(AbstractDispatcher.java:116)  at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:101)  at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)  at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)  at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:277)  at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:239)  at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:115)  at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:45)  at org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:95)  at org.springframework.integration.handler.AbstractMessageProducingHandler.sendOutput(AbstractMessageProducingHandler.java:248)  at org.springframework.integration.handler.AbstractMessageProducingHandler.produceOutput(AbstractMessageProducingHandler.java:171)  at org.springframework.integration.handler.AbstractMessageProducingHandler.sendOutputs(AbstractMessageProducingHandler.java:119)  at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:105)  at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78)  at org.springframework.integration.dispatcher.AbstractDispatcher.tryOptimizedDispatch(AbstractDispatcher.java:116)  at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:101)  at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)  at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)  at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:277)  at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:239)  at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)  at java.lang.reflect.Method.invoke(Method.java:606)  at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)  at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)  at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)  at org.springframework.integration.monitor.DirectChannelMetrics.monitorSend(DirectChannelMetrics.java:114)  at org.springframework.integration.monitor.DirectChannelMetrics.doInvoke(DirectChannelMetrics.java:98)  at org.springframework.integration.monitor.DirectChannelMetrics.invoke(DirectChannelMetrics.java:92)  at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)  at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)  at com.sun.proxy.$Proxy137.send(Unknown Source)  at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:115)  at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:45)  at org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:95)  at org.springframework.integration.endpoint.SourcePollingChannelAdapter.handleMessage(SourcePollingChannelAdapter.java:130)  at org.springframework.integration.endpoint.AbstractPollingEndpoint.doPoll(AbstractPollingEndpoint.java:219)  at org.springframework.integration.endpoint.AbstractPollingEndpoint.access$000(AbstractPollingEndpoint.java:55)  at org.springframework.integration.endpoint.AbstractPollingEndpoint$1.call(AbstractPollingEndpoint.java:149)  at org.springframework.integration.endpoint.AbstractPollingEndpoint$1.call(AbstractPollingEndpoint.java:146)  at org.springframework.integration.endpoint.AbstractPollingEndpoint$Poller$1.run(AbstractPollingEndpoint.java:298)  at org.springframework.integration.util.ErrorHandlingTaskExecutor$1.run(ErrorHandlingTaskExecutor.java:52)  at org.springframework.core.task.SyncTaskExecutor.execute(SyncTaskExecutor.java:50)  at org.springframework.integration.util.ErrorHandlingTaskExecutor.execute(ErrorHandlingTaskExecutor.java:49)  at org.springframework.integration.endpoint.AbstractPollingEndpoint$Poller.run(AbstractPollingEndpoint.java:292)  at org.springframework.scheduling.support.DelegatingErrorHandlingRunnable.run(DelegatingErrorHandlingRunnable.java:54)  at org.springframework.scheduling.concurrent.ReschedulingRunnable.run(ReschedulingRunnable.java:81)  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)  at java.util.concurrent.FutureTask.run(FutureTask.java:262)  at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)  at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)  at java.lang.Thread.run(Thread.java:745) Caused by: org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /xd/errtest/errtest-7.txt.tmp could only be replicated to 0 nodes instead of minReplication (=1).  There are 1 datanode(s) running and 1 node(s) are excluded in this operation.  at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1549)  at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3200)  at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:641)  at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:482)  at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)  at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)  at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:962)  at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2039)  at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2035)  at java.security.AccessController.doPrivileged(Native Method)  at javax.security.auth.Subject.doAs(Subject.java:415)  at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)  at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2033)   at org.apache.hadoop.ipc.Client.call(Client.java:1468)  at org.apache.hadoop.ipc.Client.call(Client.java:1399)  at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)  at com.sun.proxy.$Proxy134.addBlock(Unknown Source)  at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:399)  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)  at java.lang.reflect.Method.invoke(Method.java:606)  at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)  at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)  at com.sun.proxy.$Proxy135.addBlock(Unknown Source)  at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1532)  at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1349)  at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:588) """,5.0
1,XD-2773,BUG,Done,MEDIUM,"""spring-xd-module-parent creates resources outside of target folder""","""Custom modules that use  {code} <groupId>org.springframework.xd</groupId> <artifactId>spring-xd-module-parent</artifactId> <version>1.1.0.RELEASE</version> {code}  as a parent will get a """"lib"""" directory created in the module root source directory. This forces us to add additional ignores in version control system.  Following Maven convention all build files should be created under """"target"""" folder so """"lib"""" folder should be created as """"target/lib"""".""","""""""Custom modules that use    as a parent will get a """"""""lib"""""""" directory created in the module root source directory. This forces us to add additional ignores in version control system.  Following Maven convention all build files should be created under """"""""target"""""""" folder so """"""""lib"""""""" folder should be created as """"""""target/lib"""""""".""""""",""" <groupId>org.springframework.xd</groupId> <artifactId>spring-xd-module-parent</artifactId> <version>1.1.0.RELEASE</version> """,1.0
1,XD-2772,IMPROVEMENT,Done,HIGH,"""No validation for module packaging during module upload""","""One can upload a module which doesn't contain """"config"""" package with module configuration artifacts (e.g. XML, groovy, properties etc.) using the """"module upload"""" command in Spring XD shell. During stream deployment this will result in a cryptic exception:  {code}Mar 02, 2015 10:45:48 PM org.springframework.shell.core.SimpleExecutionStrategy invoke SEVERE: Command failed org.springframework.xd.rest.client.impl.SpringXDException: Multiple top level module resources found :file [/opt/spring-xd/spring-xd-1.1.0.RELEASE/xd/config/jms-activemq.properties],file [/opt/spring-xd/spring-xd-1.1.0.RELEASE/xd/config/xd-container-logger.properties],file [/opt/spring-xd/spring-xd-1.1.0.RELEASE/xd/config/jms-hornetq.properties],file [/opt/spring-xd/spring-xd-1.1.0.RELEASE/xd/config/xd-singlenode-logger.properties],file [/opt/spring-xd/spring-xd-1.1.0.RELEASE/xd/config/xd-admin-logger.properties],file [/opt/spring-xd/spring-xd-1.1.0.RELEASE/xd/config/httpSSL.properties],file [/opt/spring-xd/spring-xd-1.1.0.RELEASE/xd/config/hadoop.properties]{code}  Root cause is most likely the module ClassLoader setup which delegates to parent ClassLoader which in turn scans $XD_HOME/config directory and discovers a number of properties files.  It would nice if Spring XD would validate the module during """"module upload"""" command and prevent uploading of invalid modules.""","""""""One can upload a module which doesn't contain """"""""config"""""""" package with module configuration artifacts (e.g. XML, groovy, properties etc.) using the """"""""module upload"""""""" command in Spring XD shell. During stream deployment this will result in a cryptic exception:    Root cause is most likely the module ClassLoader setup which delegates to parent ClassLoader which in turn scans $XD_HOME/config directory and discovers a number of properties files.  It would nice if Spring XD would validate the module during """"""""module upload"""""""" command and prevent uploading of invalid modules.""""""","""Mar 02, 2015 10:45:48 PM org.springframework.shell.core.SimpleExecutionStrategy invoke SEVERE: Command failed org.springframework.xd.rest.client.impl.SpringXDException: Multiple top level module resources found :file [/opt/spring-xd/spring-xd-1.1.0.RELEASE/xd/config/jms-activemq.properties],file [/opt/spring-xd/spring-xd-1.1.0.RELEASE/xd/config/xd-container-logger.properties],file [/opt/spring-xd/spring-xd-1.1.0.RELEASE/xd/config/jms-hornetq.properties],file [/opt/spring-xd/spring-xd-1.1.0.RELEASE/xd/config/xd-singlenode-logger.properties],file [/opt/spring-xd/spring-xd-1.1.0.RELEASE/xd/config/xd-admin-logger.properties],file [/opt/spring-xd/spring-xd-1.1.0.RELEASE/xd/config/httpSSL.properties],file [/opt/spring-xd/spring-xd-1.1.0.RELEASE/xd/config/hadoop.properties]""",2.0
1,XD-2771,BUG,Done,HIGH,"""spring-xd-dirt is not providing all $XD_HOME/lib libraries""","""Spring XD is packaging a spring-xd-dirt dependency which aims to provide runtime libraries.  spring-xd-drit-1.1.0.RELEASE is not providing all libraries from $XD_HOME/lib. See spring-xd-dirt-vs-lib.xlsx attachment generated with """"ls  $XD_HOME/lib"""" and """"mvn dependency:list"""" on a module with spring-xd-module-parent parent: - 100+ JARs are not provided - some are provided in different versions - some are provided but not available in $XD_HOME/lib  This forces us to add and maintain missing dependencies in our own module parent e.g. to use commons-lang3 in our code which is present in $XD_HOME/lib but is not provided by spring-xd-dirt.  Why there are so many differences between $XD_HOME/lib and spring-xd-dirt?""","""""""Spring XD is packaging a spring-xd-dirt dependency which aims to provide runtime libraries.  spring-xd-drit-1.1.0.RELEASE is not providing all libraries from $XD_HOME/lib. See spring-xd-dirt-vs-lib.xlsx attachment generated with """"""""ls  $XD_HOME/lib"""""""" and """"""""mvn dependency:list"""""""" on a module with spring-xd-module-parent parent: - 100+ JARs are not provided - some are provided in different versions - some are provided but not available in $XD_HOME/lib  This forces us to add and maintain missing dependencies in our own module parent e.g. to use commons-lang3 in our code which is present in $XD_HOME/lib but is not provided by spring-xd-dirt.  Why there are so many differences between $XD_HOME/lib and spring-xd-dirt?""""""",,2.0
1,XD-2769,BUG,Done,HIGH,"""Inconsistent API in AbstractSingleNodeNamedChannelSink ""","""In [AbstractSingleNodeNamedChannelSink|https://github.com/spring-projects/spring-xd/blob/6bd17162c8a6da0f09f6f8809f694a060c71ecc0/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/test/sink/AbstractSingleNodeNamedChannelSink.java] the receive() and receivePayload() methods  are non-blocking.  Methods without timeout parameter are usually blocking and return the first message delivered to the channel (e.g. org.springframework.integration.channel.AbstractPollableChannel#receive()).   Integration tests based on spring-xd-test dependency and embedded xd-singlenode are asynchronous. This makes AbstractSingleNodeNamedChannelSink receive method return null in all invocations because test thread is progressing faster than container can process the message in the background.  Would it be possible to make receive methods behave like in AbstractPollableChannel?""","""""""In [AbstractSingleNodeNamedChannelSink|https://github.com/spring-projects/spring-xd/blob/6bd17162c8a6da0f09f6f8809f694a060c71ecc0/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/test/sink/AbstractSingleNodeNamedChannelSink.java] the receive() and receivePayload() methods  are non-blocking.  Methods without timeout parameter are usually blocking and return the first message delivered to the channel (e.g. org.springframework.integration.channel.AbstractPollableChannel#receive()).   Integration tests based on spring-xd-test dependency and embedded xd-singlenode are asynchronous. This makes AbstractSingleNodeNamedChannelSink receive method return null in all invocations because test thread is progressing faster than container can process the message in the background.  Would it be possible to make receive methods behave like in AbstractPollableChannel?""""""",,1.0
1,XD-2768,FEATURE,Done,MEDIUM,"""Inconsistent Handling of Inherited servers.yml Properties""","""Some modules inherit {{application.yml}} / {{servers.yml}} via a properties file in {{/config/modules}} ; others have the values defined in the {{...OptionsMetadata}} classes.  Switch all modules to use the latter technique for consistency.""","""""""Some modules inherit {{application.yml}} / {{servers.yml}} via a properties file in {{/config/modules}} ; others have the values defined in the {{...OptionsMetadata}} classes.  Switch all modules to use the latter technique for consistency.""""""",,3.0
1,XD-2767,BUG,Done,MEDIUM,"""JMS Source Does Not Expose `acknowledge`""","""Since the message-driven adapter uses a {{DMLC}}, the default behavior is to lose messages on exceptions (with the DMLC, the message is ack'd before the listener is invoked).  In order to provide recovery of such situations, the source needs to expose {{acknowledge}} so it can be set to {{transacted}}.  Or, perhaps, given that we don't expose complex configuration, the source should use a {{SimpleMessageListenerContainer}} instead (where the ack is sent after the listener is successfully invoked). ""","""""""Since the message-driven adapter uses a {{DMLC}}, the default behavior is to lose messages on exceptions (with the DMLC, the message is ack'd before the listener is invoked).  In order to provide recovery of such situations, the source needs to expose {{acknowledge}} so it can be set to {{transacted}}.  Or, perhaps, given that we don't expose complex configuration, the source should use a {{SimpleMessageListenerContainer}} instead (where the ack is sent after the listener is successfully invoked). """"""",,1.0
1,XD-2766,FEATURE,Done,MEDIUM,"""Add metadata for description of a module (itself)""","""As a user, I'd like to have the description for each of the modules so that I can use it to understand the module purpose and it's capabilities (presumably what is captured in javadoc for the module definition).""","""""""As a user, I'd like to have the description for each of the modules so that I can use it to understand the module purpose and it's capabilities (presumably what is captured in javadoc for the module definition).""""""",,8.0
1,XD-2765,FEATURE,Done,HIGH,"""Spike: Research Zookeeper-based mechanism for partitioned job management""","""The current implementation of partitioned job management is entirely based on message exchange over the message bus, in a request reply scenario. This creates challenges when it comes to using certain types of transports, as well as acknowledging crashes.  To that effect, the option of using a different partitioned job coordination strategy, that relies on a distributed computing coordination mechanism such as ZooKeeper should be investigated. ""","""""""The current implementation of partitioned job management is entirely based on message exchange over the message bus, in a request reply scenario. This creates challenges when it comes to using certain types of transports, as well as acknowledging crashes.  To that effect, the option of using a different partitioned job coordination strategy, that relies on a distributed computing coordination mechanism such as ZooKeeper should be investigated. """"""",,8.0
1,XD-2764,BUG,Done,MEDIUM,"""Fix failing KafkaSingleNodeStreamDeploymentIntegrationTests.verifyOnDemandQueues()""","""Test fails in CI when the topic used by the test had its initial segment removed during cleanup.""","""""""Test fails in CI when the topic used by the test had its initial segment removed during cleanup.""""""",,1.0
1,XD-2763,MAINTENANCE,Done,MEDIUM,"""Create RabbitMQ environment and record baseline results.""","""# Setup Environment  ## Create 1.1.0 XD cluster with 1 admin and 3 containers ## Create 3 node RabbitMQ Cluster ## Associate Each XD Container to a single Rabbit Node. ## All EC2 instances in the same placement group # Execute the following streams {noformat} stream create q1 --definition """"load-generator --messageCount=2000000 > queue:q1""""  stream deploy q1 --properties """"module.load-generator.criteria=groups.contains('one')"""" stream create q2 --definition """"load-generator --messageCount=4000000 > queue:q2""""  stream deploy q2 --properties """"module.load-generator.criteria=groups.contains('two')"""" stream create q3 --definition """"load-generator --messageCount=4000000 > queue:q3""""  stream deploy q3 --properties """"module.load-generator.criteria=groups.contains('three')"""" stream create q4 --definition """"load-generator --messageCount=4000000 > queue:q4""""  stream deploy q4 --properties """"module.load-generator.criteria=groups.contains('one')"""" stream create q5 --definition """"load-generator --messageCount=4000000 > queue:q5""""  stream deploy q5 --properties """"module.load-generator.criteria=groups.contains('two')"""" stream create q6 --definition """"load-generator --messageCount=4000000 > queue:q6""""  stream deploy q6 --properties """"module.load-generator.criteria=groups.contains('three')"""" {noformat} # use the following rabbitmq perf tests to retrieve benchmark rates {noformat} ./runjava.sh com.rabbitmq.examples.PerfTest -u xdbus.queue:q1 -p -x 0 -y 2 -q 500 -z 120 -h amqp://xx1.compute-1.amazonaws.com >q1.txt & ./runjava.sh com.rabbitmq.examples.PerfTest -u xdbus.queue:q2 -p -x 0 -y 1 -q 500 -z 120 -h amqp://xx2.compute-1.amazonaws.com >q2.txt & ./runjava.sh com.rabbitmq.examples.PerfTest -u xdbus.queue:q3 -p -x 0 -y 1 -q 500 -z 120 -h amqp://xx3.compute-1.amazonaws.com >q3.txt & ./runjava.sh com.rabbitmq.examples.PerfTest -u xdbus.queue:q4 -p -x 0 -y 1 -q 500 -z 120 -h amqp://xx1.compute-1.amazonaws.com >q4.txt & ./runjava.sh com.rabbitmq.examples.PerfTest -u xdbus.queue:q5 -p -x 0 -y 1 -q 500 -z 120 -h amqp://xx2.compute-1.amazonaws.com >q5.txt & ./runjava.sh com.rabbitmq.examples.PerfTest -u xdbus.queue:q6 -p -x 0 -y 1 -q 500 -z 120 -h amqp://xx3.compute-1.amazonaws.com >q6.txt & {noformat} # Reach out to SME's to identify proper configuration""","""""""# Setup Environment  ## Create 1.1.0 XD cluster with 1 admin and 3 containers ## Create 3 node RabbitMQ Cluster ## Associate Each XD Container to a single Rabbit Node. ## All EC2 instances in the same placement group # Execute the following streams  # use the following rabbitmq perf tests to retrieve benchmark rates  # Reach out to SME's to identify proper configuration""""""",""" stream create q1 --definition """"""""load-generator --messageCount=2000000 > queue:q1""""""""  stream deploy q1 --properties """"""""module.load-generator.criteria=groups.contains('one')"""""""" stream create q2 --definition """"""""load-generator --messageCount=4000000 > queue:q2""""""""  stream deploy q2 --properties """"""""module.load-generator.criteria=groups.contains('two')"""""""" stream create q3 --definition """"""""load-generator --messageCount=4000000 > queue:q3""""""""  stream deploy q3 --properties """"""""module.load-generator.criteria=groups.contains('three')"""""""" stream create q4 --definition """"""""load-generator --messageCount=4000000 > queue:q4""""""""  stream deploy q4 --properties """"""""module.load-generator.criteria=groups.contains('one')"""""""" stream create q5 --definition """"""""load-generator --messageCount=4000000 > queue:q5""""""""  stream deploy q5 --properties """"""""module.load-generator.criteria=groups.contains('two')"""""""" stream create q6 --definition """"""""load-generator --messageCount=4000000 > queue:q6""""""""  stream deploy q6 --properties """"""""module.load-generator.criteria=groups.contains('three')""""""""  ./runjava.sh com.rabbitmq.examples.PerfTest -u xdbus.queue:q1 -p -x 0 -y 2 -q 500 -z 120 -h amqp://xx1.compute-1.amazonaws.com >q1.txt & ./runjava.sh com.rabbitmq.examples.PerfTest -u xdbus.queue:q2 -p -x 0 -y 1 -q 500 -z 120 -h amqp://xx2.compute-1.amazonaws.com >q2.txt & ./runjava.sh com.rabbitmq.examples.PerfTest -u xdbus.queue:q3 -p -x 0 -y 1 -q 500 -z 120 -h amqp://xx3.compute-1.amazonaws.com >q3.txt & ./runjava.sh com.rabbitmq.examples.PerfTest -u xdbus.queue:q4 -p -x 0 -y 1 -q 500 -z 120 -h amqp://xx1.compute-1.amazonaws.com >q4.txt & ./runjava.sh com.rabbitmq.examples.PerfTest -u xdbus.queue:q5 -p -x 0 -y 1 -q 500 -z 120 -h amqp://xx2.compute-1.amazonaws.com >q5.txt & ./runjava.sh com.rabbitmq.examples.PerfTest -u xdbus.queue:q6 -p -x 0 -y 1 -q 500 -z 120 -h amqp://xx3.compute-1.amazonaws.com >q6.txt & """,3.0
1,XD-2762,FEATURE,Done,MEDIUM,"""Update RHEL/CentOS yum/rpm installation instructions""","""As a build manager, I'd like to have Spring XD RPMs published in spring.io repository so that users can directly download the bits without having to go through appsuite repo or the EULA.   *Location for 1.1.0 RELEASE:* http://repo.spring.io/libs-release-local/org/springframework/xd/spring-xd/1.1.0.RELEASE/""","""""""As a build manager, I'd like to have Spring XD RPMs published in spring.io repository so that users can directly download the bits without having to go through appsuite repo or the EULA.   *Location for 1.1.0 RELEASE:* http://repo.spring.io/libs-release-local/org/springframework/xd/spring-xd/1.1.0.RELEASE/""""""",,2.0
1,XD-2761,FEATURE,Done,URGENT,"""Register only known classes with Kryo in PojoCodec""","""Currently PojoCodec calls kryo.register(Class<?> type) on every ser/deser invocation. This fails with 1.1 because instances are pooled and a different instance may be used to serialize and deserialize.  See https://github.com/EsotericSoftware/kryo#registration.  The fix is to not register classes on the fly. Classes serialized by PojoCodec will not be registered by default. This will work but is less efficient. XD should provide an easy way to register types known to be serialized on the MessageBus (passed between modules)""","""""""Currently PojoCodec calls kryo.register(Class<?> type) on every ser/deser invocation. This fails with 1.1 because instances are pooled and a different instance may be used to serialize and deserialize.  See https://github.com/EsotericSoftware/kryo#registration.  The fix is to not register classes on the fly. Classes serialized by PojoCodec will not be registered by default. This will work but is less efficient. XD should provide an easy way to register types known to be serialized on the MessageBus (passed between modules)""""""",,5.0
1,XD-2759,BUG,Done,MEDIUM,"""LocalMessageBus PubSub Needs a Bounded Task Exectutor""","""Since 1.1, {{PubSub}} channels in the {{LocalMessageBus}} run on a {{CachedThreadPoolExecutor}}.  For high volume environments, where back-pressure might occur on a {{topic:}} thread we could overwhelm the system with threads.  Add a local bus configuration to limit the thread pool used for PubSubs and queue tasks where there are no threads available.  It would be a bus-wide setting.""","""""""Since 1.1, {{PubSub}} channels in the {{LocalMessageBus}} run on a {{CachedThreadPoolExecutor}}.  For high volume environments, where back-pressure might occur on a {{topic:}} thread we could overwhelm the system with threads.  Add a local bus configuration to limit the thread pool used for PubSubs and queue tasks where there are no threads available.  It would be a bus-wide setting.""""""",,1.0
1,XD-2758,MAINTENANCE,Done,MEDIUM,"""Add module description to the JSON response""","""As  a user, I'd like to have the description for each of the modules so that I can use it to understand the module purpose and it's capabilities (presumably what is captured in javadoc for the module definition). ""","""""""As  a user, I'd like to have the description for each of the modules so that I can use it to understand the module purpose and it's capabilities (presumably what is captured in javadoc for the module definition). """"""",,3.0
1,XD-2757,FEATURE,Done,MEDIUM,"""Return description for each module in the JSON response""","""As a developer, I'd like to have the high-level description for each of the modules so that I can use the description (presumably what is captured in javadoc for the module definition) to understand the purpose of the module itself. ""","""""""As a developer, I'd like to have the high-level description for each of the modules so that I can use the description (presumably what is captured in javadoc for the module definition) to understand the purpose of the module itself. """"""",,3.0
1,XD-2756,BUG,Done,MEDIUM,"""Large number of required options in jdbc sink definition""","""I'd like to use the following command to define a stream in our Spring XD for PivotalCF test environment:  {code} stream create --name test --definition """"http --port=9999 | jdbc --username=spring-xd --password=spring-xd --driverClassName=org.postgresql.Driver --url=jdbc:postgresql://1.2.3.4:5432/spring-xd"""" {code}  I have to add the following options to the definition to make it work (otherwise I get exceptions and a failed deploy):  {code} -validationQuery='' --validatorClassName='#{null}' --connectionProperties='' --initSQL='' --jdbcInterceptors='' --initializerScript=''  {code}  Given that they're empty anyway it seems like some or all of these should not be necessary.  _Notes_ * The validatorClassName cannot be '' like the others, it needs the null. * Without initSQL='' stream creation fails because it can't find init_db.sql (a file I don't have in my environment), even though it won't be run anyway.""","""""""I'd like to use the following command to define a stream in our Spring XD for PivotalCF test environment:    I have to add the following options to the definition to make it work (otherwise I get exceptions and a failed deploy):    Given that they're empty anyway it seems like some or all of these should not be necessary.  _Notes_ * The validatorClassName cannot be '' like the others, it needs the null. * Without initSQL='' stream creation fails because it can't find init_db.sql (a file I don't have in my environment), even though it won't be run anyway.""""""",""" stream create --name test --definition """"""""http --port=9999 | jdbc --username=spring-xd --password=spring-xd --driverClassName=org.postgresql.Driver --url=jdbc:postgresql://1.2.3.4:5432/spring-xd""""""""  -validationQuery='' --validatorClassName='#{null}' --connectionProperties='' --initSQL='' --jdbcInterceptors='' --initializerScript=''  """,5.0
1,XD-2755,BUG,Done,HIGH,"""Scala processor module executor trims messages""","""How to reproduce:  1. Run xd-singlenode (for which setting the Spark master URL to 'local' is a requirement). Use more than 1 worker thread. e.g. {{local[4]}}  2. Deploy the word-count example  3. Create a stream {{stream create spark-streaming-word-count --definition """"http | word-count | log"""" --deploy}}  4. Send data {{xd:>http post --data """"a b c d e f g""""}}  {{xd:>http post --data """"a b c""""}}  5.Observe the result  2015-02-24 15:12:46,018 1.2.0.SNAP  INFO Executor task launch worker-3 sink.spark-streaming-word-count - (e,1) 2015-02-24 15:12:46,018 1.2.0.SNAP  INFO Executor task launch worker-1 sink.spark-streaming-word-count - (d,1) 2015-02-24 15:12:46,019 1.2.0.SNAP  INFO Executor task launch worker-2 sink.spark-streaming-word-count - (b,1) 2015-02-24 15:12:46,020 1.2.0.SNAP  INFO Executor task launch worker-1 sink.spark-streaming-word-count - (g,1) 2015-02-24 15:13:40,020 1.2.0.SNAP  INFO Executor task launch worker-1 sink.spark-streaming-word-count - (a,1) 2015-02-24 15:13:40,020 1.2.0.SNAP  INFO Executor task launch worker-2 sink.spark-streaming-word-count - (b,1) 2015-02-24 15:13:40,021 1.2.0.SNAP  INFO Executor task launch worker-3 sink.spark-streaming-word-count - (c,1)  (the last three results are coming from the second invocation))  Note: there seems to be a correlation between the number of values emitted and the number of workers, as, in all the attempts, there aren't more values emitted than the number of workers.""","""""""How to reproduce:  1. Run xd-singlenode (for which setting the Spark master URL to 'local' is a requirement). Use more than 1 worker thread. e.g. {{local[4]}}  2. Deploy the word-count example  3. Create a stream {{stream create spark-streaming-word-count --definition """"""""http | word-count | log"""""""" --deploy}}  4. Send data {{xd:>http post --data """"""""a b c d e f g""""""""}}  {{xd:>http post --data """"""""a b c""""""""}}  5.Observe the result  2015-02-24 15:12:46,018 1.2.0.SNAP  INFO Executor task launch worker-3 sink.spark-streaming-word-count - (e,1) 2015-02-24 15:12:46,018 1.2.0.SNAP  INFO Executor task launch worker-1 sink.spark-streaming-word-count - (d,1) 2015-02-24 15:12:46,019 1.2.0.SNAP  INFO Executor task launch worker-2 sink.spark-streaming-word-count - (b,1) 2015-02-24 15:12:46,020 1.2.0.SNAP  INFO Executor task launch worker-1 sink.spark-streaming-word-count - (g,1) 2015-02-24 15:13:40,020 1.2.0.SNAP  INFO Executor task launch worker-1 sink.spark-streaming-word-count - (a,1) 2015-02-24 15:13:40,020 1.2.0.SNAP  INFO Executor task launch worker-2 sink.spark-streaming-word-count - (b,1) 2015-02-24 15:13:40,021 1.2.0.SNAP  INFO Executor task launch worker-3 sink.spark-streaming-word-count - (c,1)  (the last three results are coming from the second invocation))  Note: there seems to be a correlation between the number of values emitted and the number of workers, as, in all the attempts, there aren't more values emitted than the number of workers.""""""",,5.0
1,XD-2752,BUG,Done,HIGH,"""SqoopTasklet not using hadoop configuration""","""Hey Guys,  I'm trying to use a SqoopTasklet but for some reason it is not getting the hadoop configuration. In the attached sqoop job configuration using the sqooprunner class directly works without problems but the SqoopTasklet is not getting the correct configuration throwing kerberos authentication problems (see singlenode.log).  Could please you guys help me to solve this problem?  Thanks in advance. Regards,""","""""""Hey Guys,  I'm trying to use a SqoopTasklet but for some reason it is not getting the hadoop configuration. In the attached sqoop job configuration using the sqooprunner class directly works without problems but the SqoopTasklet is not getting the correct configuration throwing kerberos authentication problems (see singlenode.log).  Could please you guys help me to solve this problem?  Thanks in advance. Regards,""""""",,8.0
1,XD-2751,BUG,Done,MEDIUM,"""JDBC | FILE throws ConverterNotFoundException when split=0""","""I am trying to create a simple JDBC|FILE stream with Split=0 at the jdbc source. following is the DSL  stream create --name test --definition """"jdbc --fixedDelay=5 --split=0 --query='select * from top_movie_companies'|file --dir=/tmp --suffix=xd --name=test"""" --deploy  It throws  org.springframework.core.convert.ConverterNotFoundException: No converter found capable of converting from type java.util.ArrayList<?> to type java.lang.String  at org.springframework.integration.util.AbstractExpressionEvaluator.evaluateExpression(AbstractExpressionEvaluator.java:138)  It works fine when I use LOG sink instead of FILE.   I am assuming that if LOG sink works with JDBC then file should be similar. The converter should be registered out of the box.  It could be something basic I am missing as I'm relatively new to XD.""","""""""I am trying to create a simple JDBC|FILE stream with Split=0 at the jdbc source. following is the DSL  stream create --name test --definition """"""""jdbc --fixedDelay=5 --split=0 --query='select * from top_movie_companies'|file --dir=/tmp --suffix=xd --name=test"""""""" --deploy  It throws  org.springframework.core.convert.ConverterNotFoundException: No converter found capable of converting from type java.util.ArrayList<?> to type java.lang.String  at org.springframework.integration.util.AbstractExpressionEvaluator.evaluateExpression(AbstractExpressionEvaluator.java:138)  It works fine when I use LOG sink instead of FILE.   I am assuming that if LOG sink works with JDBC then file should be similar. The converter should be registered out of the box.  It could be something basic I am missing as I'm relatively new to XD.""""""",,1.0
1,XD-2750,FEATURE,Done,MEDIUM,"""Placeholder for Lattice/Diego POC #2""","""Placeholder for Lattice/Diego POC #2""","""Placeholder for Lattice/Diego POC #2""",,8.0
1,XD-2749,FEATURE,Done,MEDIUM,"""Placeholder for Lattice/Diego POC #1""","""Placeholder for Lattice/Diego POC #1""","""Placeholder for Lattice/Diego POC #1""",,8.0
1,XD-2748,FEATURE,Done,MEDIUM,"""Implement Reliable spark streaming receiver ""","""The spark streaming message bus receiver isn't reliable yet. The receiver needs to handle data loss in case of worker node that has it running.  We currently handle the driver failure automatically by re-deploying spark streaming module. But, this is about the data loss when the worker node dies.  Please see the documents here:  https://databricks.com/blog/2015/01/15/improved-driver-fault-tolerance-and-zero-data-loss-in-spark-streaming.html  http://spark.apache.org/docs/latest/streaming-custom-receivers.html""","""""""The spark streaming message bus receiver isn't reliable yet. The receiver needs to handle data loss in case of worker node that has it running.  We currently handle the driver failure automatically by re-deploying spark streaming module. But, this is about the data loss when the worker node dies.  Please see the documents here:  https://databricks.com/blog/2015/01/15/improved-driver-fault-tolerance-and-zero-data-loss-in-spark-streaming.html  http://spark.apache.org/docs/latest/streaming-custom-receivers.html""""""",,3.0
1,XD-2747,FEATURE,Done,MEDIUM,"""Document test scenarios for performance testing""","""As a developer, I'd like to benchmark Rabbit performance so that I can use the results as reference to setup XD cluster.""","""""""As a developer, I'd like to benchmark Rabbit performance so that I can use the results as reference to setup XD cluster.""""""",,8.0
1,XD-2744,FEATURE,Done,MEDIUM,"""Placeholder for Spring XD Lab""","""Placeholder for Spring XD Lab""","""Placeholder for Spring XD Lab""",,8.0
1,XD-2743,FEATURE,Done,MEDIUM,"""Improve acceptance testing coverage""","""The scope is to address the sub-tasks linked with this story. ""","""""""The scope is to address the sub-tasks linked with this story. """"""",,5.0
1,XD-2742,FEATURE,Done,MEDIUM,"""Prep for DEBS Challenge""","""As a developer, I'd like to study the taxi trips based on a stream of trip reports from New York City so that I can evaluate event-based systems in the context of real-time analytics using Spring XD.  [Challenge Details|http://www.debs2015.org/call-grand-challenge.html]""","""""""As a developer, I'd like to study the taxi trips based on a stream of trip reports from New York City so that I can evaluate event-based systems in the context of real-time analytics using Spring XD.  [Challenge Details|http://www.debs2015.org/call-grand-challenge.html]""""""",,8.0
1,XD-2741,FEATURE,Done,MEDIUM,"""Redis sink should default to using spring.redis configuration in servers.yml""","""The configuration for the redis sink must be provided for explicitly instead of falling back to values defined in servers.yml.  The default behavior configuration should be address in a manner consistent with the default behavior of module config for rabbit source/sink, jdbc sink....""","""""""The configuration for the redis sink must be provided for explicitly instead of falling back to values defined in servers.yml.  The default behavior configuration should be address in a manner consistent with the default behavior of module config for rabbit source/sink, jdbc sink....""""""",,1.0
1,XD-2740,BUG,Done,HIGH,"""HDFS StoreMessageHandlers are not very resilient""","""There are cases where hdfs-outbound-channel-adapter (from spring-xd-hadoop module) will keep a broken datawriter's stream open.  For example if the underlying HDFS stream throws a RemoteException (ie. some hadoop error occurred) there's no mitigation. Subsequent writes will always fail.  The MessageHandler could potentially try to close and re-open the stream or trigger a rollover strategy to occur (creating a new stream).""","""""""There are cases where hdfs-outbound-channel-adapter (from spring-xd-hadoop module) will keep a broken datawriter's stream open.  For example if the underlying HDFS stream throws a RemoteException (ie. some hadoop error occurred) there's no mitigation. Subsequent writes will always fail.  The MessageHandler could potentially try to close and re-open the stream or trigger a rollover strategy to occur (creating a new stream).""""""",,10.0
1,XD-2739,FEATURE,Done,MEDIUM,"""Add a Sqoop example""","""Add a Sqoop example""","""Add a Sqoop example""",,3.0
1,XD-2738,FEATURE,Done,MEDIUM,"""Add arbitrary """"side channels"""" to track module progress""","""As a user, I'd like to have an optional  arbitrary """"side channels"""" created so that when creating a module channels other than the primary stream channels (input, output) could be added to the bus (i.e. creating a tap channel *within* a flow). The optional """"side channels"""" can be used to trace/track module progress.""","""""""As a user, I'd like to have an optional  arbitrary """"""""side channels"""""""" created so that when creating a module channels other than the primary stream channels (input, output) could be added to the bus (i.e. creating a tap channel *within* a flow). The optional """"""""side channels"""""""" can be used to trace/track module progress.""""""",,8.0
1,XD-2737,FEATURE,Done,MEDIUM,"""Add support for global wiretap config""","""As a user, I'd like to have an optional _trace_ as inline deployment properties for _stream_ so that I can declare which _module_ in the stream needs to be traced for logging or notifications. This gives the flexibility to track the stage progress between individual modules.  *Example:*  {code:xml} xd:> stream create foo """"http | log""""  xd:> stream deploy foo --properties """"module.http.trace,module.log.trace""""  (or)  xd:> stream deploy foo --properties """"module.*.trace"""" {code}  Wildcard wiretap config: http://docs.spring.io/spring-integration/reference/html/messaging-channels-section.html#channel-global-wiretap""","""""""As a user, I'd like to have an optional _trace_ as inline deployment properties for _stream_ so that I can declare which _module_ in the stream needs to be traced for logging or notifications. This gives the flexibility to track the stage progress between individual modules.  *Example:*    Wildcard wiretap config: http://docs.spring.io/spring-integration/reference/html/messaging-channels-section.html#channel-global-wiretap""""""",""" xd:> stream create foo """"""""http | log""""""""  xd:> stream deploy foo --properties """"""""module.http.trace,module.log.trace""""""""  (or)  xd:> stream deploy foo --properties """"""""module.*.trace"""""""" """,8.0
1,XD-2735,IMPROVEMENT,Done,MEDIUM,"""Make local message bus properties configurable""","""Currently, the local message bus has couple of properties """"queueSize"""" and """"polling"""". But these properties can not be configurable via servers.yml. Also, the property prefix needs to align with other message bus properties.""","""""""Currently, the local message bus has couple of properties """"""""queueSize"""""""" and """"""""polling"""""""". But these properties can not be configurable via servers.yml. Also, the property prefix needs to align with other message bus properties.""""""",,1.0
1,XD-2734,FEATURE,Done,MEDIUM,"""Create first-cut on reference architectures for domain specific use-cases""","""As a field engineer, I'd like to have reference architectures built on Spring XD so that I can use that as reference building POCs. The scope is to get the raw domain specific ideas captured as first step. ""","""""""As a field engineer, I'd like to have reference architectures built on Spring XD so that I can use that as reference building POCs. The scope is to get the raw domain specific ideas captured as first step. """"""",,8.0
1,XD-2733,BUG,Done,MEDIUM,"""Custom Modules can't be found wen using xd.customModule.home on windows ""","""XD can not find the custom modules directory after Setting the xd.customModule.home in the windows environment   Deployment * xd-singlenode (embedded zookeeper) * Java 8 * Windows 8 or Windows Server 2012 r2  Steps to reproduce:  1) Start xd-singlenode 2) Start Shell 3) Build either the payload-conversion or rss-feed-source from the spring-xd-samples 4) use the shell to execute a module upload for the custom module (rss-feed-source, payload-conversion) 5) verify it uploaded xd:>module info processor:myTupleProcessor 6) stop xd single node 7) From the command line execute set xd.customModule.home=[path to your custom modules] i.e. C:\project\spring-xd-1.1.0.RELEASE\xd\custom-modules 8) restart xd-singlenode 9) execute module info processor:myTupleProcessor 10) you will get the following error {noformat} Command failed org.springframework.xd.rest.client.impl.SpringXDException: Could not find module with name 'myTupleProcessor' and type 'processor' {noformat}""","""""""XD can not find the custom modules directory after Setting the xd.customModule.home in the windows environment   Deployment * xd-singlenode (embedded zookeeper) * Java 8 * Windows 8 or Windows Server 2012 r2  Steps to reproduce:  1) Start xd-singlenode 2) Start Shell 3) Build either the payload-conversion or rss-feed-source from the spring-xd-samples 4) use the shell to execute a module upload for the custom module (rss-feed-source, payload-conversion) 5) verify it uploaded xd:>module info processor:myTupleProcessor 6) stop xd single node 7) From the command line execute set xd.customModule.home=[path to your custom modules] i.e. C:\project\spring-xd-1.1.0.RELEASE\xd\custom-modules 8) restart xd-singlenode 9) execute module info processor:myTupleProcessor 10) you will get the following error """"""",""" Command failed org.springframework.xd.rest.client.impl.SpringXDException: Could not find module with name 'myTupleProcessor' and type 'processor' """,3.0
1,XD-2732,FEATURE,Done,MEDIUM,"""Move $XD_HOME/modules/processor/scripts out of the way""","""Since the refactoring of the module registry that does not """"look inside"""" a module, it can't know that the scripts directory is not a module.  Everything that is a direct child of source, processor, sink, job should be a module archive. Everything else supporting that should be moved out, e.g. in modules/common""","""""""Since the refactoring of the module registry that does not """"""""look inside"""""""" a module, it can't know that the scripts directory is not a module.  Everything that is a direct child of source, processor, sink, job should be a module archive. Everything else supporting that should be moved out, e.g. in modules/common""""""",,2.0
1,XD-2731,BUG,Done,URGENT,"""Temp files for stream create not being cleaned""","""During testing for Spring XD for PivotalCF we create, deploy, use, undeploy and destroy many streams. Each stream generates {{tmp}} directories (I think 2, one for source, one for sink) in the xd-admin VM's {{/tmp}} directory, e.g.  {noformat} dummy-module4635787551932601017sinkredis dummy-module252960009195893204sourcehttp {noformat}  These {{tmp}} directories are not being cleared up, so our system has hit the inode limit of 32768 files for a volume:  {noformat} Filesystem     Inodes IUsed  IFree IUse% Mounted on /dev/loop0      32768 32768      0  100% /tmp {noformat}  This causes a Java {{IOException}}, the immediately relevant part of which appears to be:  {noformat} [Caught] exception while handling a request Feb 18 09:06:06 10.85.30.142-2 xd-admin-partition-default_az_guid-0:  [java.lang.RuntimeException] java.io.IOException: No space left on device Feb 18 09:06:06 10.85.30.142-2 xd-admin-partition-default_az_guid-0:  []    at org.springframework.xd.module.ModuleDefinitions.dummy(ModuleDefinitions.java:81) {noformat}  This causes the test system to fail entirely.""","""""""During testing for Spring XD for PivotalCF we create, deploy, use, undeploy and destroy many streams. Each stream generates {{tmp}} directories (I think 2, one for source, one for sink) in the xd-admin VM's {{/tmp}} directory, e.g.    These {{tmp}} directories are not being cleared up, so our system has hit the inode limit of 32768 files for a volume:    This causes a Java {{IOException}}, the immediately relevant part of which appears to be:    This causes the test system to fail entirely.""""""",""" dummy-module4635787551932601017sinkredis dummy-module252960009195893204sourcehttp  Filesystem     Inodes IUsed  IFree IUse% Mounted on /dev/loop0      32768 32768      0  100% /tmp  [Caught] exception while handling a request Feb 18 09:06:06 10.85.30.142-2 xd-admin-partition-default_az_guid-0:  [java.lang.RuntimeException] java.io.IOException: No space left on device Feb 18 09:06:06 10.85.30.142-2 xd-admin-partition-default_az_guid-0:  []    at org.springframework.xd.module.ModuleDefinitions.dummy(ModuleDefinitions.java:81) """,3.0
1,XD-2730,FEATURE,Done,MEDIUM,"""Add support to include deployment manifest from file""","""As a user, I'd like to include the deployment manifest from the file so that I don't have spend time typing as """"inline properties"""".""","""""""As a user, I'd like to include the deployment manifest from the file so that I don't have spend time typing as """"""""inline properties"""""""".""""""",,3.0
1,XD-2729,FEATURE,Done,MEDIUM,"""Update payload-conversion demo to latest module spec""","""Upload payload conversion demo such that a user can use the module upload feature against the sample.""","""""""Upload payload conversion demo such that a user can use the module upload feature against the sample.""""""",,3.0
1,XD-2728,BUG,Done,MEDIUM,"""Spark streaming processor module: Dispatcher has no subscribers""","""The spark streaming processor module emits the following exception when there are more messages in the RDD partitions:  015-02-17 12:45:02,026 1.2.0.SNAP ERROR Executor task launch worker-2 executor.Executor - Exception in task 0.0 in stage 56.0 (TID 142) org.springframework.messaging.MessageDeliveryException: Dispatcher has no subscribers for channel 'output'.; nested exception is org.springframework.integration.MessageDispatchingException: Dispatcher has no subscribers  at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:81)  at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:277)  at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:239)  at org.springframework.xd.dirt.plugins.spark.streaming.MessageBusSender.send(MessageBusSender.java:105)  at org.springframework.xd.spark.streaming.java.ModuleExecutor$1$1.call(ModuleExecutor.java:80)  at org.springframework.xd.spark.streaming.java.ModuleExecutor$1$1.call(ModuleExecutor.java:55)  at org.apache.spark.api.java.JavaRDDLike$$anonfun$foreachPartition$1.apply(JavaRDDLike.scala:195)  at org.apache.spark.api.java.JavaRDDLike$$anonfun$foreachPartition$1.apply(JavaRDDLike.scala:195)  at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:790)  at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:790)  at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1353)  at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1353)  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)  at org.apache.spark.scheduler.Task.run(Task.scala:56)  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:200)  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)  at java.lang.Thread.run(Thread.java:745) Caused by: org.springframework.integration.MessageDispatchingException: Dispatcher has no subscribers  at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:107)  at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)  at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)  ... 17 more 2015-02-17 12:45:02,032 1.2.0.SNAP  WARN task-result-getter-1 scheduler.TaskSetManager - Lost task 0.0 in stage 56.0 (TID 142, localhost): org.springframework.messaging.MessageDeliveryException: Dispatcher has no subscribers for channel 'output'.; nested exception is org.springframework.integration.MessageDispatchingException: Dispatcher has no subscribers  at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:81)  at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:277)  at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:239)  at org.springframework.xd.dirt.plugins.spark.streaming.MessageBusSender.send(MessageBusSender.java:105)  at org.springframework.xd.spark.streaming.java.ModuleExecutor$1$1.call(ModuleExecutor.java:80)  at org.springframework.xd.spark.streaming.java.ModuleExecutor$1$1.call(ModuleExecutor.java:55)  at org.apache.spark.api.java.JavaRDDLike$$anonfun$foreachPartition$1.apply(JavaRDDLike.scala:195)  at org.apache.spark.api.java.JavaRDDLike$$anonfun$foreachPartition$1.apply(JavaRDDLike.scala:195)  at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:790)  at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:790)  at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1353)  at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1353)  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)  at org.apache.spark.scheduler.Task.run(Task.scala:56)  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:200)  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)  at java.lang.Thread.run(Thread.java:745) Caused by: org.springframework.integration.MessageDispatchingException: Dispatcher has no subscribers  at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:107)  at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)  at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)  ... 17 more ""","""""""The spark streaming processor module emits the following exception when there are more messages in the RDD partitions:  015-02-17 12:45:02,026 1.2.0.SNAP ERROR Executor task launch worker-2 executor.Executor - Exception in task 0.0 in stage 56.0 (TID 142) org.springframework.messaging.MessageDeliveryException: Dispatcher has no subscribers for channel 'output'.; nested exception is org.springframework.integration.MessageDispatchingException: Dispatcher has no subscribers  at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:81)  at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:277)  at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:239)  at org.springframework.xd.dirt.plugins.spark.streaming.MessageBusSender.send(MessageBusSender.java:105)  at org.springframework.xd.spark.streaming.java.ModuleExecutor$1$1.call(ModuleExecutor.java:80)  at org.springframework.xd.spark.streaming.java.ModuleExecutor$1$1.call(ModuleExecutor.java:55)  at org.apache.spark.api.java.JavaRDDLike$$anonfun$foreachPartition$1.apply(JavaRDDLike.scala:195)  at org.apache.spark.api.java.JavaRDDLike$$anonfun$foreachPartition$1.apply(JavaRDDLike.scala:195)  at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:790)  at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:790)  at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1353)  at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1353)  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)  at org.apache.spark.scheduler.Task.run(Task.scala:56)  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:200)  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)  at java.lang.Thread.run(Thread.java:745) Caused by: org.springframework.integration.MessageDispatchingException: Dispatcher has no subscribers  at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:107)  at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)  at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)  ... 17 more 2015-02-17 12:45:02,032 1.2.0.SNAP  WARN task-result-getter-1 scheduler.TaskSetManager - Lost task 0.0 in stage 56.0 (TID 142, localhost): org.springframework.messaging.MessageDeliveryException: Dispatcher has no subscribers for channel 'output'.; nested exception is org.springframework.integration.MessageDispatchingException: Dispatcher has no subscribers  at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:81)  at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:277)  at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:239)  at org.springframework.xd.dirt.plugins.spark.streaming.MessageBusSender.send(MessageBusSender.java:105)  at org.springframework.xd.spark.streaming.java.ModuleExecutor$1$1.call(ModuleExecutor.java:80)  at org.springframework.xd.spark.streaming.java.ModuleExecutor$1$1.call(ModuleExecutor.java:55)  at org.apache.spark.api.java.JavaRDDLike$$anonfun$foreachPartition$1.apply(JavaRDDLike.scala:195)  at org.apache.spark.api.java.JavaRDDLike$$anonfun$foreachPartition$1.apply(JavaRDDLike.scala:195)  at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:790)  at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:790)  at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1353)  at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1353)  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)  at org.apache.spark.scheduler.Task.run(Task.scala:56)  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:200)  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)  at java.lang.Thread.run(Thread.java:745) Caused by: org.springframework.integration.MessageDispatchingException: Dispatcher has no subscribers  at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:107)  at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)  at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)  ... 17 more """"""",,3.0
1,XD-2727,BUG,Done,MEDIUM,"""Spark streaming integration with kafka message does not respect offsetStoreTopic config option""","""The property """"xd.messagebus.kafka.offsetStoreTopic"""" was added to Kafka message bus which is not updated to Spark streaming message bus properties that will be transferred to spark cluster for streaming module deployment.  We also need a better approach to re-use the message bus properties so that we don't have to update the properties in Connection Property Names.""","""""""The property """"""""xd.messagebus.kafka.offsetStoreTopic"""""""" was added to Kafka message bus which is not updated to Spark streaming message bus properties that will be transferred to spark cluster for streaming module deployment.  We also need a better approach to re-use the message bus properties so that we don't have to update the properties in Connection Property Names.""""""",,3.0
1,XD-2726,FEATURE,Done,MEDIUM,"""Allow deletion of all metrics of a kind""","""To be added in AbstractMetricsController, as well as the various shell commands (""""counter all delete"""", etc...)""","""""""To be added in AbstractMetricsController, as well as the various shell commands (""""""""counter all delete"""""""", etc...)""""""",,5.0
1,XD-2724,FEATURE,Done,MEDIUM,"""Add support to edit deployed/undeployed stream""","""As a user, I'd like to have the option of editing the deployed/undeployed stream so that I don't have to destroy to just change any deployment property.""","""""""As a user, I'd like to have the option of editing the deployed/undeployed stream so that I don't have to destroy to just change any deployment property.""""""",,8.0
1,XD-2723,FEATURE,Done,HIGH,"""Increase the partitionResultsTimeout""","""The partitionResultsTimeout is set to 300000 as default (5min). This is way to short for long running steps. We should increase this default.""","""""""The partitionResultsTimeout is set to 300000 as default (5min). This is way to short for long running steps. We should increase this default.""""""",,1.0
1,XD-2722,BUG,Done,MEDIUM,"""Partitioned job throws: java.lang.RuntimeException: Could not serialize lambda""","""Running a partitioned jdbchdfs job with 12 partitions and 3 xd-containers. Some steps fail with the jdbc connection pool exception XD-2720. I also sometimes see a serialization exception. This results in the partitioner never getting the status for some of the steps, so it keeps running until it times out even though all steps are either complete of failed.  {code} 2015-02-13 13:18:36,294 1.1.0.RELEASE ERROR inbound.files4.0-redis:queue-inbound-channel-adapter1 redis.RedisMessageBus$1 - Failed to deliver message; retries exhausted; message sent to queue 'ERRORS:name' org.springframework.messaging.MessageHandlingException: error occurred in message handler [files4.0.bridge.handler]; nested exception is com.esotericsoftware.kryo.KryoException: java.lang.RuntimeException: Could not serialize lambda Serialization trace: stepExecutions (org.springframework.batch.core.JobExecution) jobExecution (org.springframework.batch.core.StepExecution)  at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:84)  at org.springframework.integration.dispatcher.AbstractDispatcher.tryOptimizedDispatch(AbstractDispatcher.java:116)  at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:101)  at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)  at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)  at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:277)  at org.springframework.xd.dirt.integration.redis.RedisMessageBus$1$1.doWithRetry(RedisMessageBus.java:267)  at org.springframework.xd.dirt.integration.redis.RedisMessageBus$1$1.doWithRetry(RedisMessageBus.java:263)  at org.springframework.retry.support.RetryTemplate.doExecute(RetryTemplate.java:263)  at org.springframework.retry.support.RetryTemplate.execute(RetryTemplate.java:168)  at org.springframework.xd.dirt.integration.redis.RedisMessageBus$1.doSend(RedisMessageBus.java:263)  at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:277)  at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:239)  at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:115)  at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:45)  at org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:95)  at org.springframework.integration.endpoint.MessageProducerSupport.sendMessage(MessageProducerSupport.java:101)  at org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint.popMessageAndSend(RedisQueueMessageDrivenEndpoint.java:220)  at org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint.access$300(RedisQueueMessageDrivenEndpoint.java:50)  at org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint$ListenerTask.run(RedisQueueMessageDrivenEndpoint.java:314)  at org.springframework.integration.util.ErrorHandlingTaskExecutor$1.run(ErrorHandlingTaskExecutor.java:52)  at java.lang.Thread.run(Thread.java:745) Caused by: com.esotericsoftware.kryo.KryoException: java.lang.RuntimeException: Could not serialize lambda Serialization trace: stepExecutions (org.springframework.batch.core.JobExecution) jobExecution (org.springframework.batch.core.StepExecution)  at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125)  at com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:528)  at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:704)  at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:106)  at com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:528)  at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:682)  at org.springframework.xd.dirt.integration.bus.serializer.kryo.PojoCodec.doDeserialize(PojoCodec.java:41)  at org.springframework.xd.dirt.integration.bus.serializer.kryo.AbstractKryoMultiTypeCodec$1.execute(AbstractKryoMultiTypeCodec.java:63)  at com.esotericsoftware.kryo.pool.KryoPoolQueueImpl.run(KryoPoolQueueImpl.java:43)  at org.springframework.xd.dirt.integration.bus.serializer.kryo.AbstractKryoMultiTypeCodec.deserialize(AbstractKryoMultiTypeCodec.java:60)  at org.springframework.xd.dirt.integration.bus.serializer.kryo.PojoCodec.deserialize(PojoCodec.java:30)  at org.springframework.xd.dirt.integration.bus.serializer.CompositeCodec.deserialize(CompositeCodec.java:72)  at org.springframework.xd.dirt.integration.bus.serializer.CompositeCodec.deserialize(CompositeCodec.java:78)  at org.springframework.xd.dirt.integration.bus.MessageBusSupport.deserializePayload(MessageBusSupport.java:588)  at org.springframework.xd.dirt.integration.bus.MessageBusSupport.deserializePayload(MessageBusSupport.java:573)  at org.springframework.xd.dirt.integration.bus.MessageBusSupport.deserializePayloadIfNecessary(MessageBusSupport.java:556)  at org.springframework.xd.dirt.integration.redis.RedisMessageBus.access$1000(RedisMessageBus.java:68)  at org.springframework.xd.dirt.integration.redis.RedisMessageBus$ReceivingHandler.handleRequestMessage(RedisMessageBus.java:465)  at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:99)  at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78)  ... 21 more Caused by: java.lang.RuntimeException: Could not serialize lambda  at com.esotericsoftware.kryo.serializers.ClosureSerializer.read(ClosureSerializer.java:52)  at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:786)  at com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:116)  at com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:22)  at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:704)  at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:106)  ... 40 more Caused by: java.lang.ArrayIndexOutOfBoundsException: -2  at java.util.ArrayList.elementData(ArrayList.java:418)  at java.util.ArrayList.get(ArrayList.java:431)  at com.esotericsoftware.kryo.util.MapReferenceResolver.getReadObject(MapReferenceResolver.java:42)  at com.esotericsoftware.kryo.Kryo.readReferenceOrNull(Kryo.java:830)  at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:680)  at com.esotericsoftware.kryo.serializers.ClosureSerializer.read(ClosureSerializer.java:49)  ... 45 more {code} ""","""""""Running a partitioned jdbchdfs job with 12 partitions and 3 xd-containers. Some steps fail with the jdbc connection pool exception XD-2720. I also sometimes see a serialization exception. This results in the partitioner never getting the status for some of the steps, so it keeps running until it times out even though all steps are either complete of failed.   """"""",""" 2015-02-13 13:18:36,294 1.1.0.RELEASE ERROR inbound.files4.0-redis:queue-inbound-channel-adapter1 redis.RedisMessageBus$1 - Failed to deliver message; retries exhausted; message sent to queue 'ERRORS:name' org.springframework.messaging.MessageHandlingException: error occurred in message handler [files4.0.bridge.handler]; nested exception is com.esotericsoftware.kryo.KryoException: java.lang.RuntimeException: Could not serialize lambda Serialization trace: stepExecutions (org.springframework.batch.core.JobExecution) jobExecution (org.springframework.batch.core.StepExecution)  at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:84)  at org.springframework.integration.dispatcher.AbstractDispatcher.tryOptimizedDispatch(AbstractDispatcher.java:116)  at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:101)  at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)  at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)  at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:277)  at org.springframework.xd.dirt.integration.redis.RedisMessageBus$1$1.doWithRetry(RedisMessageBus.java:267)  at org.springframework.xd.dirt.integration.redis.RedisMessageBus$1$1.doWithRetry(RedisMessageBus.java:263)  at org.springframework.retry.support.RetryTemplate.doExecute(RetryTemplate.java:263)  at org.springframework.retry.support.RetryTemplate.execute(RetryTemplate.java:168)  at org.springframework.xd.dirt.integration.redis.RedisMessageBus$1.doSend(RedisMessageBus.java:263)  at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:277)  at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:239)  at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:115)  at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:45)  at org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:95)  at org.springframework.integration.endpoint.MessageProducerSupport.sendMessage(MessageProducerSupport.java:101)  at org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint.popMessageAndSend(RedisQueueMessageDrivenEndpoint.java:220)  at org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint.access$300(RedisQueueMessageDrivenEndpoint.java:50)  at org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint$ListenerTask.run(RedisQueueMessageDrivenEndpoint.java:314)  at org.springframework.integration.util.ErrorHandlingTaskExecutor$1.run(ErrorHandlingTaskExecutor.java:52)  at java.lang.Thread.run(Thread.java:745) Caused by: com.esotericsoftware.kryo.KryoException: java.lang.RuntimeException: Could not serialize lambda Serialization trace: stepExecutions (org.springframework.batch.core.JobExecution) jobExecution (org.springframework.batch.core.StepExecution)  at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125)  at com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:528)  at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:704)  at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:106)  at com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:528)  at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:682)  at org.springframework.xd.dirt.integration.bus.serializer.kryo.PojoCodec.doDeserialize(PojoCodec.java:41)  at org.springframework.xd.dirt.integration.bus.serializer.kryo.AbstractKryoMultiTypeCodec$1.execute(AbstractKryoMultiTypeCodec.java:63)  at com.esotericsoftware.kryo.pool.KryoPoolQueueImpl.run(KryoPoolQueueImpl.java:43)  at org.springframework.xd.dirt.integration.bus.serializer.kryo.AbstractKryoMultiTypeCodec.deserialize(AbstractKryoMultiTypeCodec.java:60)  at org.springframework.xd.dirt.integration.bus.serializer.kryo.PojoCodec.deserialize(PojoCodec.java:30)  at org.springframework.xd.dirt.integration.bus.serializer.CompositeCodec.deserialize(CompositeCodec.java:72)  at org.springframework.xd.dirt.integration.bus.serializer.CompositeCodec.deserialize(CompositeCodec.java:78)  at org.springframework.xd.dirt.integration.bus.MessageBusSupport.deserializePayload(MessageBusSupport.java:588)  at org.springframework.xd.dirt.integration.bus.MessageBusSupport.deserializePayload(MessageBusSupport.java:573)  at org.springframework.xd.dirt.integration.bus.MessageBusSupport.deserializePayloadIfNecessary(MessageBusSupport.java:556)  at org.springframework.xd.dirt.integration.redis.RedisMessageBus.access$1000(RedisMessageBus.java:68)  at org.springframework.xd.dirt.integration.redis.RedisMessageBus$ReceivingHandler.handleRequestMessage(RedisMessageBus.java:465)  at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:99)  at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78)  ... 21 more Caused by: java.lang.RuntimeException: Could not serialize lambda  at com.esotericsoftware.kryo.serializers.ClosureSerializer.read(ClosureSerializer.java:52)  at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:786)  at com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:116)  at com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:22)  at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:704)  at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:106)  ... 40 more Caused by: java.lang.ArrayIndexOutOfBoundsException: -2  at java.util.ArrayList.elementData(ArrayList.java:418)  at java.util.ArrayList.get(ArrayList.java:431)  at com.esotericsoftware.kryo.util.MapReferenceResolver.getReadObject(MapReferenceResolver.java:42)  at com.esotericsoftware.kryo.Kryo.readReferenceOrNull(Kryo.java:830)  at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:680)  at com.esotericsoftware.kryo.serializers.ClosureSerializer.read(ClosureSerializer.java:49)  ... 45 more """,5.0
1,XD-2721,IMPROVEMENT,Done,MEDIUM,"""Remove requirement for executionId to display step execution in shell""","""When viewing a job's step execution via the shell, the user is required to provide both the job execution id and the step execution id.  Since the job repository is backed by a database and the step execution id is unique across jobs, the step execution id should be enough.""","""""""When viewing a job's step execution via the shell, the user is required to provide both the job execution id and the step execution id.  Since the job repository is backed by a database and the step execution id is unique across jobs, the step execution id should be enough.""""""",,2.0
1,XD-2720,BUG,Done,URGENT,"""Frequent connection pool errors with multi-partitioned jdbchdfs jobs""","""I'm running a jdbchdfs job with 8 partitions and 2 containers. Some steps complete ok while some (3-4 on average) fail with a connection pool error (see below). This happens with a decent size table (1.8M rows).  I tried two different databases - Oracle 11g on a separate server and MySQL running locally where the XD containers where running. Same pattern with both databases.  {code} 2015-02-13 12:21:33,436 1.1.0.RELEASE ERROR inbound.files3.0-redis:queue-inbound-channel-adapter1 step.AbstractStep - Encountered an error executing step step1 in job files3 org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'scopedTarget.itemReader' defined in file [/home/trisberg/Test/spring-xd-1.1.0.RELEASE/xd/modules/job/jdbchdfs/config/jdbchdfs.xml]: Invocation of init method failed; nested exception is org.springframework.jdbc.support.MetaDataAccessException: Could not get Connection for extracting meta data; nested exception is org.springframework.jdbc.CannotGetJdbcConnectionException: Could not get JDBC Connection; nested exception is java.sql.SQLException: Failed to validate a newly established connection.  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1566)  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:539)  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:476)  at org.springframework.beans.factory.support.AbstractBeanFactory$2.getObject(AbstractBeanFactory.java:342)  at org.springframework.batch.core.scope.StepScope.get(StepScope.java:110)  at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:337)  at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:194)  at org.springframework.aop.target.SimpleBeanTargetSource.getTarget(SimpleBeanTargetSource.java:35)  at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:187)  at com.sun.proxy.$Proxy90.open(Unknown Source)  at org.springframework.batch.item.support.CompositeItemStream.open(CompositeItemStream.java:96)  at org.springframework.batch.core.step.tasklet.TaskletStep.open(TaskletStep.java:310)  at org.springframework.batch.core.step.AbstractStep.execute(AbstractStep.java:195)  at org.springframework.batch.integration.partition.StepExecutionRequestHandler.handle(StepExecutionRequestHandler.java:64)  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)  at java.lang.reflect.Method.invoke(Method.java:606)  at org.springframework.expression.spel.support.ReflectiveMethodExecutor.execute(ReflectiveMethodExecutor.java:112)  at org.springframework.expression.spel.ast.MethodReference.getValueInternal(MethodReference.java:102)  at org.springframework.expression.spel.ast.MethodReference.access$000(MethodReference.java:49)  at org.springframework.expression.spel.ast.MethodReference$MethodValueRef.getValue(MethodReference.java:342)  at org.springframework.expression.spel.ast.CompoundExpression.getValueInternal(CompoundExpression.java:88)  at org.springframework.expression.spel.ast.SpelNodeImpl.getTypedValue(SpelNodeImpl.java:131)  at org.springframework.expression.spel.standard.SpelExpression.getValue(SpelExpression.java:330)  at org.springframework.integration.util.AbstractExpressionEvaluator.evaluateExpression(AbstractExpressionEvaluator.java:164)  at org.springframework.integration.util.MessagingMethodInvokerHelper.processInternal(MessagingMethodInvokerHelper.java:276)  at org.springframework.integration.util.MessagingMethodInvokerHelper.process(MessagingMethodInvokerHelper.java:142)  at org.springframework.integration.handler.MethodInvokingMessageProcessor.processMessage(MethodInvokingMessageProcessor.java:75)  at org.springframework.integration.handler.ServiceActivatingHandler.handleRequestMessage(ServiceActivatingHandler.java:71)  at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:99)  at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78)  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)  at java.lang.reflect.Method.invoke(Method.java:606)  at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)  at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)  at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)  at org.springframework.integration.monitor.SimpleMessageHandlerMetrics.handleMessage(SimpleMessageHandlerMetrics.java:107)  at org.springframework.integration.monitor.SimpleMessageHandlerMetrics.invoke(SimpleMessageHandlerMetrics.java:87)  at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)  at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)  at com.sun.proxy.$Proxy94.handleMessage(Unknown Source)  at org.springframework.integration.dispatcher.AbstractDispatcher.tryOptimizedDispatch(AbstractDispatcher.java:116)  at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:101)  at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)  at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)  at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:277)  at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:239)  at sun.reflect.GeneratedMethodAccessor69.invoke(Unknown Source)  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)  at java.lang.reflect.Method.invoke(Method.java:606)  at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)  at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)  at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)  at org.springframework.integration.monitor.DirectChannelMetrics.monitorSend(DirectChannelMetrics.java:114)  at org.springframework.integration.monitor.DirectChannelMetrics.doInvoke(DirectChannelMetrics.java:98)  at org.springframework.integration.monitor.DirectChannelMetrics.invoke(DirectChannelMetrics.java:92)  at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)  at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)  at com.sun.proxy.$Proxy91.send(Unknown Source)  at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:115)  at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:45)  at org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:95)  at org.springframework.integration.handler.AbstractMessageProducingHandler.sendOutput(AbstractMessageProducingHandler.java:248)  at org.springframework.integration.handler.AbstractMessageProducingHandler.produceOutput(AbstractMessageProducingHandler.java:171)  at org.springframework.integration.handler.AbstractMessageProducingHandler.sendOutputs(AbstractMessageProducingHandler.java:119)  at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:105)  at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78)  at org.springframework.integration.dispatcher.AbstractDispatcher.tryOptimizedDispatch(AbstractDispatcher.java:116)  at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:101)  at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)  at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)  at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:277)  at org.springframework.xd.dirt.integration.redis.RedisMessageBus$1$1.doWithRetry(RedisMessageBus.java:267)  at org.springframework.xd.dirt.integration.redis.RedisMessageBus$1$1.doWithRetry(RedisMessageBus.java:263)  at org.springframework.retry.support.RetryTemplate.doExecute(RetryTemplate.java:263)  at org.springframework.retry.support.RetryTemplate.execute(RetryTemplate.java:168)  at org.springframework.xd.dirt.integration.redis.RedisMessageBus$1.doSend(RedisMessageBus.java:263)  at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:277)  at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:239)  at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:115)  at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:45)  at org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:95)  at org.springframework.integration.endpoint.MessageProducerSupport.sendMessage(MessageProducerSupport.java:101)  at org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint.popMessageAndSend(RedisQueueMessageDrivenEndpoint.java:220)  at org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint.access$300(RedisQueueMessageDrivenEndpoint.java:50)  at org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint$ListenerTask.run(RedisQueueMessageDrivenEndpoint.java:314)  at org.springframework.integration.util.ErrorHandlingTaskExecutor$1.run(ErrorHandlingTaskExecutor.java:52)  at java.lang.Thread.run(Thread.java:745) Caused by: org.springframework.jdbc.support.MetaDataAccessException: Could not get Connection for extracting meta data; nested exception is org.springframework.jdbc.CannotGetJdbcConnectionException: Could not get JDBC Connection; nested exception is java.sql.SQLException: Failed to validate a newly established connection.  at org.springframework.jdbc.support.JdbcUtils.extractDatabaseMetaData(JdbcUtils.java:302)  at org.springframework.jdbc.support.JdbcUtils.extractDatabaseMetaData(JdbcUtils.java:329)  at org.springframework.batch.support.DatabaseType.fromMetaData(DatabaseType.java:95)  at org.springframework.xd.jdbc.NamedColumnJdbcItemReaderFactory.afterPropertiesSet(NamedColumnJdbcItemReaderFactory.java:101)  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1625)  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1562)  ... 90 more Caused by: org.springframework.jdbc.CannotGetJdbcConnectionException: Could not get JDBC Connection; nested exception is java.sql.SQLException: Failed to validate a newly established connection.  at org.springframework.jdbc.datasource.DataSourceUtils.getConnection(DataSourceUtils.java:80)  at org.springframework.jdbc.support.JdbcUtils.extractDatabaseMetaData(JdbcUtils.java:289)  ... 95 more Caused by: java.sql.SQLException: Failed to validate a newly established connection.  at org.apache.tomcat.jdbc.pool.ConnectionPool.borrowConnection(ConnectionPool.java:802)  at org.apache.tomcat.jdbc.pool.ConnectionPool.borrowConnection(ConnectionPool.java:617)  at org.apache.tomcat.jdbc.pool.ConnectionPool.getConnection(ConnectionPool.java:186)  at org.apache.tomcat.jdbc.pool.DataSourceProxy.getConnection(DataSourceProxy.java:127)  at org.springframework.jdbc.datasource.DataSourceUtils.doGetConnection(DataSourceUtils.java:111)  at org.springframework.jdbc.datasource.DataSourceUtils.getConnection(DataSourceUtils.java:77)  ... 96 more {code}""","""""""I'm running a jdbchdfs job with 8 partitions and 2 containers. Some steps complete ok while some (3-4 on average) fail with a connection pool error (see below). This happens with a decent size table (1.8M rows).  I tried two different databases - Oracle 11g on a separate server and MySQL running locally where the XD containers where running. Same pattern with both databases.  """"""",""" 2015-02-13 12:21:33,436 1.1.0.RELEASE ERROR inbound.files3.0-redis:queue-inbound-channel-adapter1 step.AbstractStep - Encountered an error executing step step1 in job files3 org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'scopedTarget.itemReader' defined in file [/home/trisberg/Test/spring-xd-1.1.0.RELEASE/xd/modules/job/jdbchdfs/config/jdbchdfs.xml]: Invocation of init method failed; nested exception is org.springframework.jdbc.support.MetaDataAccessException: Could not get Connection for extracting meta data; nested exception is org.springframework.jdbc.CannotGetJdbcConnectionException: Could not get JDBC Connection; nested exception is java.sql.SQLException: Failed to validate a newly established connection.  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1566)  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:539)  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:476)  at org.springframework.beans.factory.support.AbstractBeanFactory$2.getObject(AbstractBeanFactory.java:342)  at org.springframework.batch.core.scope.StepScope.get(StepScope.java:110)  at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:337)  at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:194)  at org.springframework.aop.target.SimpleBeanTargetSource.getTarget(SimpleBeanTargetSource.java:35)  at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:187)  at com.sun.proxy.$Proxy90.open(Unknown Source)  at org.springframework.batch.item.support.CompositeItemStream.open(CompositeItemStream.java:96)  at org.springframework.batch.core.step.tasklet.TaskletStep.open(TaskletStep.java:310)  at org.springframework.batch.core.step.AbstractStep.execute(AbstractStep.java:195)  at org.springframework.batch.integration.partition.StepExecutionRequestHandler.handle(StepExecutionRequestHandler.java:64)  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)  at java.lang.reflect.Method.invoke(Method.java:606)  at org.springframework.expression.spel.support.ReflectiveMethodExecutor.execute(ReflectiveMethodExecutor.java:112)  at org.springframework.expression.spel.ast.MethodReference.getValueInternal(MethodReference.java:102)  at org.springframework.expression.spel.ast.MethodReference.access$000(MethodReference.java:49)  at org.springframework.expression.spel.ast.MethodReference$MethodValueRef.getValue(MethodReference.java:342)  at org.springframework.expression.spel.ast.CompoundExpression.getValueInternal(CompoundExpression.java:88)  at org.springframework.expression.spel.ast.SpelNodeImpl.getTypedValue(SpelNodeImpl.java:131)  at org.springframework.expression.spel.standard.SpelExpression.getValue(SpelExpression.java:330)  at org.springframework.integration.util.AbstractExpressionEvaluator.evaluateExpression(AbstractExpressionEvaluator.java:164)  at org.springframework.integration.util.MessagingMethodInvokerHelper.processInternal(MessagingMethodInvokerHelper.java:276)  at org.springframework.integration.util.MessagingMethodInvokerHelper.process(MessagingMethodInvokerHelper.java:142)  at org.springframework.integration.handler.MethodInvokingMessageProcessor.processMessage(MethodInvokingMessageProcessor.java:75)  at org.springframework.integration.handler.ServiceActivatingHandler.handleRequestMessage(ServiceActivatingHandler.java:71)  at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:99)  at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78)  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)  at java.lang.reflect.Method.invoke(Method.java:606)  at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)  at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)  at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)  at org.springframework.integration.monitor.SimpleMessageHandlerMetrics.handleMessage(SimpleMessageHandlerMetrics.java:107)  at org.springframework.integration.monitor.SimpleMessageHandlerMetrics.invoke(SimpleMessageHandlerMetrics.java:87)  at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)  at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)  at com.sun.proxy.$Proxy94.handleMessage(Unknown Source)  at org.springframework.integration.dispatcher.AbstractDispatcher.tryOptimizedDispatch(AbstractDispatcher.java:116)  at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:101)  at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)  at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)  at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:277)  at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:239)  at sun.reflect.GeneratedMethodAccessor69.invoke(Unknown Source)  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)  at java.lang.reflect.Method.invoke(Method.java:606)  at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)  at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)  at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)  at org.springframework.integration.monitor.DirectChannelMetrics.monitorSend(DirectChannelMetrics.java:114)  at org.springframework.integration.monitor.DirectChannelMetrics.doInvoke(DirectChannelMetrics.java:98)  at org.springframework.integration.monitor.DirectChannelMetrics.invoke(DirectChannelMetrics.java:92)  at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)  at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)  at com.sun.proxy.$Proxy91.send(Unknown Source)  at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:115)  at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:45)  at org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:95)  at org.springframework.integration.handler.AbstractMessageProducingHandler.sendOutput(AbstractMessageProducingHandler.java:248)  at org.springframework.integration.handler.AbstractMessageProducingHandler.produceOutput(AbstractMessageProducingHandler.java:171)  at org.springframework.integration.handler.AbstractMessageProducingHandler.sendOutputs(AbstractMessageProducingHandler.java:119)  at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:105)  at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78)  at org.springframework.integration.dispatcher.AbstractDispatcher.tryOptimizedDispatch(AbstractDispatcher.java:116)  at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:101)  at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)  at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)  at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:277)  at org.springframework.xd.dirt.integration.redis.RedisMessageBus$1$1.doWithRetry(RedisMessageBus.java:267)  at org.springframework.xd.dirt.integration.redis.RedisMessageBus$1$1.doWithRetry(RedisMessageBus.java:263)  at org.springframework.retry.support.RetryTemplate.doExecute(RetryTemplate.java:263)  at org.springframework.retry.support.RetryTemplate.execute(RetryTemplate.java:168)  at org.springframework.xd.dirt.integration.redis.RedisMessageBus$1.doSend(RedisMessageBus.java:263)  at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:277)  at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:239)  at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:115)  at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:45)  at org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:95)  at org.springframework.integration.endpoint.MessageProducerSupport.sendMessage(MessageProducerSupport.java:101)  at org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint.popMessageAndSend(RedisQueueMessageDrivenEndpoint.java:220)  at org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint.access$300(RedisQueueMessageDrivenEndpoint.java:50)  at org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint$ListenerTask.run(RedisQueueMessageDrivenEndpoint.java:314)  at org.springframework.integration.util.ErrorHandlingTaskExecutor$1.run(ErrorHandlingTaskExecutor.java:52)  at java.lang.Thread.run(Thread.java:745) Caused by: org.springframework.jdbc.support.MetaDataAccessException: Could not get Connection for extracting meta data; nested exception is org.springframework.jdbc.CannotGetJdbcConnectionException: Could not get JDBC Connection; nested exception is java.sql.SQLException: Failed to validate a newly established connection.  at org.springframework.jdbc.support.JdbcUtils.extractDatabaseMetaData(JdbcUtils.java:302)  at org.springframework.jdbc.support.JdbcUtils.extractDatabaseMetaData(JdbcUtils.java:329)  at org.springframework.batch.support.DatabaseType.fromMetaData(DatabaseType.java:95)  at org.springframework.xd.jdbc.NamedColumnJdbcItemReaderFactory.afterPropertiesSet(NamedColumnJdbcItemReaderFactory.java:101)  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1625)  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1562)  ... 90 more Caused by: org.springframework.jdbc.CannotGetJdbcConnectionException: Could not get JDBC Connection; nested exception is java.sql.SQLException: Failed to validate a newly established connection.  at org.springframework.jdbc.datasource.DataSourceUtils.getConnection(DataSourceUtils.java:80)  at org.springframework.jdbc.support.JdbcUtils.extractDatabaseMetaData(JdbcUtils.java:289)  ... 95 more Caused by: java.sql.SQLException: Failed to validate a newly established connection.  at org.apache.tomcat.jdbc.pool.ConnectionPool.borrowConnection(ConnectionPool.java:802)  at org.apache.tomcat.jdbc.pool.ConnectionPool.borrowConnection(ConnectionPool.java:617)  at org.apache.tomcat.jdbc.pool.ConnectionPool.getConnection(ConnectionPool.java:186)  at org.apache.tomcat.jdbc.pool.DataSourceProxy.getConnection(DataSourceProxy.java:127)  at org.springframework.jdbc.datasource.DataSourceUtils.doGetConnection(DataSourceUtils.java:111)  at org.springframework.jdbc.datasource.DataSourceUtils.getConnection(DataSourceUtils.java:77)  ... 96 more """,5.0
1,XD-2719,FEATURE,Done,MEDIUM,"""Invoke Rabbit REST-API to clean-up resources""","""As a user, I'd like to clean-up stale queues/topics associated with the stream so when the stream gets destroyed I can clean-up resources. ""","""""""As a user, I'd like to clean-up stale queues/topics associated with the stream so when the stream gets destroyed I can clean-up resources. """"""",,3.0
1,XD-2718,FEATURE,Done,MEDIUM,"""Add a separate --clean 'Admin' command to clean-up queues/topics""","""As a user, I'd like to clean up message bus resources associated with the stream so that when the stream is destroyed so does the coupled queues/topics.""","""""""As a user, I'd like to clean up message bus resources associated with the stream so that when the stream is destroyed so does the coupled queues/topics.""""""",,3.0
1,XD-2717,FEATURE,Done,MEDIUM,"""Add nameExpression Property to File Sink""","""As a stream definer, when defining a stream ending with a file sink, I would like to have more flexibility for naming the file.  Add an alternative {{--nameExpression}} option, allowing complete control over the {{finename-generator-expression}} attribute.  See: http://stackoverflow.com/questions/28466477/issue-with-file-sink-and-filename-expression/28467069#28467069""","""""""As a stream definer, when defining a stream ending with a file sink, I would like to have more flexibility for naming the file.  Add an alternative {{--nameExpression}} option, allowing complete control over the {{finename-generator-expression}} attribute.  See: http://stackoverflow.com/questions/28466477/issue-with-file-sink-and-filename-expression/28467069#28467069""""""",,1.0
1,XD-2716,FEATURE,Done,MEDIUM,"""Create a Batch example to demonstrate JDBC->HDFS""","""As a developer, I'd like to create a example to demonstrate JDBC to HDFS data movement.""","""""""As a developer, I'd like to create a example to demonstrate JDBC to HDFS data movement.""""""",,8.0
1,XD-2715,FEATURE,Done,MEDIUM,"""Add Smart Grid demo to XD samples repo""","""As a PM, I'd like to have the Smart Grid demo (from s1-2014) ported into Spring XD samples repo.""","""""""As a PM, I'd like to have the Smart Grid demo (from s1-2014) ported into Spring XD samples repo.""""""",,8.0
1,XD-2713,FEATURE,Done,MEDIUM,"""Replicate Storm examples in XD""","""As a field engineer, I'd like to have a comparison of Storm examples in Spring XD so that it is easy to relate from implementation standpoint. ""","""""""As a field engineer, I'd like to have a comparison of Storm examples in Spring XD so that it is easy to relate from implementation standpoint. """"""",,8.0
1,XD-2712,FEATURE,Done,MEDIUM,"""Embed local message bus in DIRT as default for singlenode""","""Currently all message bus implementations are removed from the runtime classpath and loaded on demand from the file system according to the transport setting. Custom module projects that include in container testing must install messagebus-local on the local file system. This is currently configured as a task for module build scripts. This is also a dependency for testing in the IDE and developers need to execute the build task or configure the messagebus manually. Embedding the local MB for the singlenode application (local is not a valid transport for distributed) eliminates this step.""","""""""Currently all message bus implementations are removed from the runtime classpath and loaded on demand from the file system according to the transport setting. Custom module projects that include in container testing must install messagebus-local on the local file system. This is currently configured as a task for module build scripts. This is also a dependency for testing in the IDE and developers need to execute the build task or configure the messagebus manually. Embedding the local MB for the singlenode application (local is not a valid transport for distributed) eliminates this step.""""""",,2.0
1,XD-2711,MAINTENANCE,Done,MEDIUM,"""CI Environment Needs test resources updated to latest versions""","""Update Hadoop, Spark, Mongo, gemfire, and ubuntu to latest versions for both CI  and Utility instances.   ""","""""""Update Hadoop, Spark, Mongo, gemfire, and ubuntu to latest versions for both CI  and Utility instances.   """"""",,3.0
1,XD-2710,BUG,Done,HIGH,"""Kafka Tests shouldn't assume offset 0 ""","""In `org.springframework.xd.dirt.stream.KafkaSingleNodeStreamDeploymentIntegrationTests#verifyOnDemandQueues`, when testing the queue partitions for content, the read is assumed to start at offset 0.  This is incorrect, because the topics may exist already, especially in a CI environment""","""""""In `org.springframework.xd.dirt.stream.KafkaSingleNodeStreamDeploymentIntegrationTests#verifyOnDemandQueues`, when testing the queue partitions for content, the read is assumed to start at offset 0.  This is incorrect, because the topics may exist already, especially in a CI environment""""""",,1.0
1,XD-2709,MAINTENANCE,Done,MEDIUM,"""Investigate root cause of Spring XML parsing failures""","""The error below appears to be a concurrency issue related to multiple classloaders loading the same spring XML resource.  The root cause needs further investigation, but it causes intermittent test failures in JobCommandsTests.  see https://build.spring.io/browse/XD-JDK8-1375  java.lang.AssertionError: java.lang.AssertionError: Failure.  CommandResult = CommandResult [success=false, result=null, exception=org.springframework.xd.rest.client.impl.SpringXDException: Unexpected exception parsing XML document from file [/data/bamboo-home/xml-data/build-dir/XD-JDK8-JOB1/spring-xd-shell/build/resources/test/spring-xd/xd/modules/job/jobWithParameters/config/jobWithParameters.xml]; nested exception is java.lang.NullPointerException: Inflater has been closed ] java.lang.AssertionError: Failure.  CommandResult = CommandResult [success=false, result=null, exception=org.springframework.xd.rest.client.impl.SpringXDException: Unexpected exception parsing XML document from file [/data/bamboo-home/xml-data/build-dir/XD-JDK8-JOB1/spring-xd-shell/build/resources/test/spring-xd/xd/modules/job/jobWithParameters/config/jobWithParameters.xml]; nested exception is java.lang.NullPointerException: Inflater has been closed ] ""","""""""The error below appears to be a concurrency issue related to multiple classloaders loading the same spring XML resource.  The root cause needs further investigation, but it causes intermittent test failures in JobCommandsTests.  see https://build.spring.io/browse/XD-JDK8-1375  java.lang.AssertionError: java.lang.AssertionError: Failure.  CommandResult = CommandResult [success=false, result=null, exception=org.springframework.xd.rest.client.impl.SpringXDException: Unexpected exception parsing XML document from file [/data/bamboo-home/xml-data/build-dir/XD-JDK8-JOB1/spring-xd-shell/build/resources/test/spring-xd/xd/modules/job/jobWithParameters/config/jobWithParameters.xml]; nested exception is java.lang.NullPointerException: Inflater has been closed ] java.lang.AssertionError: Failure.  CommandResult = CommandResult [success=false, result=null, exception=org.springframework.xd.rest.client.impl.SpringXDException: Unexpected exception parsing XML document from file [/data/bamboo-home/xml-data/build-dir/XD-JDK8-JOB1/spring-xd-shell/build/resources/test/spring-xd/xd/modules/job/jobWithParameters/config/jobWithParameters.xml]; nested exception is java.lang.NullPointerException: Inflater has been closed ] """"""",,5.0
1,XD-2706,FEATURE,Done,MEDIUM,"""Improve Redis Bus Error Log Entry""","""{code} logger.error(""""Failed to deliver message; retries exhausted; message sent to queue 'ERRORS:name'"""",          context.getLastThrowable()); {code}  Should be:  {code} logger.error(""""Failed to deliver message; retries exhausted; message sent to queue 'ERRORS:"""" + name + '"""",          context.getLastThrowable()); {code}  So the actual name is logged. ""","""""""  Should be:    So the actual name is logged. """"""",""" logger.error(""""""""Failed to deliver message; retries exhausted; message sent to queue 'ERRORS:name'"""""""",          context.getLastThrowable());  logger.error(""""""""Failed to deliver message; retries exhausted; message sent to queue 'ERRORS:"""""""" + name + '"""""""",          context.getLastThrowable()); """,1.0
1,XD-2705,FEATURE,Done,MEDIUM,"""Add ability to add Custom Metrics/Controllers To Admin Context ""","""See original report here https://github.com/spring-projects/spring-xd/issues/1300  """"I've created a module with a custom Metric, Handler and Repository patterned after the AggregateCounter but then it appears that there doesn't seem to be a way to add anything to the Admin Context.  The diagram at https://github.com/spring-projects/spring-xd/wiki/Extending-XD shows the Plugin Context as a sibling to the Admin Context which seems to verify my fears.  It would be convenient to be able to add custom Metrics/Controllers/Repositories into the Admin Context so that they can be accessed through the same REST api as the other Metrics.""""""","""""""See original report here https://github.com/spring-projects/spring-xd/issues/1300  """"""""I've created a module with a custom Metric, Handler and Repository patterned after the AggregateCounter but then it appears that there doesn't seem to be a way to add anything to the Admin Context.  The diagram at https://github.com/spring-projects/spring-xd/wiki/Extending-XD shows the Plugin Context as a sibling to the Admin Context which seems to verify my fears.  It would be convenient to be able to add custom Metrics/Controllers/Repositories into the Admin Context so that they can be accessed through the same REST api as the other Metrics.""""""""""""""",,5.0
1,XD-2704,FEATURE,Done,MEDIUM,"""Move documentation from wiki to main repo""","""As a consequence,  * change gradle script regarding generation of documentation * remove pushGeneratedDocs task, etc * remove link rewriting that is no longer needed ""","""""""As a consequence,  * change gradle script regarding generation of documentation * remove pushGeneratedDocs task, etc * remove link rewriting that is no longer needed """"""",,8.0
1,XD-2703,FEATURE,Done,MEDIUM,"""Create Sqoop example""","""As a developer, I'd like to build batch sample using _Sqoop_ so that we can demonstrate some of the capabilities.  *Use cases to consider:* * JDBC to HDFS * HDFS to JDBC ""","""""""As a developer, I'd like to build batch sample using _Sqoop_ so that we can demonstrate some of the capabilities.  *Use cases to consider:* * JDBC to HDFS * HDFS to JDBC """"""",,8.0
1,XD-2702,FEATURE,Done,MEDIUM,"""Create Kafka data pipeline example""","""As a developer, I'd like to build data pipeline using _Kafka_ as as message bus in XD so that we can demonstrate some of the capabilities.  *Use case to consider:* * Log aggregation and analysis * Lambda architecture ** how to avoid code duplication ** how to eliminate tight coupling of business logic ** how Kafka can be used for reliable reprocessing  ""","""""""As a developer, I'd like to build data pipeline using _Kafka_ as as message bus in XD so that we can demonstrate some of the capabilities.  *Use case to consider:* * Log aggregation and analysis * Lambda architecture ** how to avoid code duplication ** how to eliminate tight coupling of business logic ** how Kafka can be used for reliable reprocessing  """"""",,8.0
1,XD-2700,FEATURE,Done,MEDIUM,"""Remove spark and hadoop dependencies from custom module classpath""","""remove spark and hadoop requirements from spring-xd-module-parent and gradle module plugin""","""""""remove spark and hadoop requirements from spring-xd-module-parent and gradle module plugin""""""",,2.0
1,XD-2699,FEATURE,Done,MEDIUM,"""Review and fix Sonar violations""","""As a developer, I'd like to review the current sonar violations so that I can fix the relevant and update the irrelevant ones as invalid.""","""""""As a developer, I'd like to review the current sonar violations so that I can fix the relevant and update the irrelevant ones as invalid.""""""",,3.0
1,XD-2698,FEATURE,Done,MEDIUM,"""Kafka Tests should use an external broker""","""As a developer, I want to have to run Kafka tests on an external broker, so that I reduce the footprint of the build process. ""","""""""As a developer, I want to have to run Kafka tests on an external broker, so that I reduce the footprint of the build process. """"""",,2.0
1,XD-2697,MAINTENANCE,Done,MEDIUM,"""Make Sqoop job and MapReduce samples work with Hortonworks HDP 2.2 single-node cluster ""","""Having problems testing against the Sandbox 2.2. We need to set the following properties:  yarn.application.classpath yarn.app.mapreduce.am.command-opts mapreduce.application.classpath mapreduce.application.framework.path  ""","""""""Having problems testing against the Sandbox 2.2. We need to set the following properties:  yarn.application.classpath yarn.app.mapreduce.am.command-opts mapreduce.application.classpath mapreduce.application.framework.path  """"""",,5.0
1,XD-2696,FEATURE,Done,HIGH,"""Downgrade reactor based modules to reactor 1.x GA""","""Can revert part of the commit that went into upgrading to reactor 2.0  https://github.com/spring-projects/spring-xd/pull/1342/files ""","""""""Can revert part of the commit that went into upgrading to reactor 2.0  https://github.com/spring-projects/spring-xd/pull/1342/files """"""",,1.0
1,XD-2694,FEATURE,Done,MEDIUM,"""Spring XD module parent should use dependencies.properties for versioning""","""As a developer, I'd like the {{publish-maven.gradle}} script to use values for dependencies (e.g. Spring Boot and {{hadoop-common}}) from our central dependency list (in this case {{dependencies.properties}}) so that I don't have to update them manually anymore.""","""""""As a developer, I'd like the {{publish-maven.gradle}} script to use values for dependencies (e.g. Spring Boot and {{hadoop-common}}) from our central dependency list (in this case {{dependencies.properties}}) so that I don't have to update them manually anymore.""""""",,2.0
1,XD-2693,FEATURE,Done,MEDIUM,"""Run Kafka tests with Kafka Server as separate process""","""As a developer, I'd like to run Kafka tests with Kafka Server as a separate running process so that I can improve build experience. ""","""""""As a developer, I'd like to run Kafka tests with Kafka Server as a separate running process so that I can improve build experience. """"""",,3.0
1,XD-2691,FEATURE,Done,MEDIUM,"""Upgrade to the latest Gradle 2.x Release""","""Gradle 2.x is required for the latest Sonar version (sonar.spring.io)  We may need to wait for a fix in Groovy itself (2.4.1) - Please see the following links for details:  * http://forums.gradle.org/gradle/topics/after_upgrade_gradle_to_2_0_version_the_maven_pom_not_support_build_property  * http://jira.codehaus.org/browse/GROOVY-7023  ""","""""""Gradle 2.x is required for the latest Sonar version (sonar.spring.io)  We may need to wait for a fix in Groovy itself (2.4.1) - Please see the following links for details:  * http://forums.gradle.org/gradle/topics/after_upgrade_gradle_to_2_0_version_the_maven_pom_not_support_build_property  * http://jira.codehaus.org/browse/GROOVY-7023  """"""",,5.0
1,XD-2690,FEATURE,Done,MEDIUM,"""Restrict entry filter in loading module artifacts""","""see https://github.com/spring-projects/spring-boot/issues/2454""","""""""see https://github.com/spring-projects/spring-boot/issues/2454""""""",,1.0
1,XD-2689,MAINTENANCE,Done,URGENT,"""Fix Sqoop job to allow for setting yarn.application.classpath""","""Running Sqoop job against non Apache Hadoop installation  - YARN app fails       Error: Could not find or load main class org.apache.hadoop.mapreduce.v2.app.MRAppMaster  - Need to be able to set yarn.application.classpath for any distro that doesn't use the Hadoop defult classpath (Cloudera, Hortonworks, Pivotal HD)""","""""""Running Sqoop job against non Apache Hadoop installation  - YARN app fails       Error: Could not find or load main class org.apache.hadoop.mapreduce.v2.app.MRAppMaster  - Need to be able to set yarn.application.classpath for any distro that doesn't use the Hadoop defult classpath (Cloudera, Hortonworks, Pivotal HD)""""""",,3.0
1,XD-2688,MAINTENANCE,Done,URGENT,"""Fix mapreduce job submission on Cloudera CDH5""","""Submitting jobs that submit YARN MR tasks on Cloudera 5.3.0  - job fails when submitting the YARN app       java.lang.NoClassDefFoundError: com/google/common/io/LimitInputStream  - this is from Guava and that class was removed starting with v. 15.0  - I can get around this by including guava-14.0.1.jar in lib/cdh5 (not sure if this breaks something else) ""","""""""Submitting jobs that submit YARN MR tasks on Cloudera 5.3.0  - job fails when submitting the YARN app       java.lang.NoClassDefFoundError: com/google/common/io/LimitInputStream  - this is from Guava and that class was removed starting with v. 15.0  - I can get around this by including guava-14.0.1.jar in lib/cdh5 (not sure if this breaks something else) """"""",,1.0
1,XD-2687,FEATURE,Done,MEDIUM,"""Fix #jsonPath evaluation following JsonPath version upgrade""","""Fix #jsonPath evaluation following JsonPath version upgrade""","""Fix #jsonPath evaluation following JsonPath version upgrade""",,1.0
1,XD-2686,FEATURE,Done,MEDIUM,"""Update log4j properties to include DATE in the logs""","""As a user, I'd like to see the 'date' in logs so that I can troubleshoot issues that had occurred on a specific day and time.  Property that needs adjusted: https://github.com/spring-projects/spring-xd/blob/master/config/xd-container-logger.properties#L11""","""""""As a user, I'd like to see the 'date' in logs so that I can troubleshoot issues that had occurred on a specific day and time.  Property that needs adjusted: https://github.com/spring-projects/spring-xd/blob/master/config/xd-container-logger.properties#L11""""""",,1.0
1,XD-2684,FEATURE,Done,HIGH,"""Set sourceCompatibility to JDK 1.7""","""Min JDK version for XD 1.1 is 7.  Change the sourceCompatibility to 1.7 but leave targetCompatibility at 1.6.   Changes are also needed in the mdule parent pom and gradle plugins.""","""""""Min JDK version for XD 1.1 is 7.  Change the sourceCompatibility to 1.7 but leave targetCompatibility at 1.6.   Changes are also needed in the mdule parent pom and gradle plugins.""""""",,1.0
1,XD-2683,BUG,Done,MEDIUM,"""Message conversion support for spark streaming module""","""The spark streaming module needs to support inputType (for both processor and sink module) and outputType (for processor module) so that message conversion can happen at the MessageBusReceiver and MessageBusSender.""","""""""The spark streaming module needs to support inputType (for both processor and sink module) and outputType (for processor module) so that message conversion can happen at the MessageBusReceiver and MessageBusSender.""""""",,5.0
1,XD-2681,FEATURE,Done,MEDIUM,"""Document recommended 'ulimit' setting for XD""","""As a developer, I'd like to refer to wiki so that I can configure machines with recommended _ulimit_ setting for XD's distributed setup.  *Note:* Recommended _ulimit_ setting is 10K under """"Troubleshooting"""" (new) section  *Exception:* (reason to increase _ulimit_) 8:25:52,266 1.1.0.SNAP ERROR DeploymentsPathChildrenCache-0 server.DeploymentListener - Exception deploying module java.lang.IllegalStateException: java.io.FileNotFoundException: /var/vcap/data/packages/springxd/ee02bd3482eeb620a65862fb54e1f23fcece8022.1-bd a341640a5de2f922fd3db906ce504b85819c1e/spring-xd-1.1.0.BUILD-SNAPSHOT/xd/config/modules/modules.yml (Too many open files)""","""""""As a developer, I'd like to refer to wiki so that I can configure machines with recommended _ulimit_ setting for XD's distributed setup.  *Note:* Recommended _ulimit_ setting is 10K under """"""""Troubleshooting"""""""" (new) section  *Exception:* (reason to increase _ulimit_) 8:25:52,266 1.1.0.SNAP ERROR DeploymentsPathChildrenCache-0 server.DeploymentListener - Exception deploying module java.lang.IllegalStateException: java.io.FileNotFoundException: /var/vcap/data/packages/springxd/ee02bd3482eeb620a65862fb54e1f23fcece8022.1-bd a341640a5de2f922fd3db906ce504b85819c1e/spring-xd-1.1.0.BUILD-SNAPSHOT/xd/config/modules/modules.yml (Too many open files)""""""",,1.0
1,XD-2679,MAINTENANCE,Done,HIGH,"""Add dependencies needed for running a Hive job""","""Add dependencies needed for running a Hive job""","""Add dependencies needed for running a Hive job""",,3.0
1,XD-2678,MAINTENANCE,Done,MEDIUM,"""Add dependencies needed for running a Pig job""","""Add dependencies needed for running a Pig job""","""Add dependencies needed for running a Pig job""",,5.0
1,XD-2677,MAINTENANCE,Done,URGENT,"""Remove jline from xd-dirt classpath""","""Currently jline 2.11 gets added via zookeeper dependency, we need to remove this so we can have jline 1.0 fir Pig jobs in the hadoop depndencies  This jline version should remain for xd-shell classpath though""","""""""Currently jline 2.11 gets added via zookeeper dependency, we need to remove this so we can have jline 1.0 fir Pig jobs in the hadoop depndencies  This jline version should remain for xd-shell classpath though""""""",,1.0
1,XD-2676,FEATURE,Done,URGENT,"""Resolve classloading issues for custom Hadoop based batch jobs""","""There are several issues making it hard to impossible to create batch jobs that use Pig, Hive, HBase or other technologies supported by Spring for Apache Hadoop project. We need to make the corresponding dependencies available on the Hadoop classpath.""","""""""There are several issues making it hard to impossible to create batch jobs that use Pig, Hive, HBase or other technologies supported by Spring for Apache Hadoop project. We need to make the corresponding dependencies available on the Hadoop classpath.""""""",,8.0
1,XD-2674,IMPROVEMENT,Done,MEDIUM,"""Provide more options for the MongoDB Sink""","""See the SO question on the matter: http://stackoverflow.com/questions/28280206/how-can-i-use-authentication-in-mongo-sink""","""""""See the SO question on the matter: http://stackoverflow.com/questions/28280206/how-can-i-use-authentication-in-mongo-sink""""""",,1.0
1,XD-2673,MAINTENANCE,Done,MEDIUM,"""Create a smple to invoke Hive query from XD""","""As a user, I'd like to refer to Hive sample so that I can use that as a reference to integrate Hive to query and analyze.""","""""""As a user, I'd like to refer to Hive sample so that I can use that as a reference to integrate Hive to query and analyze.""""""",,2.0
1,XD-2672,FEATURE,Done,MEDIUM,"""Add scala support for spark streaming module""","""As a scala developer, someone could easily deploy the spark streaming module developed using scala.   ""","""""""As a scala developer, someone could easily deploy the spark streaming module developed using scala.   """"""",,8.0
1,XD-2671,BUG,Done,MEDIUM,"""Support rapid creation and deletion of streams""","""OS: Ubuntu Deployment Admin Server.  1 Container Transport Rabbit  * Using the shell execute the file.  ./bin/shell --cmdfile loop.txt   ** This should run successfully * Rerun the same script ./bin/shell --cmdfile loop.txt ** it states that the stream foo1 is already deployed.  ** Open shell app and destroy foo1 and run a stream list.  No streams will display ** Exit shell and rerun the file.   *** Another stream will fail to create stating  stream already exists.  * If you look at the modules deployed on the container some are still present. * To clear you have to shutdown the cluster and flush the xd directories on Zookeeper. * Note that if I shutdown the container the modules disappear.   ""","""""""OS: Ubuntu Deployment Admin Server.  1 Container Transport Rabbit  * Using the shell execute the file.  ./bin/shell --cmdfile loop.txt   ** This should run successfully * Rerun the same script ./bin/shell --cmdfile loop.txt ** it states that the stream foo1 is already deployed.  ** Open shell app and destroy foo1 and run a stream list.  No streams will display ** Exit shell and rerun the file.   *** Another stream will fail to create stating  stream already exists.  * If you look at the modules deployed on the container some are still present. * To clear you have to shutdown the cluster and flush the xd directories on Zookeeper. * Note that if I shutdown the container the modules disappear.   """"""",,8.0
1,XD-2670,FEATURE,Done,MEDIUM,"""Bump Spring AMQP to 1.4.3""","""When available""","""""""When available""""""",,1.0
1,XD-2668,BUG,Done,HIGH,"""Can't execute query on Hive""","""Hey Guys,  We are currently trying to connect Hive by spring-xd to create a table.  Unfortunatly we are not able to do it. We are using the attached job configuration (hive.xml) trying to connect by Hive Thrift Client with the following configuration:  <hadoop:hive-client-factory host=""""hive_host"""" port=""""10000"""" /> <hadoop:hive-tasklet id=""""hive-script"""">    <hadoop:script>      USE TESTING;      DROP TABLE IF EXITS testHiveBatchTable;       CREATE TABLE testHiveBatchTable (key int, value string);    </hadoop:script> </hadoop:hive-tasklet>  For Some reason spring XD is not finding the TTransportException class. The full log is in the singlenode_hivequery.log  file  attached. After that we tried uploading all the dependencies to the lib folder of the job, but now we are facing problems with log4j (full log is in singlenode_hivequery2.log attached) and we are unable to deploy the job in any of the two cases. Please,  Could you help us to solve this problem?   Thanks in advance.""","""""""Hey Guys,  We are currently trying to connect Hive by spring-xd to create a table.  Unfortunatly we are not able to do it. We are using the attached job configuration (hive.xml) trying to connect by Hive Thrift Client with the following configuration:  <hadoop:hive-client-factory host=""""""""hive_host"""""""" port=""""""""10000"""""""" /> <hadoop:hive-tasklet id=""""""""hive-script"""""""">    <hadoop:script>      USE TESTING;      DROP TABLE IF EXITS testHiveBatchTable;       CREATE TABLE testHiveBatchTable (key int, value string);    </hadoop:script> </hadoop:hive-tasklet>  For Some reason spring XD is not finding the TTransportException class. The full log is in the singlenode_hivequery.log  file  attached. After that we tried uploading all the dependencies to the lib folder of the job, but now we are facing problems with log4j (full log is in singlenode_hivequery2.log attached) and we are unable to deploy the job in any of the two cases. Please,  Could you help us to solve this problem?   Thanks in advance.""""""",,3.0
1,XD-2667,FEATURE,Done,MEDIUM,"""Upgrade to SHDP GA Release""","""As a developer, I'd like to upgrade to SHDP GA release so that I can sync -up with the latest bits. ""","""""""As a developer, I'd like to upgrade to SHDP GA release so that I can sync -up with the latest bits. """"""",,1.0
1,XD-2666,FEATURE,Done,MEDIUM,"""Upgrade to SI Kafka GA release""","""As a developer, I'd like to upgrade to Kafka's SI GA release so that I can sync -up with the latest bits.   The scope is to backport Kafka XD changes to SI Kafka and then upgrade to the GA release.""","""""""As a developer, I'd like to upgrade to Kafka's SI GA release so that I can sync -up with the latest bits.   The scope is to backport Kafka XD changes to SI Kafka and then upgrade to the GA release.""""""",,5.0
1,XD-2665,FEATURE,Done,MEDIUM,"""Update copyright message in PDF from 2014 to 2015""","""As a PM, I'd like to have the copyright message in the reference guide (PDF) updated to include 2015 instead of 2014. ""","""""""As a PM, I'd like to have the copyright message in the reference guide (PDF) updated to include 2015 instead of 2014. """"""",,1.0
1,XD-2664,FEATURE,Done,MEDIUM,"""Create Spark Streaming example""","""As a developer, I'd like to build _Spark Streaming_ as data processors in XD so that we can demonstrate some of the capabilities.  *Implement using:* * Java / Java Lambdas * Scala""","""""""As a developer, I'd like to build _Spark Streaming_ as data processors in XD so that we can demonstrate some of the capabilities.  *Implement using:* * Java / Java Lambdas * Scala""""""",,8.0
1,XD-2661,FEATURE,Done,MEDIUM,"""Add build support for XD in Windows""","""As a user, I'd like to build XD in Windows machine so that I can develop, test,  and contributed to OSS.""","""""""As a user, I'd like to build XD in Windows machine so that I can develop, test,  and contributed to OSS.""""""",,5.0
1,XD-2656,FEATURE,Done,MEDIUM,"""Create a sample to invoke Pig script/job from XD""","""As a user, I'd like to refer to a Pig script/job sample so that I can use that as a reference to integrate Pig jobs in XD.""","""""""As a user, I'd like to refer to a Pig script/job sample so that I can use that as a reference to integrate Pig jobs in XD.""""""",,5.0
1,XD-2655,IMPROVEMENT,Done,MEDIUM,"""Make deployment timeout configurable""","""Currently, ModuleDeploymentWriter uses default timeout 30s which can not be overridden. We need to make this configurable.""","""""""Currently, ModuleDeploymentWriter uses default timeout 30s which can not be overridden. We need to make this configurable.""""""",,1.0
1,XD-2654,FEATURE,Done,MEDIUM,"""Update com.jayway.jsonpath to latest version""","""Use latest version, might need to exclude version from other dependencies, e.g. SI, in build-common.gradle.""","""""""Use latest version, might need to exclude version from other dependencies, e.g. SI, in build-common.gradle.""""""",,1.0
1,XD-2652,FEATURE,Done,MEDIUM,"""Document migration strategy for custom modules (from 1.0 to 1.1)""","""As a user, I'd like to migrate from 1.0 to 1.1 and be able to port my custom modules so that I can operationalize existing data pipelines and also take advantage of latest XD features.""","""""""As a user, I'd like to migrate from 1.0 to 1.1 and be able to port my custom modules so that I can operationalize existing data pipelines and also take advantage of latest XD features.""""""",,2.0
1,XD-2651,FEATURE,Done,HIGH,"""Spike: Research request/reply support to Kafka Message Bus""","""The scope is to research the available options to provide request/reply support for Kafka.   * Document findings * POCs  Previous Desc: The bindRequestor and bindReplier methods of the message bus need to be implemented.""","""""""The scope is to research the available options to provide request/reply support for Kafka.   * Document findings * POCs  Previous Desc: The bindRequestor and bindReplier methods of the message bus need to be implemented.""""""",,5.0
1,XD-2649,FEATURE,Done,MEDIUM,"""Add Kafka-based implementation for AbstractSingleNodeStreamDeploymentIntegrationTests""","""Add Kafka-based implementation for AbstractSingleNodeStreamDeploymentIntegrationTests""","""Add Kafka-based implementation for AbstractSingleNodeStreamDeploymentIntegrationTests""",,2.0
1,XD-2648,FEATURE,Done,MEDIUM,"""We Need a KafkaSingleNodeStreamDeploymentIntegrationTests""","""See {{RedisSingleNodeStreamDeploymentIntegrationTests}} etc.""","""""""See {{RedisSingleNodeStreamDeploymentIntegrationTests}} etc.""""""",,3.0
1,XD-2646,BUG,Done,MEDIUM,"""XD should use same hadoop security keys as Spring for Apache Hadoop""","""For kerberos and other security related settings we use keys like 'spring.hadoop.userPrincipal' mentioned in https://github.com/spring-projects/spring-xd/wiki/Hadoop-Kerberos. However when we added boot config props to shdp, we used a sub keys like 'spring.hadoop.security.userPrincipal'.  It'd be good if we'd fix these to be same in both XD and SHDP not to cause confusion.""","""""""For kerberos and other security related settings we use keys like 'spring.hadoop.userPrincipal' mentioned in https://github.com/spring-projects/spring-xd/wiki/Hadoop-Kerberos. However when we added boot config props to shdp, we used a sub keys like 'spring.hadoop.security.userPrincipal'.  It'd be good if we'd fix these to be same in both XD and SHDP not to cause confusion.""""""",,1.0
1,XD-2645,BUG,Done,MEDIUM,"""servers.yaml has a typo that causes XD startup to fail when....""","""Line 295 has 2 dashes instead of 3.  There is no effect on XD unless the XD extensions block is uncommented.  I have been using the XD extensions in version 1.0.3 and have merged my servers.yaml with all of the new content in 1.1.0.M2.  The Admin JVM crahes without logging any exception and the container JVM crashes and logs the following:  /  ___|          (-)             \ \ / /  _  \ \ `--. _ __  _ __ _ _ __   __ _   \ V /| | | | `--. \ '_ \| '__| | '_ \ / _` |  / ^ \| | | | /\__/ / |_) | |  | | | | | (_| | / / \ \ |/ / \____/| .__/|_|  |_|_| |_|\__, | \/   \/___/       | |                  __/ |       |_|                 |___/ 1.1.0.M2                         eXtreme Data   Started : ContainerServerApplication Documentation: https://github.com/spring-projects/spring-xd/wiki  expected '<document start>', but found BlockMappingStart in 'reader', line 303, column 1:     xd:     ^          at org.yaml.snakeyaml.parser.ParserImpl$ParseDocumentStart.produce(ParserImpl.java:225)         at org.yaml.snakeyaml.parser.ParserImpl.peekEvent(ParserImpl.java:158)         at org.yaml.snakeyaml.parser.ParserImpl.checkEvent(ParserImpl.java:143)         at org.yaml.snakeyaml.composer.Composer.checkNode(Composer.java:68)         at org.yaml.snakeyaml.constructor.BaseConstructor.checkData(BaseConstructor.java:93)         at org.yaml.snakeyaml.Yaml$1.hasNext(Yaml.java:498)         at org.springframework.beans.factory.config.YamlProcessor.process(YamlProcessor.java:152)         at org.springframework.beans.factory.config.YamlProcessor.process(YamlProcessor.java:137)         at org.springframework.boot.env.YamlPropertySourceLoader$Processor.process(YamlPropertySourceLoader.java:78)         at org.springframework.boot.env.YamlPropertySourceLoader.load(YamlPropertySourceLoader.java:50)         at org.springframework.boot.env.PropertySourcesLoader.load(PropertySourcesLoader.java:126)         at org.springframework.boot.context.config.ConfigFileApplicationListener$Loader.loadIntoGroup(ConfigFileApplicationListener.java:378)         at org.springframework.boot.context.config.ConfigFileApplicationListener$Loader.load(ConfigFileApplicationListener.java:366)         at org.springframework.boot.context.config.ConfigFileApplicationListener$Loader.load(ConfigFileApplicationListener.java:336)         at org.springframework.boot.context.config.ConfigFileApplicationListener.addPropertySources(ConfigFileApplicationListener.java:173)         at org.springframework.boot.context.config.ConfigFileApplicationListener.onApplicationEnvironmentPreparedEvent(ConfigFileApplicationListener.java:144)         at org.springframework.boot.context.config.ConfigFileApplicationListener.onApplicationEnvironmentPreparedEvent(ConfigFileApplicationListener.java:137)         at org.springframework.boot.context.config.ConfigFileApplicationListener.onApplicationEvent(ConfigFileApplicationListener.java:126)         at org.springframework.context.event.SimpleApplicationEventMulticaster.invokeListener(SimpleApplicationEventMulticaster.java:151)         at org.springframework.context.event.SimpleApplicationEventMulticaster.multicastEvent(SimpleApplicationEventMulticaster.java:128)         at org.springframework.boot.context.event.EventPublishingRunListener.publishEvent(EventPublishingRunListener.java:100)         at org.springframework.boot.context.event.EventPublishingRunListener.environmentPrepared(EventPublishingRunListener.java:59)         at org.springframework.boot.SpringApplication.run(SpringApplication.java:286)         at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:139)         at org.springframework.xd.dirt.server.ContainerBootstrapContext.<init>(ContainerBootstrapContext.java:48)         at org.springframework.xd.dirt.server.ContainerServerApplication.run(ContainerServerApplication.java:83)         at org.springframework.xd.dirt.server.ContainerServerApplication.main(ContainerServerApplication.java:72)     ""","""""""Line 295 has 2 dashes instead of 3.  There is no effect on XD unless the XD extensions block is uncommented.  I have been using the XD extensions in version 1.0.3 and have merged my servers.yaml with all of the new content in 1.1.0.M2.  The Admin JVM crahes without logging any exception and the container JVM crashes and logs the following:  /  ___|          (-)             \ \ / /  _  \ \ `--. _ __  _ __ _ _ __   __ _   \ V /| | | | `--. \ '_ \| '__| | '_ \ / _` |  / ^ \| | | | /\__/ / |_) | |  | | | | | (_| | / / \ \ |/ / \____/| .__/|_|  |_|_| |_|\__, | \/   \/___/       | |                  __/ |       |_|                 |___/ 1.1.0.M2                         eXtreme Data   Started : ContainerServerApplication Documentation: https://github.com/spring-projects/spring-xd/wiki  expected '<document start>', but found BlockMappingStart in 'reader', line 303, column 1:     xd:     ^          at org.yaml.snakeyaml.parser.ParserImpl$ParseDocumentStart.produce(ParserImpl.java:225)         at org.yaml.snakeyaml.parser.ParserImpl.peekEvent(ParserImpl.java:158)         at org.yaml.snakeyaml.parser.ParserImpl.checkEvent(ParserImpl.java:143)         at org.yaml.snakeyaml.composer.Composer.checkNode(Composer.java:68)         at org.yaml.snakeyaml.constructor.BaseConstructor.checkData(BaseConstructor.java:93)         at org.yaml.snakeyaml.Yaml$1.hasNext(Yaml.java:498)         at org.springframework.beans.factory.config.YamlProcessor.process(YamlProcessor.java:152)         at org.springframework.beans.factory.config.YamlProcessor.process(YamlProcessor.java:137)         at org.springframework.boot.env.YamlPropertySourceLoader$Processor.process(YamlPropertySourceLoader.java:78)         at org.springframework.boot.env.YamlPropertySourceLoader.load(YamlPropertySourceLoader.java:50)         at org.springframework.boot.env.PropertySourcesLoader.load(PropertySourcesLoader.java:126)         at org.springframework.boot.context.config.ConfigFileApplicationListener$Loader.loadIntoGroup(ConfigFileApplicationListener.java:378)         at org.springframework.boot.context.config.ConfigFileApplicationListener$Loader.load(ConfigFileApplicationListener.java:366)         at org.springframework.boot.context.config.ConfigFileApplicationListener$Loader.load(ConfigFileApplicationListener.java:336)         at org.springframework.boot.context.config.ConfigFileApplicationListener.addPropertySources(ConfigFileApplicationListener.java:173)         at org.springframework.boot.context.config.ConfigFileApplicationListener.onApplicationEnvironmentPreparedEvent(ConfigFileApplicationListener.java:144)         at org.springframework.boot.context.config.ConfigFileApplicationListener.onApplicationEnvironmentPreparedEvent(ConfigFileApplicationListener.java:137)         at org.springframework.boot.context.config.ConfigFileApplicationListener.onApplicationEvent(ConfigFileApplicationListener.java:126)         at org.springframework.context.event.SimpleApplicationEventMulticaster.invokeListener(SimpleApplicationEventMulticaster.java:151)         at org.springframework.context.event.SimpleApplicationEventMulticaster.multicastEvent(SimpleApplicationEventMulticaster.java:128)         at org.springframework.boot.context.event.EventPublishingRunListener.publishEvent(EventPublishingRunListener.java:100)         at org.springframework.boot.context.event.EventPublishingRunListener.environmentPrepared(EventPublishingRunListener.java:59)         at org.springframework.boot.SpringApplication.run(SpringApplication.java:286)         at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:139)         at org.springframework.xd.dirt.server.ContainerBootstrapContext.<init>(ContainerBootstrapContext.java:48)         at org.springframework.xd.dirt.server.ContainerServerApplication.run(ContainerServerApplication.java:83)         at org.springframework.xd.dirt.server.ContainerServerApplication.main(ContainerServerApplication.java:72)     """"""",,1.0
1,XD-2643,FEATURE,Done,HIGH,"""Acceptance tests need to create or use an existing topic prior to executing test""","""Acceptance tests need to create or use an existing topic prior to executing test""","""Acceptance tests need to create or use an existing topic prior to executing test""",,3.0
1,XD-2642,FEATURE,Done,HIGH,"""Can't run WordCount example in hadoop kerberized cluster""","""Hey Guys,  Im trying to run WordCount example in kerberized cluster with the attached job configuration. The job upload the test file to the HDFS without problems but it fails in wordcount step. I am running the example on singlenode and  I configured the config/hadoop.properties file for shell and container with kerberos setting such as https://github.com/spring-projects/spring-xd/wiki/Hadoop-Kerberos.  The error log is attached.  Thanks in advance for the help. Regards""","""""""Hey Guys,  Im trying to run WordCount example in kerberized cluster with the attached job configuration. The job upload the test file to the HDFS without problems but it fails in wordcount step. I am running the example on singlenode and  I configured the config/hadoop.properties file for shell and container with kerberos setting such as https://github.com/spring-projects/spring-xd/wiki/Hadoop-Kerberos.  The error log is attached.  Thanks in advance for the help. Regards""""""",,3.0
1,XD-2641,IMPROVEMENT,Done,MEDIUM,"""Validate that topics used for storing offsets are compacted""","""Validate that topics used for storing offsets are compacted""","""Validate that topics used for storing offsets are compacted""",,1.0
1,XD-2638,BUG,Done,URGENT,"""The shell distribution zip is missing hadoop26 libraries""","""The spring-xd-[version]-shell.zip distribution zip doesn't include the lib/hadoop26 directory and libraries, so we get the following exception when starting the shell:  Exception in thread """"main"""" java.lang.NoClassDefFoundError: org/apache/hadoop/conf/Configuration ""","""""""The spring-xd-[version]-shell.zip distribution zip doesn't include the lib/hadoop26 directory and libraries, so we get the following exception when starting the shell:  Exception in thread """"""""main"""""""" java.lang.NoClassDefFoundError: org/apache/hadoop/conf/Configuration """"""",,1.0
1,XD-2637,FEATURE,Done,MEDIUM,"""Shell processor is not thread safe""","""Multiple threads invoke the shell processor result in I/O errors and/or data corruption. send() and receive() should be synchronized. ""","""""""Multiple threads invoke the shell processor result in I/O errors and/or data corruption. send() and receive() should be synchronized. """"""",,1.0
1,XD-2635,FEATURE,Done,MEDIUM,"""Update 'partitionColumn' wiki to include mutual exclusiveness of 'sql' and 'tableName' options""","""As a user, I'd like to refer to the wiki so that I can create a job with 'partitions' that in turn expects _tableName_ and _columns_ be explicitly included in the job definition.   It is also beneficial to call-out _sql_ and _tableName_ metadata options are mutually exclusive. Following logic in _JdbcHdfsOptionsMetadata_ needs documented.  {code:java} @AssertTrue(message = """"Use ('tableName' AND 'columns') when using partition column"""") boolean isPartitionedWithTableName() {  if (StringUtils.hasText(partitionColumn)) {   return StringUtils.hasText(tableName) && !StringUtils.hasText(sql);  }  else {   return true;  } } {code}  ""","""""""As a user, I'd like to refer to the wiki so that I can create a job with 'partitions' that in turn expects _tableName_ and _columns_ be explicitly included in the job definition.   It is also beneficial to call-out _sql_ and _tableName_ metadata options are mutually exclusive. Following logic in _JdbcHdfsOptionsMetadata_ needs documented.    """"""",""" @AssertTrue(message = """"""""Use ('tableName' AND 'columns') when using partition column"""""""") boolean isPartitionedWithTableName() {  if (StringUtils.hasText(partitionColumn)) {   return StringUtils.hasText(tableName) && !StringUtils.hasText(sql);  }  else {   return true;  } } """,1.0
1,XD-2631,FEATURE,Done,MEDIUM,"""For time being checkin UI build artifacts""","""For time being checkin UI build artifacts""","""For time being checkin UI build artifacts""",,1.0
1,XD-2630,FEATURE,Done,HIGH,"""Add Ambari plugin (beta) to build and install Spring XD""","""As a developer, I'd like to use Ambari plugin so that I can provision, manage, and monitor Spring XD cluster using the same tool I use for Hadoop clusters.""","""""""As a developer, I'd like to use Ambari plugin so that I can provision, manage, and monitor Spring XD cluster using the same tool I use for Hadoop clusters.""""""",,5.0
1,XD-2628,FEATURE,Done,HIGH,"""Test Redis Sentinel setup and document recommended configuration""","""Configure Redis Cluster with Sentinal v 2.8.19.   Verify fail over, experiment with settings.  Useful reference https://code.flickr.net/2014/07/31/redis-sentinel-at-flickr/  All analytics test cases should be run as well as test that deploy streams that make use of redis analytics.   There might be some minor code changes required as mentioned in the flickr article.""","""""""Configure Redis Cluster with Sentinal v 2.8.19.   Verify fail over, experiment with settings.  Useful reference https://code.flickr.net/2014/07/31/redis-sentinel-at-flickr/  All analytics test cases should be run as well as test that deploy streams that make use of redis analytics.   There might be some minor code changes required as mentioned in the flickr article.""""""",,5.0
1,XD-2626,IMPROVEMENT,Done,MEDIUM,"""Update deployment guide to include verbose gc""","""The deployment guide https://github.com/spring-projects/spring-xd/wiki/Deployment should have instructions on turning on verbose gc for production applications.  The gc log tends to be verbose so running it in development is not desirable. However having the gc log in production is very helpful for troubleshooting slow/unresponsive applications.""","""""""The deployment guide https://github.com/spring-projects/spring-xd/wiki/Deployment should have instructions on turning on verbose gc for production applications.  The gc log tends to be verbose so running it in development is not desirable. However having the gc log in production is very helpful for troubleshooting slow/unresponsive applications.""""""",,1.0
1,XD-2625,IMPROVEMENT,Done,MEDIUM,"""Add ZooKeeper timeout options to configuration file""","""The following timeout options should be added to our configuration file:  | session timeout | the number of milliseconds before the ZooKeeper session times out | | connection timeout | the number of milliseconds before a connection to a ZooKeeper server times out | | retry wait period | number of milliseconds to wait before attempting a connection after a failed connection | | max retries | number of times to attempt a connection after a failed connection |  ""","""""""The following timeout options should be added to our configuration file:  | session timeout | the number of milliseconds before the ZooKeeper session times out | | connection timeout | the number of milliseconds before a connection to a ZooKeeper server times out | | retry wait period | number of milliseconds to wait before attempting a connection after a failed connection | | max retries | number of times to attempt a connection after a failed connection |  """"""",,2.0
1,XD-2624,IMPROVEMENT,Done,MEDIUM,"""Add more comprehensive tests for the simple consumer-based Kafka Message Bus""","""Add more comprehensive tests for the simple consumer-based Kafka Message Bus""","""Add more comprehensive tests for the simple consumer-based Kafka Message Bus""",,3.0
1,XD-2622,IMPROVEMENT,Done,MEDIUM,"""Update documentation for Kafka sources  ""","""Include new features added by using Spring Integration Kafka M3""","""""""Include new features added by using Spring Integration Kafka M3""""""",,1.0
1,XD-2621,FEATURE,Done,MEDIUM,"""Add Kafka Native Metadata Store""","""Add Kafka Native Metadata Store""","""Add Kafka Native Metadata Store""",,3.0
1,XD-2618,FEATURE,Done,MEDIUM,"""Update Spring Batch to 3.0.3.RELEASE""","""Update Spring Batch to 3.0.3.RELEASE""","""Update Spring Batch to 3.0.3.RELEASE""",,1.0
1,XD-2617,FEATURE,Done,HIGH,"""Create reactor module in spring-xd-modules project""","""Create reactor module in spring-xd-modules project""","""Create reactor module in spring-xd-modules project""",,3.0
1,XD-2616,BUG,Done,MEDIUM,"""Ensure that metadata for Kafka message bus is propagated before producing/consuming""","""Currently, `ensureTopicCreated` will invoke the creation of the topic on the brokers, however, the calls is not blocking. So, before proceeding, we should make sure that the metadata is readable (therefore propagated)""","""""""Currently, `ensureTopicCreated` will invoke the creation of the topic on the brokers, however, the calls is not blocking. So, before proceeding, we should make sure that the metadata is readable (therefore propagated)""""""",,3.0
1,XD-2612,FEATURE,Done,MEDIUM,"""Use Kryo instance pooling to reduce instantiation overhead""","""https://github.com/EsotericSoftware/kryo#pooling-kryo-instances""","""""""https://github.com/EsotericSoftware/kryo#pooling-kryo-instances""""""",,1.0
1,XD-2611,BUG,Done,MEDIUM,"""Missing Log Configuration for throughput-sampler""","""http://stackoverflow.com/questions/28019801/how-to-read-throughput-sampler-sink-values-in-spring-xd/28027544#28027544""","""""""http://stackoverflow.com/questions/28019801/how-to-read-throughput-sampler-sink-values-in-spring-xd/28027544#28027544""""""",,1.0
1,XD-2610,BUG,Done,URGENT,"""Job definition is deleted after restart the srping xd service in single node mode""","""Job definition is deleted after restart the srping xd service in single node mode  repro step: 1.start service as single node 2.create a batch module 3.create a job based on batch module 4.restart service  expect result: job definition is displayed on the job list  actual result: job list is empty, all job definitions are missed""","""""""Job definition is deleted after restart the srping xd service in single node mode  repro step: 1.start service as single node 2.create a batch module 3.create a job based on batch module 4.restart service  expect result: job definition is displayed on the job list  actual result: job list is empty, all job definitions are missed""""""",,2.0
1,XD-2609,BUG,Done,HIGH,"""Error when listing Streams in admin-ui""","""As a user, I'm trying to list streams (>20) in admin-ui to use the pagination; however, I ended up with blank page and the server-side errored with _java.lang.IllegalStateException_.  Version: 1.1.0 SNAPSHOT (master build) Distributed: 1 admin and 2 containers  *Steps to reproduce:* 1) Deploy the following streams. stream create foo1 --definition """"time | log"""" --deploy stream create foo2 --definition """"time | log"""" --deploy stream create foo3 --definition """"time | log"""" --deploy stream create foo4 --definition """"time | log"""" --deploy stream create foo5 --definition """"time | log"""" --deploy stream create foo6 --definition """"time | log"""" --deploy stream create foo7 --definition """"time | log"""" --deploy stream create foo8 --definition """"time | log"""" --deploy stream create foo9 --definition """"time | log"""" --deploy stream create foo10 --definition """"time | log"""" --deploy stream create foo11 --definition """"time | log"""" --deploy stream create foo12 --definition """"time | log"""" --deploy stream create foo13 --definition """"time | log"""" --deploy stream create foo14 --definition """"time | log"""" --deploy stream create foo15 --definition """"time | log"""" --deploy stream create foo16 --definition """"time | log"""" --deploy stream create foo17 --definition """"time | log"""" --deploy stream create foo18 --definition """"time | log"""" --deploy stream create foo19 --definition """"time | log"""" --deploy stream create foo20 --definition """"time | log"""" --deploy stream create foo21 --definition """"time | log"""" --deploy stream create foo22 --definition """"time | log"""" --deploy  2) Go to Streams tab in admin-ui to get a blank page and the following exception in admin logs.  *Error:* 16:55:19,107 1.1.0.SNAP ERROR http-nio-9393-exec-2 rest.RestControllerAdvice - Caught exception while handling a request java.lang.IllegalStateException: Not all instances were looked at  at org.springframework.util.Assert.state(Assert.java:385)  at org.springframework.xd.dirt.rest.XDController.enhanceWithDeployments(XDController.java:207)  at org.springframework.xd.dirt.rest.XDController.listValues(XDController.java:178)  at org.springframework.xd.dirt.rest.StreamsController.list(StreamsController.java:63) ""","""""""As a user, I'm trying to list streams (>20) in admin-ui to use the pagination; however, I ended up with blank page and the server-side errored with _java.lang.IllegalStateException_.  Version: 1.1.0 SNAPSHOT (master build) Distributed: 1 admin and 2 containers  *Steps to reproduce:* 1) Deploy the following streams. stream create foo1 --definition """"""""time | log"""""""" --deploy stream create foo2 --definition """"""""time | log"""""""" --deploy stream create foo3 --definition """"""""time | log"""""""" --deploy stream create foo4 --definition """"""""time | log"""""""" --deploy stream create foo5 --definition """"""""time | log"""""""" --deploy stream create foo6 --definition """"""""time | log"""""""" --deploy stream create foo7 --definition """"""""time | log"""""""" --deploy stream create foo8 --definition """"""""time | log"""""""" --deploy stream create foo9 --definition """"""""time | log"""""""" --deploy stream create foo10 --definition """"""""time | log"""""""" --deploy stream create foo11 --definition """"""""time | log"""""""" --deploy stream create foo12 --definition """"""""time | log"""""""" --deploy stream create foo13 --definition """"""""time | log"""""""" --deploy stream create foo14 --definition """"""""time | log"""""""" --deploy stream create foo15 --definition """"""""time | log"""""""" --deploy stream create foo16 --definition """"""""time | log"""""""" --deploy stream create foo17 --definition """"""""time | log"""""""" --deploy stream create foo18 --definition """"""""time | log"""""""" --deploy stream create foo19 --definition """"""""time | log"""""""" --deploy stream create foo20 --definition """"""""time | log"""""""" --deploy stream create foo21 --definition """"""""time | log"""""""" --deploy stream create foo22 --definition """"""""time | log"""""""" --deploy  2) Go to Streams tab in admin-ui to get a blank page and the following exception in admin logs.  *Error:* 16:55:19,107 1.1.0.SNAP ERROR http-nio-9393-exec-2 rest.RestControllerAdvice - Caught exception while handling a request java.lang.IllegalStateException: Not all instances were looked at  at org.springframework.util.Assert.state(Assert.java:385)  at org.springframework.xd.dirt.rest.XDController.enhanceWithDeployments(XDController.java:207)  at org.springframework.xd.dirt.rest.XDController.listValues(XDController.java:178)  at org.springframework.xd.dirt.rest.StreamsController.list(StreamsController.java:63) """"""",,1.0
1,XD-2608,BUG,Done,HIGH,"""XD Gemfire modules fail to deploy in  Yarn""","""1 admin on slave1 1 container on slave2  Gemfire modules fail to deploy.  with the following exception: Caused by: java.io.FileNotFoundException: null/modules/common/gemfire-sink.groovy This is because the modules require a XD_HOME environment variable and this is not set by the yarn deployment.   {noformat} Offending resource: URL [file:null/modules/common/gemfire-sink.groovy]; nested exception is java.io.FileNotFoundException: null/modules/common/gemfire-sink.groovy (No such file or directory)  at org.springframework.beans.factory.groovy.GroovyBeanDefinitionReader.loadBeanDefinitions(GroovyBeanDefinitionReader.java:247)  at org.springframework.beans.factory.groovy.GroovyBeanDefinitionReader.loadBeanDefinitions(GroovyBeanDefinitionReader.java:202)  at org.springframework.boot.BeanDefinitionLoader.load(BeanDefinitionLoader.java:178)  at org.springframework.boot.BeanDefinitionLoader.load(BeanDefinitionLoader.java:138)  at org.springframework.boot.BeanDefinitionLoader.load(BeanDefinitionLoader.java:127)  at org.springframework.boot.SpringApplication.load(SpringApplication.java:620)  at org.springframework.boot.SpringApplication.run(SpringApplication.java:315)  at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:139)  at org.springframework.xd.module.core.SimpleModule.initialize(SimpleModule.java:211)  at org.springframework.xd.dirt.module.ModuleDeployer.doDeploy(ModuleDeployer.java:217)  at org.springframework.xd.dirt.module.ModuleDeployer.deploy(ModuleDeployer.java:200)  at org.springframework.xd.dirt.server.DeploymentListener.deployModule(DeploymentListener.java:363)  at org.springframework.xd.dirt.server.DeploymentListener.deployStreamModule(DeploymentListener.java:332)  at org.springframework.xd.dirt.server.DeploymentListener.onChildAdded(DeploymentListener.java:179)  at org.springframework.xd.dirt.server.DeploymentListener.childEvent(DeploymentListener.java:147)  at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:509)  at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:503)  at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)  at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297)  at org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:83)  at org.apache.curator.framework.recipes.cache.PathChildrenCache.callListeners(PathChildrenCache.java:500)  at org.apache.curator.framework.recipes.cache.EventOperation.invoke(EventOperation.java:35)  at org.apache.curator.framework.recipes.cache.PathChildrenCache$10.run(PathChildrenCache.java:762)  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)  at java.util.concurrent.FutureTask.run(FutureTask.java:262)  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)  at java.util.concurrent.FutureTask.run(FutureTask.java:262)  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)  at java.lang.Thread.run(Thread.java:745) Caused by: org.springframework.beans.factory.parsing.BeanDefinitionParsingException: Configuration problem: Error evaluating Groovy script: null/modules/common/gemfire-sink.groovy (No such file or directory) Offending resource: URL [file:null/modules/common/gemfire-sink.groovy]; nested exception is java.io.FileNotFoundException: null/modules/common/gemfire-sink.groovy (No such file or directory)  at org.springframework.beans.factory.groovy.GroovyBeanDefinitionReader.loadBeanDefinitions(GroovyBeanDefinitionReader.java:247)  at org.springframework.beans.factory.groovy.GroovyBeanDefinitionReader.loadBeanDefinitions(GroovyBeanDefinitionReader.java:202)  at org.springframework.beans.factory.support.AbstractBeanDefinitionReader.loadBeanDefinitions(AbstractBeanDefinitionReader.java:181)  at org.springframework.beans.factory.support.AbstractBeanDefinitionReader.loadBeanDefinitions(AbstractBeanDefinitionReader.java:217)  at org.springframework.beans.factory.support.AbstractBeanDefinitionReader.loadBeanDefinitions(AbstractBeanDefinitionReader.java:188)  at org.springframework.beans.factory.groovy.GroovyBeanDefinitionReader.importBeans(GroovyBeanDefinitionReader.java:337)  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)  at java.lang.reflect.Method.invoke(Method.java:606)  at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:90)  at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:324)  at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:368)  at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1016)  at org.codehaus.groovy.runtime.callsite.PogoMetaClassSite.callCurrent(PogoMetaClassSite.java:66)  at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCallCurrent(CallSiteArray.java:49)  at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:133)  at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:141)  at beans$_run_closure1.doCall(beans:4)  at beans$_run_closure1.doCall(beans)  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)  at java.lang.reflect.Method.invoke(Method.java:606)  at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:90)  at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:324)  at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:278)  at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1016)  at groovy.lang.Closure.call(Closure.java:423)  at groovy.lang.Closure.call(Closure.java:417)  at org.springframework.beans.factory.groovy.GroovyBeanDefinitionReader.invokeBeanDefiningClosure(GroovyBeanDefinitionReader.java:426)  at org.springframework.beans.factory.groovy.GroovyBeanDefinitionReader$1.call(GroovyBeanDefinitionReader.java:223)  at groovy.lang.Closure.call(Closure.java:439)  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)  at java.lang.reflect.Method.invoke(Method.java:606)  at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:90)  at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:324)  at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1207)  at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1016)  at groovy.lang.MetaClassImpl.invokePropertyOrMissing(MetaClassImpl.java:1253)  at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1209)  at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1016)  at org.codehaus.groovy.runtime.callsite.PogoMetaClassSite.callCurrent(PogoMetaClassSite.java:66)  at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCallCurrent(CallSiteArray.java:49)  at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:133)  at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:141)  at beans.run(beans:1)  at groovy.lang.GroovyShell.evaluate(GroovyShell.java:649)  at org.springframework.beans.factory.groovy.GroovyBeanDefinitionReader.loadBeanDefinitions(GroovyBeanDefinitionReader.java:243)  ... 29 more Caused by: java.io.FileNotFoundException: null/modules/common/gemfire-sink.groovy (No such file or directory)  at java.io.FileInputStream.open(Native Method)  at java.io.FileInputStream.<init>(FileInputStream.java:146)  at java.io.FileInputStream.<init>(FileInputStream.java:101)  at sun.net.www.protocol.file.FileURLConnection.connect(FileURLConnection.java:90)  at sun.net.www.protocol.file.FileURLConnection.getInputStream(FileURLConnection.java:188)  at org.springframework.core.io.UrlResource.getInputStream(UrlResource.java:168)  at org.springframework.core.io.support.EncodedResource.getReader(EncodedResource.java:132)  at org.springframework.beans.factory.groovy.GroovyBeanDefinitionReader.loadBeanDefinitions(GroovyBeanDefinitionReader.java:243)  ... 79 more {noformat}""","""""""1 admin on slave1 1 container on slave2  Gemfire modules fail to deploy.  with the following exception: Caused by: java.io.FileNotFoundException: null/modules/common/gemfire-sink.groovy This is because the modules require a XD_HOME environment variable and this is not set by the yarn deployment.   """"""",""" Offending resource: URL [file:null/modules/common/gemfire-sink.groovy]; nested exception is java.io.FileNotFoundException: null/modules/common/gemfire-sink.groovy (No such file or directory)  at org.springframework.beans.factory.groovy.GroovyBeanDefinitionReader.loadBeanDefinitions(GroovyBeanDefinitionReader.java:247)  at org.springframework.beans.factory.groovy.GroovyBeanDefinitionReader.loadBeanDefinitions(GroovyBeanDefinitionReader.java:202)  at org.springframework.boot.BeanDefinitionLoader.load(BeanDefinitionLoader.java:178)  at org.springframework.boot.BeanDefinitionLoader.load(BeanDefinitionLoader.java:138)  at org.springframework.boot.BeanDefinitionLoader.load(BeanDefinitionLoader.java:127)  at org.springframework.boot.SpringApplication.load(SpringApplication.java:620)  at org.springframework.boot.SpringApplication.run(SpringApplication.java:315)  at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:139)  at org.springframework.xd.module.core.SimpleModule.initialize(SimpleModule.java:211)  at org.springframework.xd.dirt.module.ModuleDeployer.doDeploy(ModuleDeployer.java:217)  at org.springframework.xd.dirt.module.ModuleDeployer.deploy(ModuleDeployer.java:200)  at org.springframework.xd.dirt.server.DeploymentListener.deployModule(DeploymentListener.java:363)  at org.springframework.xd.dirt.server.DeploymentListener.deployStreamModule(DeploymentListener.java:332)  at org.springframework.xd.dirt.server.DeploymentListener.onChildAdded(DeploymentListener.java:179)  at org.springframework.xd.dirt.server.DeploymentListener.childEvent(DeploymentListener.java:147)  at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:509)  at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:503)  at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)  at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297)  at org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:83)  at org.apache.curator.framework.recipes.cache.PathChildrenCache.callListeners(PathChildrenCache.java:500)  at org.apache.curator.framework.recipes.cache.EventOperation.invoke(EventOperation.java:35)  at org.apache.curator.framework.recipes.cache.PathChildrenCache$10.run(PathChildrenCache.java:762)  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)  at java.util.concurrent.FutureTask.run(FutureTask.java:262)  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)  at java.util.concurrent.FutureTask.run(FutureTask.java:262)  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)  at java.lang.Thread.run(Thread.java:745) Caused by: org.springframework.beans.factory.parsing.BeanDefinitionParsingException: Configuration problem: Error evaluating Groovy script: null/modules/common/gemfire-sink.groovy (No such file or directory) Offending resource: URL [file:null/modules/common/gemfire-sink.groovy]; nested exception is java.io.FileNotFoundException: null/modules/common/gemfire-sink.groovy (No such file or directory)  at org.springframework.beans.factory.groovy.GroovyBeanDefinitionReader.loadBeanDefinitions(GroovyBeanDefinitionReader.java:247)  at org.springframework.beans.factory.groovy.GroovyBeanDefinitionReader.loadBeanDefinitions(GroovyBeanDefinitionReader.java:202)  at org.springframework.beans.factory.support.AbstractBeanDefinitionReader.loadBeanDefinitions(AbstractBeanDefinitionReader.java:181)  at org.springframework.beans.factory.support.AbstractBeanDefinitionReader.loadBeanDefinitions(AbstractBeanDefinitionReader.java:217)  at org.springframework.beans.factory.support.AbstractBeanDefinitionReader.loadBeanDefinitions(AbstractBeanDefinitionReader.java:188)  at org.springframework.beans.factory.groovy.GroovyBeanDefinitionReader.importBeans(GroovyBeanDefinitionReader.java:337)  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)  at java.lang.reflect.Method.invoke(Method.java:606)  at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:90)  at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:324)  at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:368)  at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1016)  at org.codehaus.groovy.runtime.callsite.PogoMetaClassSite.callCurrent(PogoMetaClassSite.java:66)  at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCallCurrent(CallSiteArray.java:49)  at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:133)  at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:141)  at beans$_run_closure1.doCall(beans:4)  at beans$_run_closure1.doCall(beans)  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)  at java.lang.reflect.Method.invoke(Method.java:606)  at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:90)  at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:324)  at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:278)  at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1016)  at groovy.lang.Closure.call(Closure.java:423)  at groovy.lang.Closure.call(Closure.java:417)  at org.springframework.beans.factory.groovy.GroovyBeanDefinitionReader.invokeBeanDefiningClosure(GroovyBeanDefinitionReader.java:426)  at org.springframework.beans.factory.groovy.GroovyBeanDefinitionReader$1.call(GroovyBeanDefinitionReader.java:223)  at groovy.lang.Closure.call(Closure.java:439)  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)  at java.lang.reflect.Method.invoke(Method.java:606)  at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:90)  at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:324)  at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1207)  at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1016)  at groovy.lang.MetaClassImpl.invokePropertyOrMissing(MetaClassImpl.java:1253)  at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1209)  at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1016)  at org.codehaus.groovy.runtime.callsite.PogoMetaClassSite.callCurrent(PogoMetaClassSite.java:66)  at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCallCurrent(CallSiteArray.java:49)  at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:133)  at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:141)  at beans.run(beans:1)  at groovy.lang.GroovyShell.evaluate(GroovyShell.java:649)  at org.springframework.beans.factory.groovy.GroovyBeanDefinitionReader.loadBeanDefinitions(GroovyBeanDefinitionReader.java:243)  ... 29 more Caused by: java.io.FileNotFoundException: null/modules/common/gemfire-sink.groovy (No such file or directory)  at java.io.FileInputStream.open(Native Method)  at java.io.FileInputStream.<init>(FileInputStream.java:146)  at java.io.FileInputStream.<init>(FileInputStream.java:101)  at sun.net.www.protocol.file.FileURLConnection.connect(FileURLConnection.java:90)  at sun.net.www.protocol.file.FileURLConnection.getInputStream(FileURLConnection.java:188)  at org.springframework.core.io.UrlResource.getInputStream(UrlResource.java:168)  at org.springframework.core.io.support.EncodedResource.getReader(EncodedResource.java:132)  at org.springframework.beans.factory.groovy.GroovyBeanDefinitionReader.loadBeanDefinitions(GroovyBeanDefinitionReader.java:243)  ... 79 more """,3.0
1,XD-2607,BUG,Done,MEDIUM,"""Windows build fails""","""This Hadoop scenario will not work in Windows. The scope is to *disable* the test for windows build.  org.springframework.batch.integration.x.RemoteFileToHadoopTaskletTests > testWrite FAILED     java.lang.IllegalStateException         Caused by: org.springframework.beans.factory.BeanCreationException             Caused by: java.lang.UnsatisfiedLinkError  org.springframework.batch.integration.x.RemoteFileToHadoopTests > testSimple FAILED     java.lang.IllegalStateException         Caused by: org.springframework.beans.factory.BeanCreationException             Caused by: java.lang.UnsatisfiedLinkError Java HotSpot(TM) Client VM warning: ignoring option MaxPermSize=512m; support was removed in 8.0  3 tests completed, 2 failed :spring-xd-extension-batch:test FAILED  FAILURE: Build failed with an exception.""","""""""This Hadoop scenario will not work in Windows. The scope is to *disable* the test for windows build.  org.springframework.batch.integration.x.RemoteFileToHadoopTaskletTests > testWrite FAILED     java.lang.IllegalStateException         Caused by: org.springframework.beans.factory.BeanCreationException             Caused by: java.lang.UnsatisfiedLinkError  org.springframework.batch.integration.x.RemoteFileToHadoopTests > testSimple FAILED     java.lang.IllegalStateException         Caused by: org.springframework.beans.factory.BeanCreationException             Caused by: java.lang.UnsatisfiedLinkError Java HotSpot(TM) Client VM warning: ignoring option MaxPermSize=512m; support was removed in 8.0  3 tests completed, 2 failed :spring-xd-extension-batch:test FAILED  FAILURE: Build failed with an exception.""""""",,1.0
1,XD-2606,FEATURE,Done,MEDIUM,"""Add support to 'track history' in message headers""","""As a user, I'd like to have an option to track history so that I get the visibility of stream name, module name etc. added as part of the message header.""","""""""As a user, I'd like to have an option to track history so that I get the visibility of stream name, module name etc. added as part of the message header.""""""",,3.0
1,XD-2605,BUG,Done,HIGH,"""TwitterStream/TwitterSearch sources fail when deploying on Yarn""","""We're getting a CNF on org.apache.http.impl.client.HttpClients {noformat} 20:07:03,556 1.1.0.SNAP  INFO DeploymentsPathChildrenCache-0 server.DeploymentListener - Path cache event: path=/deployments/modules/allocated/740c5f05-03f9-4b0f-9c3a-80022b594830/ec2Test3.sink.file.1, type=CHILD_ADDED 20:07:03,557 1.1.0.SNAP  INFO DeploymentsPathChildrenCache-0 server.DeploymentListener - Deploying module 'file' for stream 'ec2Test3' 20:07:03,828 1.1.0.SNAP  INFO DeploymentsPathChildrenCache-0 server.DeploymentListener - Deploying module [ModuleDescriptor@8d11c70 moduleName = 'file', moduleLabel = 'file', group = 'ec2Test3', sourceChannelName = [null], sinkChannelName = [null], index = 1, type = sink, parameters = map['binary' -> 'true', 'mode' -> 'REPLACE'], children = list[[empty]]] 20:07:04,456 1.1.0.SNAP  INFO DeploymentsPathChildrenCache-0 server.DeploymentListener - Path cache event: path=/deployments/modules/allocated/740c5f05-03f9-4b0f-9c3a-80022b594830/ec2Test3.source.twitterstream.1, type=CHILD_ADDED 20:07:04,456 1.1.0.SNAP  INFO DeploymentsPathChildrenCache-0 server.DeploymentListener - Deploying module 'twitterstream' for stream 'ec2Test3' 20:07:05,040 1.1.0.SNAP  INFO DeploymentsPathChildrenCache-0 server.DeploymentListener - Deploying module [ModuleDescriptor@3ec4f104 moduleName = 'twitterstream', moduleLabel = 'twitterstream', group = 'ec2Test3', sourceChannelName = [null], sinkChannelName = [null], index = 0, type = source, parameters = map['consumerKey' -> '5ynZLmXyvxXzAlYHRlrb28U8n', 'accessToken' -> '2561860742-sfreUrr2jXwUPBk5eOL4Ow5GKy4Hyl12snKwfg5', 'accessTokenSecret' -> '481BGNZZDwdJ8rVw2hG9IryKuTZsv1cV1hiDpwdHt19xe', 'consumerSecret' -> 'C7ZQhJvy5RQm3QS6ruSkCriZZWtUMRbJbNeDCH7uYACWJPtBVi'], children = list[[empty]]] 20:07:05,871 1.1.0.SNAP  WARN DeploymentsPathChildrenCache-0 annotation.AnnotationConfigApplicationContext - Exception encountered during context initialization - cancelling refresh attempt org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'org.springframework.integration.x.twitter.TwitterStreamChannelAdapter#0' defined in class path resource [config/twitterstream.xml]: Cannot resolve reference to bean 'twitterTemplate' while setting constructor argument; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'twitterTemplate' defined in class path resource [config/twitterstream.xml]: Bean instantiation via constructor failed; nested exception is org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.springframework.social.twitter.api.impl.TwitterTemplate]: Constructor threw exception; nested exception is java.lang.NoClassDefFoundError: org/apache/http/impl/client/HttpClients  at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveReference(BeanDefinitionValueResolver.java:359)  at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveValueIfNecessary(BeanDefinitionValueResolver.java:108)  at org.springframework.beans.factory.support.ConstructorResolver.resolveConstructorArguments(ConstructorResolver.java:648)  at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:140)  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1131)  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1034)  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:504)  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:476)  at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:303)  at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:230)  at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:299)  at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:194)  at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:762)  at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:757)  at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:480)  at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:691)  at org.springframework.boot.SpringApplication.run(SpringApplication.java:321)  at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:139)  at org.springframework.xd.module.core.SimpleModule.initialize(SimpleModule.java:211)  at org.springframework.xd.dirt.module.ModuleDeployer.doDeploy(ModuleDeployer.java:217)  at org.springframework.xd.dirt.module.ModuleDeployer.deploy(ModuleDeployer.java:200)  at org.springframework.xd.dirt.server.DeploymentListener.deployModule(DeploymentListener.java:363)  at org.springframework.xd.dirt.server.DeploymentListener.deployStreamModule(DeploymentListener.java:332)  at org.springframework.xd.dirt.server.DeploymentListener.onChildAdded(DeploymentListener.java:179)  at org.springframework.xd.dirt.server.DeploymentListener.childEvent(DeploymentListener.java:147)  at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:509)  at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:503)  at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)  at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297)  at org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:83)  at org.apache.curator.framework.recipes.cache.PathChildrenCache.callListeners(PathChildrenCache.java:500)  at org.apache.curator.framework.recipes.cache.EventOperation.invoke(EventOperation.java:35)  at org.apache.curator.framework.recipes.cache.PathChildrenCache$10.run(PathChildrenCache.java:762)  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)  at java.util.concurrent.FutureTask.run(FutureTask.java:262)  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)  at java.util.concurrent.FutureTask.run(FutureTask.java:262)  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)  at java.lang.Thread.run(Thread.java:745) Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'twitterTemplate' defined in class path resource [config/twitterstream.xml]: Bean instantiation via constructor failed; nested exception is org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.springframework.social.twitter.api.impl.TwitterTemplate]: Constructor threw exception; nested exception is java.lang.NoClassDefFoundError: org/apache/http/impl/client/HttpClients  at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:275)  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1131)  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1034)  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:504)  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:476)  at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:303)  at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:230)  at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:299)  at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:194)  at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveReference(BeanDefinitionValueResolver.java:351)  ... 39 more Caused by: org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.springframework.social.twitter.api.impl.TwitterTemplate]: Constructor threw exception; nested exception is java.lang.NoClassDefFoundError: org/apache/http/impl/client/HttpClients  at org.springframework.beans.BeanUtils.instantiateClass(BeanUtils.java:163)  at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:122)  at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:267)  ... 48 more Caused by: java.lang.NoClassDefFoundError: org/apache/http/impl/client/HttpClients  at org.springframework.http.client.HttpComponentsClientHttpRequestFactory.<init>(HttpComponentsClientHttpRequestFactory.java:72)  at org.springframework.social.support.ClientHttpRequestFactorySelector$HttpComponentsClientRequestFactoryCreator$1.<init>(ClientHttpRequestFactorySelector.java:77)  at org.springframework.social.support.ClientHttpRequestFactorySelector$HttpComponentsClientRequestFactoryCreator.createRequestFactory(ClientHttpRequestFactorySelector.java:77)  at org.springframework.social.support.ClientHttpRequestFactorySelector.getRequestFactory(ClientHttpRequestFactorySelector.java:52)  at org.springframework.social.oauth1.AbstractOAuth1ApiBinding.createRestTemplateWithCulledMessageConverters(AbstractOAuth1ApiBinding.java:188)  at org.springframework.social.oauth1.AbstractOAuth1ApiBinding.createRestTemplate(AbstractOAuth1ApiBinding.java:169)  at org.springframework.social.oauth1.AbstractOAuth1ApiBinding.<init>(AbstractOAuth1ApiBinding.java:70)  at org.springframework.social.twitter.api.impl.TwitterTemplate.<init>(TwitterTemplate.java:79)  at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)  at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)  at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)  at java.lang.reflect.Constructor.newInstance(Constructor.java:526)  at org.springframework.beans.BeanUtils.instantiateClass(BeanUtils.java:147)  ... 50 more Caused by: java.lang.ClassNotFoundException: org.apache.http.impl.client.HttpClients  at java.net.URLClassLoader$1.run(URLClassLoader.java:366)  at java.net.URLClassLoader$1.run(URLClassLoader.java:355)  at java.security.AccessController.doPrivileged(Native Method)  at java.net.URLClassLoader.findClass(URLClassLoader.java:354)  at java.lang.ClassLoader.loadClass(ClassLoader.java:425)  at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)  at java.lang.ClassLoader.loadClass(ClassLoader.java:358)  ... 63 more 20:07:05,874 1.1.0.SNAP ERROR DeploymentsPathChildrenCache-0 boot.SpringApplication - Application startup failed org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'org.springframework.integration.x.twitter.TwitterStreamChannelAdapter#0' defined in class path resource [config/twitterstream.xml]: Cannot resolve reference to bean 'twitterTemplate' while setting constructor argument; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'twitterTemplate' defined in class path resource [config/twitterstream.xml]: Bean instantiation via constructor failed; nested exception is org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.springframework.social.twitter.api.impl.TwitterTemplate]: Constructor threw exception; nested exception is java.lang.NoClassDefFoundError: org/apache/http/impl/client/HttpClients  at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveReference(BeanDefinitionValueResolver.java:359)  at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveValueIfNecessary(BeanDefinitionValueResolver.java:108)  at org.springframework.beans.factory.support.ConstructorResolver.resolveConstructorArguments(ConstructorResolver.java:648)  at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:140)  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1131)  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1034)  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:504)  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:476)  at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:303)  at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:230)  at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:299)  at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:194)  at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:762)  at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:757)  at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:480)  at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:691)  at org.springframework.boot.SpringApplication.run(SpringApplication.java:321)  at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:139)  at org.springframework.xd.module.core.SimpleModule.initialize(SimpleModule.java:211)  at org.springframework.xd.dirt.module.ModuleDeployer.doDeploy(ModuleDeployer.java:217)  at org.springframework.xd.dirt.module.ModuleDeployer.deploy(ModuleDeployer.java:200)  at org.springframework.xd.dirt.server.DeploymentListener.deployModule(DeploymentListener.java:363)  at org.springframework.xd.dirt.server.DeploymentListener.deployStreamModule(DeploymentListener.java:332)  at org.springframework.xd.dirt.server.DeploymentListener.onChildAdded(DeploymentListener.java:179)  at org.springframework.xd.dirt.server.DeploymentListener.childEvent(DeploymentListener.java:147)  at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:509)  at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:503)  at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)  at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297)  at org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:83)  at org.apache.curator.framework.recipes.cache.PathChildrenCache.callListeners(PathChildrenCache.java:500)  at org.apache.curator.framework.recipes.cache.EventOperation.invoke(EventOperation.java:35)  at org.apache.curator.framework.recipes.cache.PathChildrenCache$10.run(PathChildrenCache.java:762)  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)  at java.util.concurrent.FutureTask.run(FutureTask.java:262)  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)  at java.util.concurrent.FutureTask.run(FutureTask.java:262)  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)  at java.lang.Thread.run(Thread.java:745) Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'twitterTemplate' defined in class path resource [config/twitterstream.xml]: Bean instantiation via constructor failed; nested exception is org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.springframework.social.twitter.api.impl.TwitterTemplate]: Constructor threw exception; nested exception is java.lang.NoClassDefFoundError: org/apache/http/impl/client/HttpClients  at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:275)  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1131)  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1034)  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:504)  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:476)  at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:303)  at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:230)  at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:299)  at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:194)  at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveReference(BeanDefinitionValueResolver.java:351)  ... 39 more Caused by: org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.springframework.social.twitter.api.impl.TwitterTemplate]: Constructor threw exception; nested exception is java.lang.NoClassDefFoundError: org/apache/http/impl/client/HttpClients  at org.springframework.beans.BeanUtils.instantiateClass(BeanUtils.java:163)  at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:122)  at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:267)  ... 48 more Caused by: java.lang.NoClassDefFoundError: org/apache/http/impl/client/HttpClients  at org.springframework.http.client.HttpComponentsClientHttpRequestFactory.<init>(HttpComponentsClientHttpRequestFactory.java:72)  at org.springframework.social.support.ClientHttpRequestFactorySelector$HttpComponentsClientRequestFactoryCreator$1.<init>(ClientHttpRequestFactorySelector.java:77)  at org.springframework.social.support.ClientHttpRequestFactorySelector$HttpComponentsClientRequestFactoryCreator.createRequestFactory(ClientHttpRequestFactorySelector.java:77)  at org.springframework.social.support.ClientHttpRequestFactorySelector.getRequestFactory(ClientHttpRequestFactorySelector.java:52)  at org.springframework.social.oauth1.AbstractOAuth1ApiBinding.createRestTemplateWithCulledMessageConverters(AbstractOAuth1ApiBinding.java:188)  at org.springframework.social.oauth1.AbstractOAuth1ApiBinding.createRestTemplate(AbstractOAuth1ApiBinding.java:169)  at org.springframework.social.oauth1.AbstractOAuth1ApiBinding.<init>(AbstractOAuth1ApiBinding.java:70)  at org.springframework.social.twitter.api.impl.TwitterTemplate.<init>(TwitterTemplate.java:79)  at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)  at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)  at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)  at java.lang.reflect.Constructor.newInstance(Constructor.java:526)  at org.springframework.beans.BeanUtils.instantiateClass(BeanUtils.java:147)  ... 50 more Caused by: java.lang.ClassNotFoundException: org.apache.http.impl.client.HttpClients  at java.net.URLClassLoader$1.run(URLClassLoader.java:366)  at java.net.URLClassLoader$1.run(URLClassLoader.java:355)  at java.security.AccessController.doPrivileged(Native Method)  at java.net.URLClassLoader.findClass(URLClassLoader.java:354)  at java.lang.ClassLoader.loadClass(ClassLoader.java:425)  at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)  at java.lang.ClassLoader.loadClass(ClassLoader.java:358)  ... 63 more 20:07:05,877 1.1.0.SNAP ERROR DeploymentsPathChildrenCache-0 server.DeploymentListener - Exception deploying module org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'org.springframework.integration.x.twitter.TwitterStreamChannelAdapter#0' defined in class path resource [config/twitterstream.xml]: Cannot resolve reference to bean 'twitterTemplate' while setting constructor argument; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'twitterTemplate' defined in class path resource [config/twitterstream.xml]: Bean instantiation via constructor failed; nested exception is org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.springframework.social.twitter.api.impl.TwitterTemplate]: Constructor threw exception; nested exception is java.lang.NoClassDefFoundError: org/apache/http/impl/client/HttpClients  at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveReference(BeanDefinitionValueResolver.java:359)  at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveValueIfNecessary(BeanDefinitionValueResolver.java:108)  at org.springframework.beans.factory.support.ConstructorResolver.resolveConstructorArguments(ConstructorResolver.java:648)  at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:140)  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1131)  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1034)  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:504)  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:476)  at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:303)  at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:230)  at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:299)  at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:194)  at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:762)  at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:757)  at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:480)  at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:691)  at org.springframework.boot.SpringApplication.run(SpringApplication.java:321)  at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:139)  at org.springframework.xd.module.core.SimpleModule.initialize(SimpleModule.java:211)  at org.springframework.xd.dirt.module.ModuleDeployer.doDeploy(ModuleDeployer.java:217)  at org.springframework.xd.dirt.module.ModuleDeployer.deploy(ModuleDeployer.java:200)  at org.springframework.xd.dirt.server.DeploymentListener.deployModule(DeploymentListener.java:363)  at org.springframework.xd.dirt.server.DeploymentListener.deployStreamModule(DeploymentListener.java:332)  at org.springframework.xd.dirt.server.DeploymentListener.onChildAdded(DeploymentListener.java:179)  at org.springframework.xd.dirt.server.DeploymentListener.childEvent(DeploymentListener.java:147)  at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:509)  at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:503)  at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)  at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297)  at org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:83)  at org.apache.curator.framework.recipes.cache.PathChildrenCache.callListeners(PathChildrenCache.java:500)  at org.apache.curator.framework.recipes.cache.EventOperation.invoke(EventOperation.java:35)  at org.apache.curator.framework.recipes.cache.PathChildrenCache$10.run(PathChildrenCache.java:762)  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)  at java.util.concurrent.FutureTask.run(FutureTask.java:262)  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)  at java.util.concurrent.FutureTask.run(FutureTask.java:262)  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)  at java.lang.Thread.run(Thread.java:745) Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'twitterTemplate' defined in class path resource [config/twitterstream.xml]: Bean instantiation via constructor failed; nested exception is org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.springframework.social.twitter.api.impl.TwitterTemplate]: Constructor threw exception; nested exception is java.lang.NoClassDefFoundError: org/apache/http/impl/client/HttpClients  at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:275)  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1131)  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1034)  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:504)  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:476)  at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:303)  at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:230)  at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:299)  at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:194)  at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveReference(BeanDefinitionValueResolver.java:351)  ... 39 more Caused by: org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.springframework.social.twitter.api.impl.TwitterTemplate]: Constructor threw exception; nested exception is java.lang.NoClassDefFoundError: org/apache/http/impl/client/HttpClients  at org.springframework.beans.BeanUtils.instantiateClass(BeanUtils.java:163)  at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:122)  at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:267)  ... 48 more Caused by: java.lang.NoClassDefFoundError: org/apache/http/impl/client/HttpClients  at org.springframework.http.client.HttpComponentsClientHttpRequestFactory.<init>(HttpComponentsClientHttpRequestFactory.java:72)  at org.springframework.social.support.ClientHttpRequestFactorySelector$HttpComponentsClientRequestFactoryCreator$1.<init>(ClientHttpRequestFactorySelector.java:77)  at org.springframework.social.support.ClientHttpRequestFactorySelector$HttpComponentsClientRequestFactoryCreator.createRequestFactory(ClientHttpRequestFactorySelector.java:77)  at org.springframework.social.support.ClientHttpRequestFactorySelector.getRequestFactory(ClientHttpRequestFactorySelector.java:52)  at org.springframework.social.oauth1.AbstractOAuth1ApiBinding.createRestTemplateWithCulledMessageConverters(AbstractOAuth1ApiBinding.java:188)  at org.springframework.social.oauth1.AbstractOAuth1ApiBinding.createRestTemplate(AbstractOAuth1ApiBinding.java:169)  at org.springframework.social.oauth1.AbstractOAuth1ApiBinding.<init>(AbstractOAuth1ApiBinding.java:70)  at org.springframework.social.twitter.api.impl.TwitterTemplate.<init>(TwitterTemplate.java:79)  at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)  at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)  at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)  at java.lang.reflect.Constructor.newInstance(Constructor.java:526)  at org.springframework.beans.BeanUtils.instantiateClass(BeanUtils.java:147)  ... 50 more Caused by: java.lang.ClassNotFoundException: org.apache.http.impl.client.HttpClients  at java.net.URLClassLoader$1.run(URLClassLoader.java:366)  at java.net.URLClassLoader$1.run(URLClassLoader.java:355)  at java.security.AccessController.doPrivileged(Native Method)  at java.net.URLClassLoader.findClass(URLClassLoader.java:354)  at java.lang.ClassLoader.loadClass(ClassLoader.java:425)  at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)  at java.lang.ClassLoader.loadClass(ClassLoader.java:358)  ... 63 more 20:07:05,890 1.1.0.SNAP  INFO DeploymentsPathChildrenCache-0 server.DeploymentListener - Path cache event: path=/deployments/modules/allocated/740c5f05-03f9-4b0f-9c3a-80022b594830/ec2Test3.source.twitterstream.1, type=CHILD_REMOVED 20:07:05,890 1.1.0.SNAP  INFO DeploymentsPathChildrenCache-0 server.DeploymentListener - Undeploying module [ModuleDescriptor@3ec4f104 moduleName = 'twitterstream', moduleLabel = 'twitterstream', group = 'ec2Test3', sourceChannelName = [null], sinkChannelName = [null], index = 0, type = source, parameters = map['consumerKey' -> '5ynZLmXyvxXzAlYHRlrb28U8n', 'accessToken' -> '2561860742-sfreUrr2jXwUPBk5eOL4Ow5GKy4Hyl12snKwfg5', 'accessTokenSecret' -> '481BGNZZDwdJ8rVw2hG9IryKuTZsv1cV1hiDpwdHt19xe', 'consumerSecret' -> 'C7ZQhJvy5RQm3QS6ruSkCriZZWtUMRbJbNeDCH7uYACWJPtBVi'], children = list[[empty]]] 20:07:19,164 1.1.0.SNAP  INFO main-EventThread server.DeploymentListener - Undeploying module [ModuleDescriptor@8d11c70 moduleName = 'file', moduleLabel = 'file', group = 'ec2Test3', sourceChannelName = [null], sinkChannelName = [null], index = 1, type = sink, parameters = map['binary' -> 'true', 'mode' -> 'REPLACE'], children = list[[empty]]] 20:07:19,457 1.1.0.SNAP  INFO DeploymentsPathChildrenCache-0 server.DeploymentListener - Path cache event: path=/deployments/modules/allocated/740c5f05-03f9-4b0f-9c3a-80022b594830/ec2Test3.sink.file.1, type=CHILD_REMOVED  {noformat}""","""""""We're getting a CNF on org.apache.http.impl.client.HttpClients """"""",""" 20:07:03,556 1.1.0.SNAP  INFO DeploymentsPathChildrenCache-0 server.DeploymentListener - Path cache event: path=/deployments/modules/allocated/740c5f05-03f9-4b0f-9c3a-80022b594830/ec2Test3.sink.file.1, type=CHILD_ADDED 20:07:03,557 1.1.0.SNAP  INFO DeploymentsPathChildrenCache-0 server.DeploymentListener - Deploying module 'file' for stream 'ec2Test3' 20:07:03,828 1.1.0.SNAP  INFO DeploymentsPathChildrenCache-0 server.DeploymentListener - Deploying module [ModuleDescriptor@8d11c70 moduleName = 'file', moduleLabel = 'file', group = 'ec2Test3', sourceChannelName = [null], sinkChannelName = [null], index = 1, type = sink, parameters = map['binary' -> 'true', 'mode' -> 'REPLACE'], children = list[[empty]]] 20:07:04,456 1.1.0.SNAP  INFO DeploymentsPathChildrenCache-0 server.DeploymentListener - Path cache event: path=/deployments/modules/allocated/740c5f05-03f9-4b0f-9c3a-80022b594830/ec2Test3.source.twitterstream.1, type=CHILD_ADDED 20:07:04,456 1.1.0.SNAP  INFO DeploymentsPathChildrenCache-0 server.DeploymentListener - Deploying module 'twitterstream' for stream 'ec2Test3' 20:07:05,040 1.1.0.SNAP  INFO DeploymentsPathChildrenCache-0 server.DeploymentListener - Deploying module [ModuleDescriptor@3ec4f104 moduleName = 'twitterstream', moduleLabel = 'twitterstream', group = 'ec2Test3', sourceChannelName = [null], sinkChannelName = [null], index = 0, type = source, parameters = map['consumerKey' -> '5ynZLmXyvxXzAlYHRlrb28U8n', 'accessToken' -> '2561860742-sfreUrr2jXwUPBk5eOL4Ow5GKy4Hyl12snKwfg5', 'accessTokenSecret' -> '481BGNZZDwdJ8rVw2hG9IryKuTZsv1cV1hiDpwdHt19xe', 'consumerSecret' -> 'C7ZQhJvy5RQm3QS6ruSkCriZZWtUMRbJbNeDCH7uYACWJPtBVi'], children = list[[empty]]] 20:07:05,871 1.1.0.SNAP  WARN DeploymentsPathChildrenCache-0 annotation.AnnotationConfigApplicationContext - Exception encountered during context initialization - cancelling refresh attempt org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'org.springframework.integration.x.twitter.TwitterStreamChannelAdapter#0' defined in class path resource [config/twitterstream.xml]: Cannot resolve reference to bean 'twitterTemplate' while setting constructor argument; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'twitterTemplate' defined in class path resource [config/twitterstream.xml]: Bean instantiation via constructor failed; nested exception is org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.springframework.social.twitter.api.impl.TwitterTemplate]: Constructor threw exception; nested exception is java.lang.NoClassDefFoundError: org/apache/http/impl/client/HttpClients  at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveReference(BeanDefinitionValueResolver.java:359)  at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveValueIfNecessary(BeanDefinitionValueResolver.java:108)  at org.springframework.beans.factory.support.ConstructorResolver.resolveConstructorArguments(ConstructorResolver.java:648)  at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:140)  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1131)  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1034)  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:504)  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:476)  at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:303)  at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:230)  at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:299)  at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:194)  at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:762)  at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:757)  at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:480)  at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:691)  at org.springframework.boot.SpringApplication.run(SpringApplication.java:321)  at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:139)  at org.springframework.xd.module.core.SimpleModule.initialize(SimpleModule.java:211)  at org.springframework.xd.dirt.module.ModuleDeployer.doDeploy(ModuleDeployer.java:217)  at org.springframework.xd.dirt.module.ModuleDeployer.deploy(ModuleDeployer.java:200)  at org.springframework.xd.dirt.server.DeploymentListener.deployModule(DeploymentListener.java:363)  at org.springframework.xd.dirt.server.DeploymentListener.deployStreamModule(DeploymentListener.java:332)  at org.springframework.xd.dirt.server.DeploymentListener.onChildAdded(DeploymentListener.java:179)  at org.springframework.xd.dirt.server.DeploymentListener.childEvent(DeploymentListener.java:147)  at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:509)  at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:503)  at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)  at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297)  at org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:83)  at org.apache.curator.framework.recipes.cache.PathChildrenCache.callListeners(PathChildrenCache.java:500)  at org.apache.curator.framework.recipes.cache.EventOperation.invoke(EventOperation.java:35)  at org.apache.curator.framework.recipes.cache.PathChildrenCache$10.run(PathChildrenCache.java:762)  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)  at java.util.concurrent.FutureTask.run(FutureTask.java:262)  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)  at java.util.concurrent.FutureTask.run(FutureTask.java:262)  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)  at java.lang.Thread.run(Thread.java:745) Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'twitterTemplate' defined in class path resource [config/twitterstream.xml]: Bean instantiation via constructor failed; nested exception is org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.springframework.social.twitter.api.impl.TwitterTemplate]: Constructor threw exception; nested exception is java.lang.NoClassDefFoundError: org/apache/http/impl/client/HttpClients  at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:275)  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1131)  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1034)  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:504)  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:476)  at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:303)  at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:230)  at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:299)  at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:194)  at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveReference(BeanDefinitionValueResolver.java:351)  ... 39 more Caused by: org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.springframework.social.twitter.api.impl.TwitterTemplate]: Constructor threw exception; nested exception is java.lang.NoClassDefFoundError: org/apache/http/impl/client/HttpClients  at org.springframework.beans.BeanUtils.instantiateClass(BeanUtils.java:163)  at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:122)  at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:267)  ... 48 more Caused by: java.lang.NoClassDefFoundError: org/apache/http/impl/client/HttpClients  at org.springframework.http.client.HttpComponentsClientHttpRequestFactory.<init>(HttpComponentsClientHttpRequestFactory.java:72)  at org.springframework.social.support.ClientHttpRequestFactorySelector$HttpComponentsClientRequestFactoryCreator$1.<init>(ClientHttpRequestFactorySelector.java:77)  at org.springframework.social.support.ClientHttpRequestFactorySelector$HttpComponentsClientRequestFactoryCreator.createRequestFactory(ClientHttpRequestFactorySelector.java:77)  at org.springframework.social.support.ClientHttpRequestFactorySelector.getRequestFactory(ClientHttpRequestFactorySelector.java:52)  at org.springframework.social.oauth1.AbstractOAuth1ApiBinding.createRestTemplateWithCulledMessageConverters(AbstractOAuth1ApiBinding.java:188)  at org.springframework.social.oauth1.AbstractOAuth1ApiBinding.createRestTemplate(AbstractOAuth1ApiBinding.java:169)  at org.springframework.social.oauth1.AbstractOAuth1ApiBinding.<init>(AbstractOAuth1ApiBinding.java:70)  at org.springframework.social.twitter.api.impl.TwitterTemplate.<init>(TwitterTemplate.java:79)  at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)  at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)  at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)  at java.lang.reflect.Constructor.newInstance(Constructor.java:526)  at org.springframework.beans.BeanUtils.instantiateClass(BeanUtils.java:147)  ... 50 more Caused by: java.lang.ClassNotFoundException: org.apache.http.impl.client.HttpClients  at java.net.URLClassLoader$1.run(URLClassLoader.java:366)  at java.net.URLClassLoader$1.run(URLClassLoader.java:355)  at java.security.AccessController.doPrivileged(Native Method)  at java.net.URLClassLoader.findClass(URLClassLoader.java:354)  at java.lang.ClassLoader.loadClass(ClassLoader.java:425)  at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)  at java.lang.ClassLoader.loadClass(ClassLoader.java:358)  ... 63 more 20:07:05,874 1.1.0.SNAP ERROR DeploymentsPathChildrenCache-0 boot.SpringApplication - Application startup failed org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'org.springframework.integration.x.twitter.TwitterStreamChannelAdapter#0' defined in class path resource [config/twitterstream.xml]: Cannot resolve reference to bean 'twitterTemplate' while setting constructor argument; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'twitterTemplate' defined in class path resource [config/twitterstream.xml]: Bean instantiation via constructor failed; nested exception is org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.springframework.social.twitter.api.impl.TwitterTemplate]: Constructor threw exception; nested exception is java.lang.NoClassDefFoundError: org/apache/http/impl/client/HttpClients  at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveReference(BeanDefinitionValueResolver.java:359)  at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveValueIfNecessary(BeanDefinitionValueResolver.java:108)  at org.springframework.beans.factory.support.ConstructorResolver.resolveConstructorArguments(ConstructorResolver.java:648)  at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:140)  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1131)  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1034)  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:504)  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:476)  at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:303)  at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:230)  at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:299)  at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:194)  at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:762)  at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:757)  at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:480)  at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:691)  at org.springframework.boot.SpringApplication.run(SpringApplication.java:321)  at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:139)  at org.springframework.xd.module.core.SimpleModule.initialize(SimpleModule.java:211)  at org.springframework.xd.dirt.module.ModuleDeployer.doDeploy(ModuleDeployer.java:217)  at org.springframework.xd.dirt.module.ModuleDeployer.deploy(ModuleDeployer.java:200)  at org.springframework.xd.dirt.server.DeploymentListener.deployModule(DeploymentListener.java:363)  at org.springframework.xd.dirt.server.DeploymentListener.deployStreamModule(DeploymentListener.java:332)  at org.springframework.xd.dirt.server.DeploymentListener.onChildAdded(DeploymentListener.java:179)  at org.springframework.xd.dirt.server.DeploymentListener.childEvent(DeploymentListener.java:147)  at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:509)  at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:503)  at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)  at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297)  at org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:83)  at org.apache.curator.framework.recipes.cache.PathChildrenCache.callListeners(PathChildrenCache.java:500)  at org.apache.curator.framework.recipes.cache.EventOperation.invoke(EventOperation.java:35)  at org.apache.curator.framework.recipes.cache.PathChildrenCache$10.run(PathChildrenCache.java:762)  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)  at java.util.concurrent.FutureTask.run(FutureTask.java:262)  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)  at java.util.concurrent.FutureTask.run(FutureTask.java:262)  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)  at java.lang.Thread.run(Thread.java:745) Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'twitterTemplate' defined in class path resource [config/twitterstream.xml]: Bean instantiation via constructor failed; nested exception is org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.springframework.social.twitter.api.impl.TwitterTemplate]: Constructor threw exception; nested exception is java.lang.NoClassDefFoundError: org/apache/http/impl/client/HttpClients  at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:275)  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1131)  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1034)  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:504)  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:476)  at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:303)  at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:230)  at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:299)  at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:194)  at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveReference(BeanDefinitionValueResolver.java:351)  ... 39 more Caused by: org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.springframework.social.twitter.api.impl.TwitterTemplate]: Constructor threw exception; nested exception is java.lang.NoClassDefFoundError: org/apache/http/impl/client/HttpClients  at org.springframework.beans.BeanUtils.instantiateClass(BeanUtils.java:163)  at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:122)  at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:267)  ... 48 more Caused by: java.lang.NoClassDefFoundError: org/apache/http/impl/client/HttpClients  at org.springframework.http.client.HttpComponentsClientHttpRequestFactory.<init>(HttpComponentsClientHttpRequestFactory.java:72)  at org.springframework.social.support.ClientHttpRequestFactorySelector$HttpComponentsClientRequestFactoryCreator$1.<init>(ClientHttpRequestFactorySelector.java:77)  at org.springframework.social.support.ClientHttpRequestFactorySelector$HttpComponentsClientRequestFactoryCreator.createRequestFactory(ClientHttpRequestFactorySelector.java:77)  at org.springframework.social.support.ClientHttpRequestFactorySelector.getRequestFactory(ClientHttpRequestFactorySelector.java:52)  at org.springframework.social.oauth1.AbstractOAuth1ApiBinding.createRestTemplateWithCulledMessageConverters(AbstractOAuth1ApiBinding.java:188)  at org.springframework.social.oauth1.AbstractOAuth1ApiBinding.createRestTemplate(AbstractOAuth1ApiBinding.java:169)  at org.springframework.social.oauth1.AbstractOAuth1ApiBinding.<init>(AbstractOAuth1ApiBinding.java:70)  at org.springframework.social.twitter.api.impl.TwitterTemplate.<init>(TwitterTemplate.java:79)  at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)  at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)  at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)  at java.lang.reflect.Constructor.newInstance(Constructor.java:526)  at org.springframework.beans.BeanUtils.instantiateClass(BeanUtils.java:147)  ... 50 more Caused by: java.lang.ClassNotFoundException: org.apache.http.impl.client.HttpClients  at java.net.URLClassLoader$1.run(URLClassLoader.java:366)  at java.net.URLClassLoader$1.run(URLClassLoader.java:355)  at java.security.AccessController.doPrivileged(Native Method)  at java.net.URLClassLoader.findClass(URLClassLoader.java:354)  at java.lang.ClassLoader.loadClass(ClassLoader.java:425)  at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)  at java.lang.ClassLoader.loadClass(ClassLoader.java:358)  ... 63 more 20:07:05,877 1.1.0.SNAP ERROR DeploymentsPathChildrenCache-0 server.DeploymentListener - Exception deploying module org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'org.springframework.integration.x.twitter.TwitterStreamChannelAdapter#0' defined in class path resource [config/twitterstream.xml]: Cannot resolve reference to bean 'twitterTemplate' while setting constructor argument; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'twitterTemplate' defined in class path resource [config/twitterstream.xml]: Bean instantiation via constructor failed; nested exception is org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.springframework.social.twitter.api.impl.TwitterTemplate]: Constructor threw exception; nested exception is java.lang.NoClassDefFoundError: org/apache/http/impl/client/HttpClients  at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveReference(BeanDefinitionValueResolver.java:359)  at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveValueIfNecessary(BeanDefinitionValueResolver.java:108)  at org.springframework.beans.factory.support.ConstructorResolver.resolveConstructorArguments(ConstructorResolver.java:648)  at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:140)  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1131)  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1034)  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:504)  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:476)  at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:303)  at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:230)  at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:299)  at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:194)  at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:762)  at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:757)  at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:480)  at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:691)  at org.springframework.boot.SpringApplication.run(SpringApplication.java:321)  at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:139)  at org.springframework.xd.module.core.SimpleModule.initialize(SimpleModule.java:211)  at org.springframework.xd.dirt.module.ModuleDeployer.doDeploy(ModuleDeployer.java:217)  at org.springframework.xd.dirt.module.ModuleDeployer.deploy(ModuleDeployer.java:200)  at org.springframework.xd.dirt.server.DeploymentListener.deployModule(DeploymentListener.java:363)  at org.springframework.xd.dirt.server.DeploymentListener.deployStreamModule(DeploymentListener.java:332)  at org.springframework.xd.dirt.server.DeploymentListener.onChildAdded(DeploymentListener.java:179)  at org.springframework.xd.dirt.server.DeploymentListener.childEvent(DeploymentListener.java:147)  at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:509)  at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:503)  at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)  at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297)  at org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:83)  at org.apache.curator.framework.recipes.cache.PathChildrenCache.callListeners(PathChildrenCache.java:500)  at org.apache.curator.framework.recipes.cache.EventOperation.invoke(EventOperation.java:35)  at org.apache.curator.framework.recipes.cache.PathChildrenCache$10.run(PathChildrenCache.java:762)  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)  at java.util.concurrent.FutureTask.run(FutureTask.java:262)  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)  at java.util.concurrent.FutureTask.run(FutureTask.java:262)  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)  at java.lang.Thread.run(Thread.java:745) Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'twitterTemplate' defined in class path resource [config/twitterstream.xml]: Bean instantiation via constructor failed; nested exception is org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.springframework.social.twitter.api.impl.TwitterTemplate]: Constructor threw exception; nested exception is java.lang.NoClassDefFoundError: org/apache/http/impl/client/HttpClients  at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:275)  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1131)  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1034)  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:504)  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:476)  at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:303)  at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:230)  at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:299)  at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:194)  at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveReference(BeanDefinitionValueResolver.java:351)  ... 39 more Caused by: org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.springframework.social.twitter.api.impl.TwitterTemplate]: Constructor threw exception; nested exception is java.lang.NoClassDefFoundError: org/apache/http/impl/client/HttpClients  at org.springframework.beans.BeanUtils.instantiateClass(BeanUtils.java:163)  at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:122)  at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:267)  ... 48 more Caused by: java.lang.NoClassDefFoundError: org/apache/http/impl/client/HttpClients  at org.springframework.http.client.HttpComponentsClientHttpRequestFactory.<init>(HttpComponentsClientHttpRequestFactory.java:72)  at org.springframework.social.support.ClientHttpRequestFactorySelector$HttpComponentsClientRequestFactoryCreator$1.<init>(ClientHttpRequestFactorySelector.java:77)  at org.springframework.social.support.ClientHttpRequestFactorySelector$HttpComponentsClientRequestFactoryCreator.createRequestFactory(ClientHttpRequestFactorySelector.java:77)  at org.springframework.social.support.ClientHttpRequestFactorySelector.getRequestFactory(ClientHttpRequestFactorySelector.java:52)  at org.springframework.social.oauth1.AbstractOAuth1ApiBinding.createRestTemplateWithCulledMessageConverters(AbstractOAuth1ApiBinding.java:188)  at org.springframework.social.oauth1.AbstractOAuth1ApiBinding.createRestTemplate(AbstractOAuth1ApiBinding.java:169)  at org.springframework.social.oauth1.AbstractOAuth1ApiBinding.<init>(AbstractOAuth1ApiBinding.java:70)  at org.springframework.social.twitter.api.impl.TwitterTemplate.<init>(TwitterTemplate.java:79)  at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)  at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)  at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)  at java.lang.reflect.Constructor.newInstance(Constructor.java:526)  at org.springframework.beans.BeanUtils.instantiateClass(BeanUtils.java:147)  ... 50 more Caused by: java.lang.ClassNotFoundException: org.apache.http.impl.client.HttpClients  at java.net.URLClassLoader$1.run(URLClassLoader.java:366)  at java.net.URLClassLoader$1.run(URLClassLoader.java:355)  at java.security.AccessController.doPrivileged(Native Method)  at java.net.URLClassLoader.findClass(URLClassLoader.java:354)  at java.lang.ClassLoader.loadClass(ClassLoader.java:425)  at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)  at java.lang.ClassLoader.loadClass(ClassLoader.java:358)  ... 63 more 20:07:05,890 1.1.0.SNAP  INFO DeploymentsPathChildrenCache-0 server.DeploymentListener - Path cache event: path=/deployments/modules/allocated/740c5f05-03f9-4b0f-9c3a-80022b594830/ec2Test3.source.twitterstream.1, type=CHILD_REMOVED 20:07:05,890 1.1.0.SNAP  INFO DeploymentsPathChildrenCache-0 server.DeploymentListener - Undeploying module [ModuleDescriptor@3ec4f104 moduleName = 'twitterstream', moduleLabel = 'twitterstream', group = 'ec2Test3', sourceChannelName = [null], sinkChannelName = [null], index = 0, type = source, parameters = map['consumerKey' -> '5ynZLmXyvxXzAlYHRlrb28U8n', 'accessToken' -> '2561860742-sfreUrr2jXwUPBk5eOL4Ow5GKy4Hyl12snKwfg5', 'accessTokenSecret' -> '481BGNZZDwdJ8rVw2hG9IryKuTZsv1cV1hiDpwdHt19xe', 'consumerSecret' -> 'C7ZQhJvy5RQm3QS6ruSkCriZZWtUMRbJbNeDCH7uYACWJPtBVi'], children = list[[empty]]] 20:07:19,164 1.1.0.SNAP  INFO main-EventThread server.DeploymentListener - Undeploying module [ModuleDescriptor@8d11c70 moduleName = 'file', moduleLabel = 'file', group = 'ec2Test3', sourceChannelName = [null], sinkChannelName = [null], index = 1, type = sink, parameters = map['binary' -> 'true', 'mode' -> 'REPLACE'], children = list[[empty]]] 20:07:19,457 1.1.0.SNAP  INFO DeploymentsPathChildrenCache-0 server.DeploymentListener - Path cache event: path=/deployments/modules/allocated/740c5f05-03f9-4b0f-9c3a-80022b594830/ec2Test3.sink.file.1, type=CHILD_REMOVED  """,3.0
1,XD-2602,IMPROVEMENT,Done,MEDIUM,"""trying to import data from MS SQL tables to hadoop using jdbc source and HDFS Sink . HDFS sink is not working""","""deploying stream:  stream create foo2 --definition """"jdbc --fixedDelay=600 --split=1 --url='jdbc:sqlserver://ip:port;database=XXXXX' --driverClassName=com.microsoft.sqlserver.jdbc.SQLServerDriver --username=XXXX --password=XXX--query='select * from abc' | hdfs --fsUri=hdfs://hadopIP:port --directory=xddir"""" --deploy  It is throwing error:  19:17:10,087 1.0.2.RELEASE ERROR inbound.foo1.0-redis:queue-inbound-channel-adapter1 redis.RedisMessageBus$1 - Failed to deliver message; retries exhausted; message sent to queue 'ERRORS:name' org.springframework.messaging.MessageHandlingException: failed to write Message payload to HDFS  at org.springframework.xd.integration.hadoop.outbound.HdfsStoreMessageHandler.handleMessageInternal(HdfsStoreMessageHandler.java:129)  at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78)  at sun.reflect.GeneratedMethodAccessor89.invoke(Unknown Source)  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)  at java.lang.reflect.Method.invoke(Method.java:606)  at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)  at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)  at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)  at org.springframework.integration.monitor.SimpleMessageHandlerMetrics.handleMessage(SimpleMessageHandlerMetrics.java:106)  at org.springframework.integration.monitor.SimpleMessageHandlerMetrics.invoke(SimpleMessageHandlerMetrics.java:86)  at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)  at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)  at com.sun.proxy.$Proxy100.handleMessage(Unknown Source)  at org.springframework.integration.dispatcher.AbstractDispatcher.tryOptimizedDispatch(AbstractDispatcher.java:116)  at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:101)  at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)  at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)  at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:255)  at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:223)  at sun.reflect.GeneratedMethodAccessor88.invoke(Unknown Source)  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)  at java.lang.reflect.Method.invoke(Method.java:606)  at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)  at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)  at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)  at org.springframework.integration.monitor.DirectChannelMetrics.monitorSend(DirectChannelMetrics.java:113)  at org.springframework.integration.monitor.DirectChannelMetrics.doInvoke(DirectChannelMetrics.java:97)  at org.springframework.integration.monitor.DirectChannelMetrics.invoke(DirectChannelMetrics.java:91)  at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)  at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)  at com.sun.proxy.$Proxy63.send(Unknown Source)  at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:109)  at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:44)  at org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:94)  at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.sendMessage(AbstractReplyProducingMessageHandler.java:260)  at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.sendReplyMessage(AbstractReplyProducingMessageHandler.java:241)  at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.produceReply(AbstractReplyProducingMessageHandler.java:205)  at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleResult(AbstractReplyProducingMessageHandler.java:199)  at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:177)  at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78)  at org.springframework.integration.dispatcher.AbstractDispatcher.tryOptimizedDispatch(AbstractDispatcher.java:116)  at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:101)  at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)  at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)  at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:255)  at org.springframework.xd.dirt.integration.redis.RedisMessageBus$1$1.doWithRetry(RedisMessageBus.java:284)  at org.springframework.xd.dirt.integration.redis.RedisMessageBus$1$1.doWithRetry(RedisMessageBus.java:280)  at org.springframework.retry.support.RetryTemplate.doExecute(RetryTemplate.java:263)  at org.springframework.retry.support.RetryTemplate.execute(RetryTemplate.java:168)  at org.springframework.xd.dirt.integration.redis.RedisMessageBus$1.doSend(RedisMessageBus.java:280)  at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:255)  at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:223)  at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:109)  at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:44)  at org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:94)  at org.springframework.integration.endpoint.MessageProducerSupport.sendMessage(MessageProducerSupport.java:98)  at org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint.popMessageAndSend(RedisQueueMessageDrivenEndpoint.java:211)  at org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint.access$300(RedisQueueMessageDrivenEndpoint.java:50)  at org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint$ListenerTask.run(RedisQueueMessageDrivenEndpoint.java:290)  at org.springframework.integration.util.ErrorHandlingTaskExecutor$1.run(ErrorHandlingTaskExecutor.java:52)  at java.lang.Thread.run(Thread.java:744) Caused by: org.springframework.messaging.MessageHandlingException: message not a String  at org.springframework.xd.integration.hadoop.outbound.HdfsDataStoreMessageHandler.doWrite(HdfsDataStoreMessageHandler.java:75)  at org.springframework.xd.integration.hadoop.outbound.HdfsStoreMessageHandler.handleMessageInternal(HdfsStoreMessageHandler.java:126)  ... 60 more  ""","""""""deploying stream:  stream create foo2 --definition """"""""jdbc --fixedDelay=600 --split=1 --url='jdbc:sqlserver://ip:port;database=XXXXX' --driverClassName=com.microsoft.sqlserver.jdbc.SQLServerDriver --username=XXXX --password=XXX--query='select * from abc' | hdfs --fsUri=hdfs://hadopIP:port --directory=xddir"""""""" --deploy  It is throwing error:  19:17:10,087 1.0.2.RELEASE ERROR inbound.foo1.0-redis:queue-inbound-channel-adapter1 redis.RedisMessageBus$1 - Failed to deliver message; retries exhausted; message sent to queue 'ERRORS:name' org.springframework.messaging.MessageHandlingException: failed to write Message payload to HDFS  at org.springframework.xd.integration.hadoop.outbound.HdfsStoreMessageHandler.handleMessageInternal(HdfsStoreMessageHandler.java:129)  at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78)  at sun.reflect.GeneratedMethodAccessor89.invoke(Unknown Source)  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)  at java.lang.reflect.Method.invoke(Method.java:606)  at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)  at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)  at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)  at org.springframework.integration.monitor.SimpleMessageHandlerMetrics.handleMessage(SimpleMessageHandlerMetrics.java:106)  at org.springframework.integration.monitor.SimpleMessageHandlerMetrics.invoke(SimpleMessageHandlerMetrics.java:86)  at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)  at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)  at com.sun.proxy.$Proxy100.handleMessage(Unknown Source)  at org.springframework.integration.dispatcher.AbstractDispatcher.tryOptimizedDispatch(AbstractDispatcher.java:116)  at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:101)  at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)  at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)  at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:255)  at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:223)  at sun.reflect.GeneratedMethodAccessor88.invoke(Unknown Source)  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)  at java.lang.reflect.Method.invoke(Method.java:606)  at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)  at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)  at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)  at org.springframework.integration.monitor.DirectChannelMetrics.monitorSend(DirectChannelMetrics.java:113)  at org.springframework.integration.monitor.DirectChannelMetrics.doInvoke(DirectChannelMetrics.java:97)  at org.springframework.integration.monitor.DirectChannelMetrics.invoke(DirectChannelMetrics.java:91)  at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)  at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)  at com.sun.proxy.$Proxy63.send(Unknown Source)  at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:109)  at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:44)  at org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:94)  at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.sendMessage(AbstractReplyProducingMessageHandler.java:260)  at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.sendReplyMessage(AbstractReplyProducingMessageHandler.java:241)  at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.produceReply(AbstractReplyProducingMessageHandler.java:205)  at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleResult(AbstractReplyProducingMessageHandler.java:199)  at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:177)  at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78)  at org.springframework.integration.dispatcher.AbstractDispatcher.tryOptimizedDispatch(AbstractDispatcher.java:116)  at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:101)  at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)  at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)  at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:255)  at org.springframework.xd.dirt.integration.redis.RedisMessageBus$1$1.doWithRetry(RedisMessageBus.java:284)  at org.springframework.xd.dirt.integration.redis.RedisMessageBus$1$1.doWithRetry(RedisMessageBus.java:280)  at org.springframework.retry.support.RetryTemplate.doExecute(RetryTemplate.java:263)  at org.springframework.retry.support.RetryTemplate.execute(RetryTemplate.java:168)  at org.springframework.xd.dirt.integration.redis.RedisMessageBus$1.doSend(RedisMessageBus.java:280)  at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:255)  at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:223)  at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:109)  at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:44)  at org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:94)  at org.springframework.integration.endpoint.MessageProducerSupport.sendMessage(MessageProducerSupport.java:98)  at org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint.popMessageAndSend(RedisQueueMessageDrivenEndpoint.java:211)  at org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint.access$300(RedisQueueMessageDrivenEndpoint.java:50)  at org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint$ListenerTask.run(RedisQueueMessageDrivenEndpoint.java:290)  at org.springframework.integration.util.ErrorHandlingTaskExecutor$1.run(ErrorHandlingTaskExecutor.java:52)  at java.lang.Thread.run(Thread.java:744) Caused by: org.springframework.messaging.MessageHandlingException: message not a String  at org.springframework.xd.integration.hadoop.outbound.HdfsDataStoreMessageHandler.doWrite(HdfsDataStoreMessageHandler.java:75)  at org.springframework.xd.integration.hadoop.outbound.HdfsStoreMessageHandler.handleMessageInternal(HdfsStoreMessageHandler.java:126)  ... 60 more  """"""",,2567.0
1,XD-2601,BUG,Done,MEDIUM,"""Mismatch between configuration class and script XML for location/script""","""The *org.springframework.xd.module.options.mixins.ScriptMixin* options class shipped with XD 1.0.3 refers to *script* rather than *location* however the XML configuration still references *$\{location\}* in the service activator:  {noformat}  <service-activator output-channel=""""output"""" input-channel=""""input"""">   <int-groovy:script location=""""${location}"""" script-variable-generator=""""variableGenerator"""" refresh-check-delay=""""60""""/>  </service-activator> {noformat}  Creating a stream using the old *location* argument no longer works obviously:  {noformat} xd:>stream create myJobArchiveTrigger --definition """"tap:job:myJob.job > script --location=job-status.groovy --variables='tgtStatus=COMPLETED' > queue:job:archiveJob"""" --deploy Command failed org.springframework.xd.rest.client.impl.SpringXDException: Error with option(s) for module script of type processor     location: option named 'location' is not supported {noformat}  Creating the same stream using *--script* reports success at the shell prompt but results in an error in the container/admin logs:  {noformat} Caused by: java.lang.IllegalArgumentException: Could not resolve placeholder 'location' in string value """"${location}"""" {noformat}  Working around this by overriding the XML setting in our deployment:  {noformat}  <service-activator output-channel=""""output"""" input-channel=""""input"""">   <int-groovy:script location=""""${script}"""" script-variable-generator=""""variableGenerator"""" refresh-check-delay=""""60""""/>  </service-activator> {noformat}""","""""""The *org.springframework.xd.module.options.mixins.ScriptMixin* options class shipped with XD 1.0.3 refers to *script* rather than *location* however the XML configuration still references *$\{location\}* in the service activator:    Creating a stream using the old *location* argument no longer works obviously:    Creating the same stream using *--script* reports success at the shell prompt but results in an error in the container/admin logs:    Working around this by overriding the XML setting in our deployment:  """"""","""  <service-activator output-channel=""""""""output"""""""" input-channel=""""""""input"""""""">   <int-groovy:script location=""""""""${location}"""""""" script-variable-generator=""""""""variableGenerator"""""""" refresh-check-delay=""""""""60""""""""/>  </service-activator>  xd:>stream create myJobArchiveTrigger --definition """"""""tap:job:myJob.job > script --location=job-status.groovy --variables='tgtStatus=COMPLETED' > queue:job:archiveJob"""""""" --deploy Command failed org.springframework.xd.rest.client.impl.SpringXDException: Error with option(s) for module script of type processor     location: option named 'location' is not supported  Caused by: java.lang.IllegalArgumentException: Could not resolve placeholder 'location' in string value """"""""${location}""""""""   <service-activator output-channel=""""""""output"""""""" input-channel=""""""""input"""""""">   <int-groovy:script location=""""""""${script}"""""""" script-variable-generator=""""""""variableGenerator"""""""" refresh-check-delay=""""""""60""""""""/>  </service-activator> """,1.0
1,XD-2600,FEATURE,Done,MEDIUM,"""Backport XD-2411 to 1.0.x branch""","""This fix for RichGauge should go into the 1.0.x line.""","""""""This fix for RichGauge should go into the 1.0.x line.""""""",,1.0
1,XD-2599,FEATURE,Done,MEDIUM,"""Create a BroadcasterMessageHandler that uses a 'Serialized' Broadcaster""","""This is a parallel implementation to the RxJava   https://github.com/spring-projects/spring-xd/blob/master/spring-xd-rxjava/src/main/java/org/springframework/xd/rxjava/SubjectMessageHandler.java  That will allow multiple threads to broadcast an event but allow processing to occur one at a time on any thread.""","""""""This is a parallel implementation to the RxJava   https://github.com/spring-projects/spring-xd/blob/master/spring-xd-rxjava/src/main/java/org/springframework/xd/rxjava/SubjectMessageHandler.java  That will allow multiple threads to broadcast an event but allow processing to occur one at a time on any thread.""""""",,2.0
1,XD-2598,IMPROVEMENT,Done,MEDIUM,"""Update PostgreSQL JDBC Driver Version""","""Update the supplied PostgreSQL JDBC Driver to the latest version (9.3-1102 - 2014-07-10), the current supplied version is from 2012.   In our particular use case the latest driver allows use of the JDBC connection.unwrap feature which gives access to the underlying connection from a pooled connection which in turn enables use of the postgres copyManager.copyIn functionality which can speed up batch inserts in a batch process.   See http://jdbc.postgresql.org/documentation/changelog.html  ""","""""""Update the supplied PostgreSQL JDBC Driver to the latest version (9.3-1102 - 2014-07-10), the current supplied version is from 2012.   In our particular use case the latest driver allows use of the JDBC connection.unwrap feature which gives access to the underlying connection from a pooled connection which in turn enables use of the postgres copyManager.copyIn functionality which can speed up batch inserts in a batch process.   See http://jdbc.postgresql.org/documentation/changelog.html  """"""",,2.0
1,XD-2597,FEATURE,Done,HIGH,"""Add an """"xd-yarn info"""" command to list admin servers and ports""","""As a user deploying XD on YARN I need a convenient way to get info like the admin port for my current deployment.  Best way, for now, would be to add an info command to the xd-yarn script.  With the latest changes the admin server runs on a random port when we deploy to YARN. In order for the user to connect they would have to query Zookeeper. This is inconvenient.""","""""""As a user deploying XD on YARN I need a convenient way to get info like the admin port for my current deployment.  Best way, for now, would be to add an info command to the xd-yarn script.  With the latest changes the admin server runs on a random port when we deploy to YARN. In order for the user to connect they would have to query Zookeeper. This is inconvenient.""""""",,5.0
1,XD-2595,FEATURE,Done,MEDIUM,"""Test recent Hadoop distro changes""","""Test basic functionality (hdfs sink, jdbchdfs job) on hadoop26, hdp22, cdh5, phd21  Test XD on YARN on hadoop26, hdp22, cdh5 and phd21 ""","""""""Test basic functionality (hdfs sink, jdbchdfs job) on hadoop26, hdp22, cdh5, phd21  Test XD on YARN on hadoop26, hdp22, cdh5 and phd21 """"""",,8.0
1,XD-2594,FEATURE,Done,MEDIUM,"""Update spring-data-hadoop version to 2.1.0.RC1""","""Update spring-data-hadoop version to 2.1.0.RC1. This also includes updating the following:  - adding hadoop26 (Apache Hadoop 2.6.0) as distro - adding hdp22 (Hortonworks HDP 2.2) as distro - set default distro to hadoop26 - update cdh5 to version 5.3.0 - remove older distros - hadoop24, hdp21 ""","""""""Update spring-data-hadoop version to 2.1.0.RC1. This also includes updating the following:  - adding hadoop26 (Apache Hadoop 2.6.0) as distro - adding hdp22 (Hortonworks HDP 2.2) as distro - set default distro to hadoop26 - update cdh5 to version 5.3.0 - remove older distros - hadoop24, hdp21 """"""",,3.0
1,XD-2593,BUG,Done,MEDIUM,"""Web UI is not displayed""","""Web UI management interface does not display the stream  list data or deploy status  js ERROR? definitions.js:28 Uncaught TypeError: undefined is not a function  eg?http://120.27.44.69:9393/admin-ui/#/streams/definitions""","""""""Web UI management interface does not display the stream  list data or deploy status  js ERROR? definitions.js:28 Uncaught TypeError: undefined is not a function  eg?http://120.27.44.69:9393/admin-ui/#/streams/definitions""""""",,3.0
1,XD-2592,FEATURE,Done,HIGH,"""XD Yarn deployment requires the ability to set permsize""","""When deploying XD using Java 7 the user must be able to set the permsize to a value larger than the default.   The reason this is required is that if we deploy a gemfire component more than 2 times or a kafka source & sink more than 2 times, stream deployment begins to fail.    The only exception that was captured was the following: {noformat} Exception in thread """"ec2Test3_ip-10-146-213-31-1421176704238-e1786039_watcher_executor"""" Exception: java.lang.OutOfMemoryError thrown from the UncaughtExceptionHandler in thread """"ec2Test3_ip-10-146-213-31-1421176704238-e1786039_watcher_executor {noformat}  Logs are not available at this time.  ""","""""""When deploying XD using Java 7 the user must be able to set the permsize to a value larger than the default.   The reason this is required is that if we deploy a gemfire component more than 2 times or a kafka source & sink more than 2 times, stream deployment begins to fail.    The only exception that was captured was the following:   Logs are not available at this time.  """"""",""" Exception in thread """"""""ec2Test3_ip-10-146-213-31-1421176704238-e1786039_watcher_executor"""""""" Exception: java.lang.OutOfMemoryError thrown from the UncaughtExceptionHandler in thread """"""""ec2Test3_ip-10-146-213-31-1421176704238-e1786039_watcher_executor """,3.0
1,XD-2591,FEATURE,Done,MEDIUM,"""Bump Spring Integration to 4.1.2; Spring AMQP to 1.4.2""","""Bump Spring Integration to 4.1.2; Spring AMQP to 1.4.2""","""Bump Spring Integration to 4.1.2; Spring AMQP to 1.4.2""",,1.0
1,XD-2590,FEATURE,Done,MEDIUM,"""Create MessageConverter interface to allow user extensions""","""As a user, I'd like to have the option to extend the default message handling behavior for HTTP source-module so that I can override the settings via _servers.yml_ to control the default message size.  *Notes:* The adapter currently has that hard-coded (1MB limit) in the HttpChunkAggregator. We will have to expose this property for overrides. [Related PR|https://github.com/spring-projects/spring-xd/pull/1367].""","""""""As a user, I'd like to have the option to extend the default message handling behavior for HTTP source-module so that I can override the settings via _servers.yml_ to control the default message size.  *Notes:* The adapter currently has that hard-coded (1MB limit) in the HttpChunkAggregator. We will have to expose this property for overrides. [Related PR|https://github.com/spring-projects/spring-xd/pull/1367].""""""",,3.0
1,XD-2589,FEATURE,Done,MEDIUM,"""Create sample application for RxJava""","""Create port of https://github.com/spring-projects/spring-xd-samples/tree/master/reactor-moving-average based on RxJava""","""""""Create port of https://github.com/spring-projects/spring-xd-samples/tree/master/reactor-moving-average based on RxJava""""""",,2.0
1,XD-2588,FEATURE,Done,MEDIUM,"""Remove all deprecated compile warnings""","""Run a clean gradle build to identify all warnings.""","""""""Run a clean gradle build to identify all warnings.""""""",,3.0
1,XD-2586,MAINTENANCE,Done,MEDIUM,"""Update to Reactor 2.0 build snapshots""","""Update to Reactor 2.0 build snapshots""","""Update to Reactor 2.0 build snapshots""",,1.0
1,XD-2584,FEATURE,Done,MEDIUM,"""Upgrade grunt/node plugins""","""Upgrade in 1.0.x branch what was done in this commit on master.  https://github.com/spring-projects/spring-xd/commit/16062d771e23187a1d9e8d549abc646ff44e435b ""","""""""Upgrade in 1.0.x branch what was done in this commit on master.  https://github.com/spring-projects/spring-xd/commit/16062d771e23187a1d9e8d549abc646ff44e435b """"""",,1.0
1,XD-2583,BUG,Done,MEDIUM,"""Spring XD Admin UI does not show all the streams""","""From Spring XD Shell, running this command """"stream list"""", we counted 30 streams, however Spring XD Admin UI shows only 20. When destroying some streams from Admin UI, the others that was not in the list start appearing. We have not reviewed if there is a configuration parameter that tells how many streams to show in the Admin UI.""","""""""From Spring XD Shell, running this command """"""""stream list"""""""", we counted 30 streams, however Spring XD Admin UI shows only 20. When destroying some streams from Admin UI, the others that was not in the list start appearing. We have not reviewed if there is a configuration parameter that tells how many streams to show in the Admin UI.""""""",,5.0
1,XD-2582,IMPROVEMENT,Done,MEDIUM,"""Provide missing JARs to enable #xpath() SpEL""","""Spring XD should package *spring-integration-xml* JAR within distribution so we can invoke *#xpath()* SpEL from processors, e.g. using transformer:  {code}... | transform --expression='#xpath(payload, """"/*[name()=''Datasource'']/*[name()=''row'']/text()"""" | ... {code} ""","""""""Spring XD should package *spring-integration-xml* JAR within distribution so we can invoke *#xpath()* SpEL from processors, e.g. using transformer:   """"""","""... | transform --expression='#xpath(payload, """"""""/*[name()=''Datasource'']/*[name()=''row'']/text()"""""""" | ... """,1.0
1,XD-2580,BUG,Done,URGENT,"""Failed to create a stream with Script processor""","""I attempted to create a stream with a Script processor using Spring XD shell: {code}xd:>stream create --name test1 --definition """"tcp --port=17654 | script --location=print-stacktrace.groovy | null"""" Command failed org.springframework.xd.rest.client.impl.SpringXDException: Error with option(s) for module script of type processor:     location: option named 'location' is not supported {code}  I've corrected the syntax as described in docs by replacing --location with --script: {code}xd:>stream create --name test1 --definition """"tcp --port=17654 | script --script=print-stacktrace.groovy | null"""" Created new stream 'test1'{code}  My stream was created but the deployment failed with exception: {code} 20:04:45,105 1.0.3.RELEASE  INFO Deployer server.StreamDeploymentListener - Deployment status for stream 'test1': DeploymentStatus{state=failed,error(s)=org.springframework.beans.factory.BeanDefinitionStoreException: Invalid bean definition with name 'org.springframework.integration.config.ServiceActivatorFactoryBean#0' defined in null: Could not resolve placeholder 'location' in string value """"${location}""""; nested exception is java.lang.IllegalArgumentException: Could not resolve placeholder 'location' in string value """"${location}""""  at org.springframework.beans.factory.config.PlaceholderConfigurerSupport.doProcessProperties(PlaceholderConfigurerSupport.java:211)  at org.springframework.context.support.PropertySourcesPlaceholderConfigurer.processProperties(PropertySourcesPlaceholderConfigurer.java:180)  at org.springframework.context.support.PropertySourcesPlaceholderConfigurer.postProcessBeanFactory(PropertySourcesPlaceholderConfigurer.java:155)  at org.springframework.context.support.PostProcessorRegistrationDelegate.invokeBeanFactoryPostProcessors(PostProcessorRegistrationDelegate.java:265)  at org.springframework.context.support.PostProcessorRegistrationDelegate.invokeBeanFactoryPostProcessors(PostProcessorRegistrationDelegate.java:162)  at org.springframework.context.support.AbstractApplicationContext.invokeBeanFactoryPostProcessors(AbstractApplicationContext.java:609)  at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:464)  at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:691)  at org.springframework.boot.SpringApplication.run(SpringApplication.java:320)  at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:142)  at org.springframework.xd.module.core.SimpleModule.initialize(SimpleModule.java:201)  at org.springframework.xd.dirt.module.ModuleDeployer.doDeploy(ModuleDeployer.java:217)  at org.springframework.xd.dirt.module.ModuleDeployer.deploy(ModuleDeployer.java:200)  at org.springframework.xd.dirt.server.DeploymentListener.deployModule(DeploymentListener.java:363)  at org.springframework.xd.dirt.server.DeploymentListener.deployStreamModule(DeploymentListener.java:332)  at org.springframework.xd.dirt.server.DeploymentListener.onChildAdded(DeploymentListener.java:179)  at org.springframework.xd.dirt.server.DeploymentListener.childEvent(DeploymentListener.java:147)  at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:509)  at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:503)  at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)  at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297)  at org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:83)  at org.apache.curator.framework.recipes.cache.PathChildrenCache.callListeners(PathChildrenCache.java:500)  at org.apache.curator.framework.recipes.cache.EventOperation.invoke(EventOperation.java:35)  at org.apache.curator.framework.recipes.cache.PathChildrenCache$10.run(PathChildrenCache.java:762)  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)  at java.util.concurrent.FutureTask.run(FutureTask.java:262)  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)  at java.util.concurrent.FutureTask.run(FutureTask.java:262)  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)  at java.lang.Thread.run(Thread.java:745) Caused by: java.lang.IllegalArgumentException: Could not resolve placeholder 'location' in string value """"${location}""""  at org.springframework.util.PropertyPlaceholderHelper.parseStringValue(PropertyPlaceholderHelper.java:174)  at org.springframework.util.PropertyPlaceholderHelper.replacePlaceholders(PropertyPlaceholderHelper.java:126)  at org.springframework.core.env.AbstractPropertyResolver.doResolvePlaceholders(AbstractPropertyResolver.java:194)  at org.springframework.core.env.AbstractPropertyResolver.resolveRequiredPlaceholders(AbstractPropertyResolver.java:158)  at org.springframework.context.support.PropertySourcesPlaceholderConfigurer$2.resolveStringValue(PropertySourcesPlaceholderConfigurer.java:175)  at org.springframework.beans.factory.config.BeanDefinitionVisitor.resolveStringValue(BeanDefinitionVisitor.java:282)  at org.springframework.beans.factory.config.BeanDefinitionVisitor.resolveValue(BeanDefinitionVisitor.java:209)  at org.springframework.beans.factory.config.BeanDefinitionVisitor.visitIndexedArgumentValues(BeanDefinitionVisitor.java:150)  at org.springframework.beans.factory.config.BeanDefinitionVisitor.visitBeanDefinition(BeanDefinitionVisitor.java:84)  at org.springframework.beans.factory.config.BeanDefinitionVisitor.resolveValue(BeanDefinitionVisitor.java:169)  at org.springframework.beans.factory.config.BeanDefinitionVisitor.visitIndexedArgumentValues(BeanDefinitionVisitor.java:150)  at org.springframework.beans.factory.config.BeanDefinitionVisitor.visitBeanDefinition(BeanDefinitionVisitor.java:84)  at org.springframework.beans.factory.config.BeanDefinitionVisitor.resolveValue(BeanDefinitionVisitor.java:169)  at org.springframework.beans.factory.config.BeanDefinitionVisitor.visitPropertyValues(BeanDefinitionVisitor.java:141)  at org.springframework.beans.factory.config.BeanDefinitionVisitor.visitBeanDefinition(BeanDefinitionVisitor.java:82)  at org.springframework.beans.factory.config.PlaceholderConfigurerSupport.doProcessProperties(PlaceholderConfigurerSupport.java:208)  ... 31 more {code}  # [script.xml in 1.0.3 tag|https://github.com/spring-projects/spring-xd/blob/v1.0.3/modules/processor/script/config/script.xml] uses {code}<int-groovy:script location=""""${location}"""" ...{code} # The [Script 1.0.3 processor docs|http://docs.spring.io/autorepo/docs/spring-xd/1.0.3.RELEASE/reference/html/#script] have issues with properties naming e.g. example is using --location while later --script is used. Same with --propertiesLocation and --properties-location.""","""""""I attempted to create a stream with a Script processor using Spring XD shell:   I've corrected the syntax as described in docs by replacing --location with --script:   My stream was created but the deployment failed with exception:   # [script.xml in 1.0.3 tag|https://github.com/spring-projects/spring-xd/blob/v1.0.3/modules/processor/script/config/script.xml] uses  # The [Script 1.0.3 processor docs|http://docs.spring.io/autorepo/docs/spring-xd/1.0.3.RELEASE/reference/html/#script] have issues with properties naming e.g. example is using --location while later --script is used. Same with --propertiesLocation and --properties-location.""""""","""xd:>stream create --name test1 --definition """"""""tcp --port=17654 | script --location=print-stacktrace.groovy | null"""""""" Command failed org.springframework.xd.rest.client.impl.SpringXDException: Error with option(s) for module script of type processor:     location: option named 'location' is not supported xd:>stream create --name test1 --definition """"""""tcp --port=17654 | script --script=print-stacktrace.groovy | null"""""""" Created new stream 'test1' 20:04:45,105 1.0.3.RELEASE  INFO Deployer server.StreamDeploymentListener - Deployment status for stream 'test1': DeploymentStatus{state=failed,error(s)=org.springframework.beans.factory.BeanDefinitionStoreException: Invalid bean definition with name 'org.springframework.integration.config.ServiceActivatorFactoryBean#0' defined in null: Could not resolve placeholder 'location' in string value """"""""${location}""""""""; nested exception is java.lang.IllegalArgumentException: Could not resolve placeholder 'location' in string value """"""""${location}""""""""  at org.springframework.beans.factory.config.PlaceholderConfigurerSupport.doProcessProperties(PlaceholderConfigurerSupport.java:211)  at org.springframework.context.support.PropertySourcesPlaceholderConfigurer.processProperties(PropertySourcesPlaceholderConfigurer.java:180)  at org.springframework.context.support.PropertySourcesPlaceholderConfigurer.postProcessBeanFactory(PropertySourcesPlaceholderConfigurer.java:155)  at org.springframework.context.support.PostProcessorRegistrationDelegate.invokeBeanFactoryPostProcessors(PostProcessorRegistrationDelegate.java:265)  at org.springframework.context.support.PostProcessorRegistrationDelegate.invokeBeanFactoryPostProcessors(PostProcessorRegistrationDelegate.java:162)  at org.springframework.context.support.AbstractApplicationContext.invokeBeanFactoryPostProcessors(AbstractApplicationContext.java:609)  at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:464)  at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:691)  at org.springframework.boot.SpringApplication.run(SpringApplication.java:320)  at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:142)  at org.springframework.xd.module.core.SimpleModule.initialize(SimpleModule.java:201)  at org.springframework.xd.dirt.module.ModuleDeployer.doDeploy(ModuleDeployer.java:217)  at org.springframework.xd.dirt.module.ModuleDeployer.deploy(ModuleDeployer.java:200)  at org.springframework.xd.dirt.server.DeploymentListener.deployModule(DeploymentListener.java:363)  at org.springframework.xd.dirt.server.DeploymentListener.deployStreamModule(DeploymentListener.java:332)  at org.springframework.xd.dirt.server.DeploymentListener.onChildAdded(DeploymentListener.java:179)  at org.springframework.xd.dirt.server.DeploymentListener.childEvent(DeploymentListener.java:147)  at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:509)  at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:503)  at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)  at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297)  at org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:83)  at org.apache.curator.framework.recipes.cache.PathChildrenCache.callListeners(PathChildrenCache.java:500)  at org.apache.curator.framework.recipes.cache.EventOperation.invoke(EventOperation.java:35)  at org.apache.curator.framework.recipes.cache.PathChildrenCache$10.run(PathChildrenCache.java:762)  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)  at java.util.concurrent.FutureTask.run(FutureTask.java:262)  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)  at java.util.concurrent.FutureTask.run(FutureTask.java:262)  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)  at java.lang.Thread.run(Thread.java:745) Caused by: java.lang.IllegalArgumentException: Could not resolve placeholder 'location' in string value """"""""${location}""""""""  at org.springframework.util.PropertyPlaceholderHelper.parseStringValue(PropertyPlaceholderHelper.java:174)  at org.springframework.util.PropertyPlaceholderHelper.replacePlaceholders(PropertyPlaceholderHelper.java:126)  at org.springframework.core.env.AbstractPropertyResolver.doResolvePlaceholders(AbstractPropertyResolver.java:194)  at org.springframework.core.env.AbstractPropertyResolver.resolveRequiredPlaceholders(AbstractPropertyResolver.java:158)  at org.springframework.context.support.PropertySourcesPlaceholderConfigurer$2.resolveStringValue(PropertySourcesPlaceholderConfigurer.java:175)  at org.springframework.beans.factory.config.BeanDefinitionVisitor.resolveStringValue(BeanDefinitionVisitor.java:282)  at org.springframework.beans.factory.config.BeanDefinitionVisitor.resolveValue(BeanDefinitionVisitor.java:209)  at org.springframework.beans.factory.config.BeanDefinitionVisitor.visitIndexedArgumentValues(BeanDefinitionVisitor.java:150)  at org.springframework.beans.factory.config.BeanDefinitionVisitor.visitBeanDefinition(BeanDefinitionVisitor.java:84)  at org.springframework.beans.factory.config.BeanDefinitionVisitor.resolveValue(BeanDefinitionVisitor.java:169)  at org.springframework.beans.factory.config.BeanDefinitionVisitor.visitIndexedArgumentValues(BeanDefinitionVisitor.java:150)  at org.springframework.beans.factory.config.BeanDefinitionVisitor.visitBeanDefinition(BeanDefinitionVisitor.java:84)  at org.springframework.beans.factory.config.BeanDefinitionVisitor.resolveValue(BeanDefinitionVisitor.java:169)  at org.springframework.beans.factory.config.BeanDefinitionVisitor.visitPropertyValues(BeanDefinitionVisitor.java:141)  at org.springframework.beans.factory.config.BeanDefinitionVisitor.visitBeanDefinition(BeanDefinitionVisitor.java:82)  at org.springframework.beans.factory.config.PlaceholderConfigurerSupport.doProcessProperties(PlaceholderConfigurerSupport.java:208)  ... 31 more <int-groovy:script location=""""""""${location}"""""""" ...""",1.0
1,XD-2578,BUG,Done,URGENT,"""Custom Module not loading class from the module/lib.""","""The module/lib contains the necessary jars but it is not taken, I am attaching the simple custom module which contains just few beans. Here is how I am creating the job from the xd-shell job create --name job1 --definition """"job-custom"""" --deploy  The server logs contains this error *************************************************************************************** 10:43:20,193 1.1.0.M2  INFO DeploymentsPathChildrenCache-0 server.DeploymentListener - Deploying module [ModuleDescriptor@2963e1e2 moduleName = 'job-custom', moduleLabel = 'job-custom', group = 'job1', sourceChannelName = [null], sinkChannelName = [null], index = 0, type = job, parameters = map[[empty]], children = list[[empty]]] 10:43:20,697 1.1.0.M2 ERROR DeploymentsPathChildrenCache-0 boot.SpringApplication - Application startup failed java.lang.NoClassDefFoundError: org/springframework/oxm/Unmarshaller  at java.lang.Class.getDeclaredMethods0(Native Method)  at java.lang.Class.privateGetDeclaredMethods(Class.java:2531)  at java.lang.Class.getDeclaredMethods(Class.java:1855)  at org.springframework.util.ReflectionUtils.getDeclaredMethods(ReflectionUtils.java:571) ***************************************************************************************  I already had been discussing this over the forums but could not get much help. stackoverflow.com/questions/27878047/noclassdefinitionerror-with-simple-bean-configuration  If I place the spring-oxm jar in the spring-xd lib I get this error *************************************************************************************** java.lang.IllegalStateException: Cannot convert value of type [org.springframework.oxm.jaxb.Jaxb2Marshaller] to required type [org.springframework.oxm.Unmarshaller] for property 'unmarshaller': no matching editors or conversion strategy found  *************************************************************************************** ""","""""""The module/lib contains the necessary jars but it is not taken, I am attaching the simple custom module which contains just few beans. Here is how I am creating the job from the xd-shell job create --name job1 --definition """"""""job-custom"""""""" --deploy  The server logs contains this error *************************************************************************************** 10:43:20,193 1.1.0.M2  INFO DeploymentsPathChildrenCache-0 server.DeploymentListener - Deploying module [ModuleDescriptor@2963e1e2 moduleName = 'job-custom', moduleLabel = 'job-custom', group = 'job1', sourceChannelName = [null], sinkChannelName = [null], index = 0, type = job, parameters = map[[empty]], children = list[[empty]]] 10:43:20,697 1.1.0.M2 ERROR DeploymentsPathChildrenCache-0 boot.SpringApplication - Application startup failed java.lang.NoClassDefFoundError: org/springframework/oxm/Unmarshaller  at java.lang.Class.getDeclaredMethods0(Native Method)  at java.lang.Class.privateGetDeclaredMethods(Class.java:2531)  at java.lang.Class.getDeclaredMethods(Class.java:1855)  at org.springframework.util.ReflectionUtils.getDeclaredMethods(ReflectionUtils.java:571) ***************************************************************************************  I already had been discussing this over the forums but could not get much help. stackoverflow.com/questions/27878047/noclassdefinitionerror-with-simple-bean-configuration  If I place the spring-oxm jar in the spring-xd lib I get this error *************************************************************************************** java.lang.IllegalStateException: Cannot convert value of type [org.springframework.oxm.jaxb.Jaxb2Marshaller] to required type [org.springframework.oxm.Unmarshaller] for property 'unmarshaller': no matching editors or conversion strategy found  *************************************************************************************** """"""",,2.0
1,XD-2577,BUG,Done,HIGH,"""XD is not logging when deployed using yarn""","""When deploying XD (admin & container) using Yarn we only get the first 495 characters of the log which is the Ascii Art and Documentation links. {noformat}  _____                           __   _______ /  ___|          (-)             \ \ / /  _  \ \ `--. _ __  _ __ _ _ __   __ _   \ V /| | | |  `--. \ '_ \| '__| | '_ \ / _` |  / ^ \| | | | /\__/ / |_) | |  | | | | | (_| | / / \ \ |/ / \____/| .__/|_|  |_|_| |_|\__, | \/   \/___/       | |                  __/ |       |_|                 |___/ 1.1.0.BUILD-SNAPSHOT             eXtreme Data   Started : ContainerServerApplication Documentation: https://github.com/spring-projects/spring-xd/wiki {noformat}""","""""""When deploying XD (admin & container) using Yarn we only get the first 495 characters of the log which is the Ascii Art and Documentation links. """"""","""  _____                           __   _______ /  ___|          (-)             \ \ / /  _  \ \ `--. _ __  _ __ _ _ __   __ _   \ V /| | | |  `--. \ '_ \| '__| | '_ \ / _` |  / ^ \| | | | /\__/ / |_) | |  | | | | | (_| | / / \ \ |/ / \____/| .__/|_|  |_|_| |_|\__, | \/   \/___/       | |                  __/ |       |_|                 |___/ 1.1.0.BUILD-SNAPSHOT             eXtreme Data   Started : ContainerServerApplication Documentation: https://github.com/spring-projects/spring-xd/wiki """,5.0
1,XD-2576,FEATURE,Done,MEDIUM,"""Add RxJava based Stream processor module""","""As a user, I'd like to use RxJava based _processor_ module so that I can leverage RxJava APIs for data computations. ""","""""""As a user, I'd like to use RxJava based _processor_ module so that I can leverage RxJava APIs for data computations. """"""",,0.0
1,XD-2575,IMPROVEMENT,Done,HIGH,"""HDFS sink should provide rolloverTime option not only idleTiemout""","""When using HDFS sink with ildeTimeout and rollover options in stream definition we have noticed that idleTimeout does not give you a flexibility when you would prefer a file to rollover after specific time regardless of the activity/inactivity of the file.  Proposed option: rolloverTimeout timeout after file will be automatically closed  Link: #XD-2413""","""""""When using HDFS sink with ildeTimeout and rollover options in stream definition we have noticed that idleTimeout does not give you a flexibility when you would prefer a file to rollover after specific time regardless of the activity/inactivity of the file.  Proposed option: rolloverTimeout timeout after file will be automatically closed  Link: #XD-2413""""""",,5.0
1,XD-2574,IMPROVEMENT,Done,URGENT,"""Gemfire sink SpringXD module does not support multiple locators""","""Gemfire sink module accepts useLocator, host and port properties but this only allows to use one locator at a time.  We have a need to use the Gemfire sink SpringXD Module in our Seamless access project that we want to go live in Q1. The Version of SpringXD we planned on using is 1.1  However we need HA and we need to connect to a cluster with multiple locators. Problem is this isnt supported yet in SpringXD.  We have used multiple locators in many projects in EMC and we dont want to revert back to a situation where we have to put virtual IPs in front of locators just for SpringXD.  Ref to the SpringXD docs found in 1.0.3 and 1.1.0GA versions:  The locator option is mostly intended for integration with an existing GemFire installation in which the cache servers are configured to use locators in accordance with best practice. While GemFire supports configuration of multiple locators for failover, this is currently not supported in XD. However, using a single virtual IP backed by hardware routers for failover has proven to be an effective and simpler alternative.  ""","""""""Gemfire sink module accepts useLocator, host and port properties but this only allows to use one locator at a time.  We have a need to use the Gemfire sink SpringXD Module in our Seamless access project that we want to go live in Q1. The Version of SpringXD we planned on using is 1.1  However we need HA and we need to connect to a cluster with multiple locators. Problem is this isnt supported yet in SpringXD.  We have used multiple locators in many projects in EMC and we dont want to revert back to a situation where we have to put virtual IPs in front of locators just for SpringXD.  Ref to the SpringXD docs found in 1.0.3 and 1.1.0GA versions:  The locator option is mostly intended for integration with an existing GemFire installation in which the cache servers are configured to use locators in accordance with best practice. While GemFire supports configuration of multiple locators for failover, this is currently not supported in XD. However, using a single virtual IP backed by hardware routers for failover has proven to be an effective and simpler alternative.  """"""",,5.0
1,XD-2573,BUG,Done,HIGH,"""Full build with tests fail on Ubuntu""","""On Ubuntu 14.04 LTS using OpenJDK version  """"1.7.0_65""""  OpenJDK Runtime Environment (IcedTea 2.5.3) (7u71-2.5.3-0ubuntu0.14.04.1) OpenJDK 64-Bit Server VM (build 24.65-b04, mixed mode)  I see the following failures:  :spring-xd-dirt:test  org.springframework.xd.dirt.security.SingleNodeApplicationWithUserBasedSecurityTest > classMethod FAILED     org.springframework.beans.factory.BeanCreationException         Caused by: java.lang.IllegalStateException             Caused by: java.net.BindException  org.springframework.xd.dirt.security.SingleNodeApplicationWithLdapSimpleBindTest > classMethod FAILED     org.springframework.beans.factory.BeanCreationException         Caused by: java.lang.IllegalStateException             Caused by: java.net.BindException  org.springframework.xd.dirt.security.SingleNodeApplicationWithDefaultSecurityTest > classMethod FAILED     org.springframework.beans.factory.BeanCreationException         Caused by: java.lang.IllegalStateException             Caused by: java.net.BindException  org.springframework.xd.dirt.security.SingleNodeApplicationWithSslTest > classMethod FAILED     org.springframework.beans.factory.BeanCreationException         Caused by: java.lang.IllegalStateException             Caused by: java.net.BindException  org.springframework.xd.dirt.security.SingleNodeApplicationWithUsersFileTest > classMethod FAILED     org.springframework.beans.factory.BeanCreationException         Caused by: java.lang.IllegalStateException             Caused by: java.net.BindException  org.springframework.xd.dirt.security.SingleNodeApplicationWithLdapSearchAndBindTest > classMethod FAILED     org.springframework.beans.factory.BeanCreationException         Caused by: java.lang.IllegalStateException             Caused by: java.net.BindException  595 tests completed, 6 failed, 2 skipped :spring-xd-dirt:test FAILED  The test reports has this:  Caused by: java.lang.IllegalStateException: HSQLDB could not be started on 0.0.0.0:7714, state: SHUTDOWN  at org.springframework.xd.batch.hsqldb.server.HSQLServerBean.startServer(HSQLServerBean.java:162)  at org.springframework.xd.batch.hsqldb.server.HSQLServerBean.afterPropertiesSet(HSQLServerBean.java:82)  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1625)  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1562)  ... 42 more Caused by: java.net.BindException: Address already in use  at java.net.PlainSocketImpl.socketBind(Native Method)  at java.net.AbstractPlainSocketImpl.bind(AbstractPlainSocketImpl.java:376)  at java.net.ServerSocket.bind(ServerSocket.java:376)  at java.net.ServerSocket.<init>(ServerSocket.java:237)  at java.net.ServerSocket.<init>(ServerSocket.java:128)  at org.hsqldb.server.HsqlSocketFactory.createServerSocket(Unknown Source)  at org.hsqldb.server.Server.openServerSocket(Unknown Source)  at org.hsqldb.server.Server.run(Unknown Source)  at org.hsqldb.server.Server.access$000(Unknown Source)  at org.hsqldb.server.Server$ServerThread.run(Unknown Source)  So I assume I see this due HSQL running from another test.""","""""""On Ubuntu 14.04 LTS using OpenJDK version  """"""""1.7.0_65""""""""  OpenJDK Runtime Environment (IcedTea 2.5.3) (7u71-2.5.3-0ubuntu0.14.04.1) OpenJDK 64-Bit Server VM (build 24.65-b04, mixed mode)  I see the following failures:  :spring-xd-dirt:test  org.springframework.xd.dirt.security.SingleNodeApplicationWithUserBasedSecurityTest > classMethod FAILED     org.springframework.beans.factory.BeanCreationException         Caused by: java.lang.IllegalStateException             Caused by: java.net.BindException  org.springframework.xd.dirt.security.SingleNodeApplicationWithLdapSimpleBindTest > classMethod FAILED     org.springframework.beans.factory.BeanCreationException         Caused by: java.lang.IllegalStateException             Caused by: java.net.BindException  org.springframework.xd.dirt.security.SingleNodeApplicationWithDefaultSecurityTest > classMethod FAILED     org.springframework.beans.factory.BeanCreationException         Caused by: java.lang.IllegalStateException             Caused by: java.net.BindException  org.springframework.xd.dirt.security.SingleNodeApplicationWithSslTest > classMethod FAILED     org.springframework.beans.factory.BeanCreationException         Caused by: java.lang.IllegalStateException             Caused by: java.net.BindException  org.springframework.xd.dirt.security.SingleNodeApplicationWithUsersFileTest > classMethod FAILED     org.springframework.beans.factory.BeanCreationException         Caused by: java.lang.IllegalStateException             Caused by: java.net.BindException  org.springframework.xd.dirt.security.SingleNodeApplicationWithLdapSearchAndBindTest > classMethod FAILED     org.springframework.beans.factory.BeanCreationException         Caused by: java.lang.IllegalStateException             Caused by: java.net.BindException  595 tests completed, 6 failed, 2 skipped :spring-xd-dirt:test FAILED  The test reports has this:  Caused by: java.lang.IllegalStateException: HSQLDB could not be started on 0.0.0.0:7714, state: SHUTDOWN  at org.springframework.xd.batch.hsqldb.server.HSQLServerBean.startServer(HSQLServerBean.java:162)  at org.springframework.xd.batch.hsqldb.server.HSQLServerBean.afterPropertiesSet(HSQLServerBean.java:82)  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1625)  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1562)  ... 42 more Caused by: java.net.BindException: Address already in use  at java.net.PlainSocketImpl.socketBind(Native Method)  at java.net.AbstractPlainSocketImpl.bind(AbstractPlainSocketImpl.java:376)  at java.net.ServerSocket.bind(ServerSocket.java:376)  at java.net.ServerSocket.<init>(ServerSocket.java:237)  at java.net.ServerSocket.<init>(ServerSocket.java:128)  at org.hsqldb.server.HsqlSocketFactory.createServerSocket(Unknown Source)  at org.hsqldb.server.Server.openServerSocket(Unknown Source)  at org.hsqldb.server.Server.run(Unknown Source)  at org.hsqldb.server.Server.access$000(Unknown Source)  at org.hsqldb.server.Server$ServerThread.run(Unknown Source)  So I assume I see this due HSQL running from another test.""""""",,5.0
1,XD-2572,FEATURE,Done,MEDIUM,"""Set fixed NPM version for Grunt Gradle Plugin""","""Ensure build works in Windows environments""","""""""Ensure build works in Windows environments""""""",,1.0
1,XD-2571,MAINTENANCE,Done,MEDIUM,"""Update reactor-ip and syslog modules to Reactor 2.0 RC1""","""As a developer, I'd like to upgrade _reactor-ip_ and _syslog_ modules to Reactor 2.0 so that we can sync up with the latest release.""","""""""As a developer, I'd like to upgrade _reactor-ip_ and _syslog_ modules to Reactor 2.0 so that we can sync up with the latest release.""""""",,1.0
1,XD-2570,FEATURE,Done,MEDIUM,"""Create a new Broadcast stream per thread""","""The data that is entering a broadcast stream can only occur from one thread at a time to prevent race conditions inside the stream implementation.  The current handler shares a single broadcast stream.  Change to create a new one per thread usage.""","""""""The data that is entering a broadcast stream can only occur from one thread at a time to prevent race conditions inside the stream implementation.  The current handler shares a single broadcast stream.  Change to create a new one per thread usage.""""""",,2.0
1,XD-2569,FEATURE,Done,MEDIUM,"""Update to Spring Boot 1.2.1.RELEASE""","""Update to Spring Boot 1.2.1.RELEASE""","""Update to Spring Boot 1.2.1.RELEASE""",,3.0
1,XD-2568,FEATURE,Done,MEDIUM,"""Yarn Environment for XD Acceptance Tests""","""Create an 2.6 Yarn Environment on EC2 for which XD can be deployed for acceptance tests.""","""""""Create an 2.6 Yarn Environment on EC2 for which XD can be deployed for acceptance tests.""""""",,5.0
1,XD-2567,BUG,Done,MEDIUM,"""Strip MessageBus DeliveryMode Header""","""Since the messagebus refactoring, we now see   {noformat} 15:43:06,379 1.1.0.M2  WARN xdbus.foo.0-1 support.DefaultAmqpHeaderMapper - skipping header 'amqp_deliveryMode' since it is not of expected type [class org.springframework.amqp.core.MessageDeliveryMode], it is [class org.springframework.amqp.core.MessageDeliveryMode] {noformat}  When using a rabbit transport and a rabbit sink (the sink Spring AMQP is in its own classloader).""","""""""Since the messagebus refactoring, we now see     When using a rabbit transport and a rabbit sink (the sink Spring AMQP is in its own classloader).""""""",""" 15:43:06,379 1.1.0.M2  WARN xdbus.foo.0-1 support.DefaultAmqpHeaderMapper - skipping header 'amqp_deliveryMode' since it is not of expected type [class org.springframework.amqp.core.MessageDeliveryMode], it is [class org.springframework.amqp.core.MessageDeliveryMode] """,1.0
1,XD-2566,FEATURE,Done,MEDIUM,"""Add support to test XD on YARN in EC2""","""As a developer, I'd like to have acceptance test coverage for XD + YARN on EC2 so that I can verify simple XD features running on YARN on every build cycle.""","""""""As a developer, I'd like to have acceptance test coverage for XD + YARN on EC2 so that I can verify simple XD features running on YARN on every build cycle.""""""",,8.0
1,XD-2565,FEATURE,Done,MEDIUM,"""Remove MongoDB from main DIRT classpath""","""MongoDb driver is present on DIRT's classpath, while it should not (should be present on mongo-related modules though).  This is blocked by the shortcoming described here: https://github.com/spring-projects/spring-xd/pull/1116""","""""""MongoDb driver is present on DIRT's classpath, while it should not (should be present on mongo-related modules though).  This is blocked by the shortcoming described here: https://github.com/spring-projects/spring-xd/pull/1116""""""",,1.0
1,XD-2564,IMPROVEMENT,Done,MEDIUM,"""Enhance XD on YARN to use SHDP container clustering""","""Currently yarn runtime needs two yarn appmaster instances(one for admins, one for containers). SHDP's container grouping added functionality to run different type of containers within a same appmaster.  Beyond this, container grouping will also give more functionality like ramping containers up/down on-demand, creating groups with different settings dynamically and restarting failed containers.""","""""""Currently yarn runtime needs two yarn appmaster instances(one for admins, one for containers). SHDP's container grouping added functionality to run different type of containers within a same appmaster.  Beyond this, container grouping will also give more functionality like ramping containers up/down on-demand, creating groups with different settings dynamically and restarting failed containers.""""""",,2.0
1,XD-2563,BUG,Done,MEDIUM,"""XD on YARN broken due to missing messagebus libs""","""Admin on YARN simply fails because messagebus libs are not copied in place during a build.  Already tried and simple fix is for gradle/build-dist.gradle:  {code} task copyYarnMessageBusLibs(type: Copy) {   from """"$rootDir/lib/messagebus""""   into """"$buildDir/dist/spring-xd-yarn/xd-yarn/lib/messagebus"""" } {code}  and execute it together with copyMessageBusLibs task.""","""""""Admin on YARN simply fails because messagebus libs are not copied in place during a build.  Already tried and simple fix is for gradle/build-dist.gradle:    and execute it together with copyMessageBusLibs task.""""""",""" task copyYarnMessageBusLibs(type: Copy) {   from """"""""$rootDir/lib/messagebus""""""""   into """"""""$buildDir/dist/spring-xd-yarn/xd-yarn/lib/messagebus"""""""" } """,1.0
1,XD-2562,FEATURE,Done,MEDIUM,"""Move the Hadoop test dependencies to a different project""","""As a developer, I'd like to isolate the Hadoop tests in a different project so that the DIRT project doesn't have to depend upon, thus eliminating the incorrect CP file generation in eclipse. ""","""""""As a developer, I'd like to isolate the Hadoop tests in a different project so that the DIRT project doesn't have to depend upon, thus eliminating the incorrect CP file generation in eclipse. """"""",,3.0
1,XD-2561,BUG,Done,MEDIUM,"""Document minimum memory requirement for Gradle builds""","""The build failed on two classes from spring-xd-extension-process: ShellCommandProcessor.java and ShellCommandProcessorTests.java with error: FAILURE: Build failed with an exception.  * What went wrong: Execution failed for task ':spring-xd-extension-process:licenseTest'. > License violations were found: /Users/victor.chugunov/git/repos/spring/spring-xd/extensions/spring-xd-extension-process/src/test/java/org/springframework/xd/extension/process/ShellCommandProcessorTests.java}   In both classes the license is misplaced: package org.springframework.xd.extension.process;/*  *  *  * Copyright 2014 the original author or authors.  *  *  *  * Licensed under the Apache License, Version 2.0 (the """"License"""");  *  * you may not use this file except in compliance with the License.  *  * You may obtain a copy of the License at  *  *  *  * http://www.apache.org/licenses/LICENSE-2.0  *  *  *  * Unless required by applicable law or agreed to in writing, software  *  * distributed under the License is distributed on an """"AS IS"""" BASIS,  *  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  *  * See the License for the specific language governing permissions and  *  * limitations under the License.  *  */""","""""""The build failed on two classes from spring-xd-extension-process: ShellCommandProcessor.java and ShellCommandProcessorTests.java with error: FAILURE: Build failed with an exception.  * What went wrong: Execution failed for task ':spring-xd-extension-process:licenseTest'. > License violations were found: /Users/victor.chugunov/git/repos/spring/spring-xd/extensions/spring-xd-extension-process/src/test/java/org/springframework/xd/extension/process/ShellCommandProcessorTests.java}   In both classes the license is misplaced: package org.springframework.xd.extension.process;/*  *  *  * Copyright 2014 the original author or authors.  *  *  *  * Licensed under the Apache License, Version 2.0 (the """"""""License"""""""");  *  * you may not use this file except in compliance with the License.  *  * You may obtain a copy of the License at  *  *  *  * http://www.apache.org/licenses/LICENSE-2.0  *  *  *  * Unless required by applicable law or agreed to in writing, software  *  * distributed under the License is distributed on an """"""""AS IS"""""""" BASIS,  *  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  *  * See the License for the specific language governing permissions and  *  * limitations under the License.  *  */""""""",,1.0
1,XD-2557,FEATURE,Done,MEDIUM,"""Upgrade to Reactor 2.0 RC1""","""As a developer, I'd like to upgrade to Reactor 2.0 RC1 release so that we can synchronize with stable dependencies.""","""""""As a developer, I'd like to upgrade to Reactor 2.0 RC1 release so that we can synchronize with stable dependencies.""""""",,5.0
1,XD-2556,FEATURE,Done,MEDIUM,"""Reference documentation on RxJava Stream processor""","""Reference documentation on RxJava Stream processor""","""Reference documentation on RxJava Stream processor""",,1.0
1,XD-2551,FEATURE,Done,MEDIUM,"""Add support to read custom module artifact from HDFS""","""As a user, I'd like to have the option to store the custom module uber-jar in HDFS so that I can rely on the HA feature to reliably read and reinstall under failure scenarios. ""","""""""As a user, I'd like to have the option to store the custom module uber-jar in HDFS so that I can rely on the HA feature to reliably read and reinstall under failure scenarios. """"""",,3.0
1,XD-2550,FEATURE,Done,MEDIUM,"""Add support to upload custom module artifact to HDFS""","""As a user, I'd like to have the option to store the custom module uber-jar in HDFS so that I can rely on the HA feature to reliably read and reinstall under failure scenarios. ""","""""""As a user, I'd like to have the option to store the custom module uber-jar in HDFS so that I can rely on the HA feature to reliably read and reinstall under failure scenarios. """"""",,3.0
1,XD-2549,MAINTENANCE,Done,MEDIUM,"""Upgrade to SI-Kafka Extension release""","""We would want to upgrade SI Kafka extension dependency to inherit the refactoring work with Kafka simple consumer API.""","""""""We would want to upgrade SI Kafka extension dependency to inherit the refactoring work with Kafka simple consumer API.""""""",,1.0
1,XD-2548,FEATURE,Done,MEDIUM,"""Investigate why CPU startup is high for admin and container servers""","""As a performance tester, I'd like to investigate why there's high CPU startup time for both admin and container servers. Perhaps profiling would assist isolating the bottlenecks.   *Scope:* * Identify the bottlenecks * Document reasons * List pros/cons""","""""""As a performance tester, I'd like to investigate why there's high CPU startup time for both admin and container servers. Perhaps profiling would assist isolating the bottlenecks.   *Scope:* * Identify the bottlenecks * Document reasons * List pros/cons""""""",,8.0
1,XD-2547,FEATURE,Done,MEDIUM,"""Accept any file name for top level module resources""","""Expecting <module-name> in module configuration is brittle, especially in conjunction with module upload command which permits the module to be registered under a different name.  The convention should be dropped in favor of any file name. This requires at most one foo.xml, foo.groovy, and/or foo.properties in the top level config folder. It is an exception if multiples are found.  Accepting any file name provides the most flexibility without sacrificing backward compatibility (except in rare cases in which a module developer may have violated the multiple xml or properties files condition). An alternate approach requiring a well known file name such as 'spring-module' were rejected over concerns that it would break any existing custom module implementations.""","""""""Expecting <module-name> in module configuration is brittle, especially in conjunction with module upload command which permits the module to be registered under a different name.  The convention should be dropped in favor of any file name. This requires at most one foo.xml, foo.groovy, and/or foo.properties in the top level config folder. It is an exception if multiples are found.  Accepting any file name provides the most flexibility without sacrificing backward compatibility (except in rare cases in which a module developer may have violated the multiple xml or properties files condition). An alternate approach requiring a well known file name such as 'spring-module' were rejected over concerns that it would break any existing custom module implementations.""""""",,3.0
1,XD-2546,FEATURE,Done,MEDIUM,"""Create AMI for Spark Server installed""","""Create AMI for Spark Server installed""","""Create AMI for Spark Server installed""",,2.0
1,XD-2544,FEATURE,Done,MEDIUM,"""Create a loadGenerator source module""","""Create a load-generator source module that  will generate messages and dispatch messages to a XD stream.    ""","""""""Create a load-generator source module that  will generate messages and dispatch messages to a XD stream.    """"""",,5.0
1,XD-2542,FEATURE,Done,MEDIUM,"""Create MessageHandler for RxJava based processor modules""","""As a user, I'd like to have a flexible RxJava module so that it can as a processor.  ""","""""""As a user, I'd like to have a flexible RxJava module so that it can as a processor.  """"""",,8.0
1,XD-2541,FEATURE,Done,MEDIUM,"""Define developer facing interfaces for RxJava processors""","""As a user, I'd like to implement the core interface contract so that I can create a processor module that uses RxJava API.""","""""""As a user, I'd like to implement the core interface contract so that I can create a processor module that uses RxJava API.""""""",,1.0
1,XD-2539,BUG,Done,URGENT,"""XD 1.1.0.M2 Won't Run on Windows""","""See http://stackoverflow.com/questions/27725905/spring-xd-1-1-0-m2-fails-to-start  With {{XD_HOME}} set with back-whacks, it fails on {{\U...}} with {{XD_HOME}} set with whacks, it fails with {{/xd\...}}. The StackOverflow failure is similar.  1.0.3 works fine.  {noformat} set XD_HOME=C:\Users\gpr\Documents\spring-xd-1.1.0.M2-dist\spring-xd-1.1.0.M2\xd  Caused by: java.util.regex.PatternSyntaxException: Illegal/unsupported escape sequence near index 5 .*C:\Users\gpr\Documents\spring-xd-1.1.0.M2-dist\spring-xd-1.1.0.M2\xd\lib\messagebus\([^/]*).*  set XD_HOME=C:/Users/gpr/Documents/spring-xd-1.1.0.M2-dist/spring-xd-1.1.0.M2/xd   Caused by: java.util.regex.PatternSyntaxException: Illegal/unsupported escape sequence near index 71 .*C:/Users/gpr/Documents/spring-xd-1.1.0.M2-dist/spring-xd-1.1.0.M2/xd\lib\messagebus\([^/]*).* {noformat}""","""""""See http://stackoverflow.com/questions/27725905/spring-xd-1-1-0-m2-fails-to-start  With {{XD_HOME}} set with back-whacks, it fails on {{\U...}} with {{XD_HOME}} set with whacks, it fails with {{/xd\...}}. The StackOverflow failure is similar.  1.0.3 works fine.  """"""",""" set XD_HOME=C:\Users\gpr\Documents\spring-xd-1.1.0.M2-dist\spring-xd-1.1.0.M2\xd  Caused by: java.util.regex.PatternSyntaxException: Illegal/unsupported escape sequence near index 5 .*C:\Users\gpr\Documents\spring-xd-1.1.0.M2-dist\spring-xd-1.1.0.M2\xd\lib\messagebus\([^/]*).*  set XD_HOME=C:/Users/gpr/Documents/spring-xd-1.1.0.M2-dist/spring-xd-1.1.0.M2/xd   Caused by: java.util.regex.PatternSyntaxException: Illegal/unsupported escape sequence near index 71 .*C:/Users/gpr/Documents/spring-xd-1.1.0.M2-dist/spring-xd-1.1.0.M2/xd\lib\messagebus\([^/]*).* """,5.0
1,XD-2537,BUG,Done,MEDIUM,"""BackPort script.xml Bug Fix""","""An additional commit (https://github.com/spring-projects/spring-xd/commit/db1f585) for XD-2230 was applied only to master; it needs to be backported to 1.0.x.  {{s/$\{location\}/$\{script\}/}}   https://gopivotal-com.socialcast.com/messages/22909482""","""""""An additional commit (https://github.com/spring-projects/spring-xd/commit/db1f585) for XD-2230 was applied only to master; it needs to be backported to 1.0.x.  {{s/$\{location\}/$\{script\}/}}   https://gopivotal-com.socialcast.com/messages/22909482""""""",,1.0
1,XD-2533,FEATURE,Done,MEDIUM,"""Upgrade to Reactor 2.0 M2""","""Upgrade to Reactor 2.0 M2""","""Upgrade to Reactor 2.0 M2""",,1.0
1,XD-2532,BUG,Done,HIGH,"""Spark Application Job fails when using remote Spark Master""","""When executing a Spark Application Job on XD against a remote Spark Master we receive a CNF exception for FSDataInputStream.  Running against a local[1] Spark Master works normally.  ""","""""""When executing a Spark Application Job on XD against a remote Spark Master we receive a CNF exception for FSDataInputStream.  Running against a local[1] Spark Master works normally.  """"""",,5.0
1,XD-2531,MAINTENANCE,Done,MEDIUM,"""Document Sqoop job""","""As a user, I'd like to refer to the documentation so that I can connect to Sqoop as recommended and create job definition based on the exposed _metadata_ options. ""","""""""As a user, I'd like to refer to the documentation so that I can connect to Sqoop as recommended and create job definition based on the exposed _metadata_ options. """"""",,1.0
1,XD-2527,FEATURE,Done,MEDIUM,"""Add support to extend message compression ""","""As a user, I'd like to have the option to extend compression support so that I can override the defaults and customize as needed.  Follow-up from this PR: https://github.com/spring-projects/spring-xd/pull/1346""","""""""As a user, I'd like to have the option to extend compression support so that I can override the defaults and customize as needed.  Follow-up from this PR: https://github.com/spring-projects/spring-xd/pull/1346""""""",,3.0
1,XD-2524,IMPROVEMENT,Done,MEDIUM,"""Update deprecated jackson methods in TupleToJsonStringConverter""","""warning: [options] bootstrap class path not set in conjunction with -source 1.6 /home/<USER>spring-xd/spring-xd-tuple/src/main/java/org/springframework/xd/tuple/TupleToJsonStringConverter.java:53: warning: [deprecation] put(String,JsonNode) in ObjectNode has been deprecated      root.put(name, toObjectNode((Tuple) value));          ^ /home/<USER>spring-xd/spring-xd-tuple/src/main/java/org/springframework/xd/tuple/TupleToJsonStringConverter.java:57: warning: [deprecation] put(String,JsonNode) in ObjectNode has been deprecated      root.put(name, root.pojoNode(value));""","""""""warning: [options] bootstrap class path not set in conjunction with -source 1.6 /home/<USER>spring-xd/spring-xd-tuple/src/main/java/org/springframework/xd/tuple/TupleToJsonStringConverter.java:53: warning: [deprecation] put(String,JsonNode) in ObjectNode has been deprecated      root.put(name, toObjectNode((Tuple) value));          ^ /home/<USER>spring-xd/spring-xd-tuple/src/main/java/org/springframework/xd/tuple/TupleToJsonStringConverter.java:57: warning: [deprecation] put(String,JsonNode) in ObjectNode has been deprecated      root.put(name, root.pojoNode(value));""""""",,1.0
1,XD-2523,FEATURE,Done,MEDIUM,"""Add gradle build support for custom module projects""","""As a user, I'd like to have a gradle build option so that I can support module projects that will declare the Spring XD dependencies as provided, configure the boot plugin for 'MODULE' layout and other boilerplate build configuration.  This is dependent on Boot's module layout scoping issue: https://github.com/spring-projects/spring-boot/issues/2187""","""""""As a user, I'd like to have a gradle build option so that I can support module projects that will declare the Spring XD dependencies as provided, configure the boot plugin for 'MODULE' layout and other boilerplate build configuration.  This is dependent on Boot's module layout scoping issue: https://github.com/spring-projects/spring-boot/issues/2187""""""",,3.0
1,XD-2521,FEATURE,Done,MEDIUM,"""Add options for supporting compression on the message bus with RabbitMQ""","""https://jira.spring.io/browse/AMQP-453 Added support for compression with RabbitMQ.  XD should expose configuration options to enable and configure compression on the message bus.  Note, some options may be specific for brokers or require additional functionality in XD.    This issue should not address adding additional functionality to make the feature set as common as possible across msg bus implementations, but expose what makes sense with the current code base for rabbitmq   As an example, Kafka supports compressed.topics which lets you pick a subset of topics to be compressed.  ""","""""""https://jira.spring.io/browse/AMQP-453 Added support for compression with RabbitMQ.  XD should expose configuration options to enable and configure compression on the message bus.  Note, some options may be specific for brokers or require additional functionality in XD.    This issue should not address adding additional functionality to make the feature set as common as possible across msg bus implementations, but expose what makes sense with the current code base for rabbitmq   As an example, Kafka supports compressed.topics which lets you pick a subset of topics to be compressed.  """"""",,2.0
1,XD-2518,FEATURE,Done,MEDIUM,"""Upgrade to Gradle 2.2""","""Upgrade to Gradle 2.2""","""Upgrade to Gradle 2.2""",,2.0
1,XD-2517,FEATURE,Done,MEDIUM,"""Clean up spring-xd-batch sub-project""","""We should move the org.springframework.xd.batch.jdbc.ColumnRangePartitioner and org.springframework.xd.batch.item.jdbc.FieldSetSqlParameterSourceProvider to the spring-xd-extension-batch project""","""""""We should move the org.springframework.xd.batch.jdbc.ColumnRangePartitioner and org.springframework.xd.batch.item.jdbc.FieldSetSqlParameterSourceProvider to the spring-xd-extension-batch project""""""",,3.0
1,XD-2516,BUG,Done,URGENT,"""spring_rabbitmq_addresses environment variable is ignored""","""When trying to configure XD to use a RabbitMQ instance other than the default localhost:5672, a user is supposedto updated the """"spring_rabbitmq_addresses"""" environment variable or the spring.rabbitmq.addresses setting in the servers.yml file.  In this case XD is ignoring this environment variable.    h3. Steps to reproduce   # set the transport by using """"export XD_TRANSPORT=rabbit"""" # set the spring_rabbitmq_addresses by """"export spring_rabbitmq_addresses=foo:5672"""" # Startup a admin container on your local machine # deploy ticktock #* this should fail #* start up a local rabbitmq #* deploy a new ticktock and stream will deploy.  ""","""""""When trying to configure XD to use a RabbitMQ instance other than the default localhost:5672, a user is supposedto updated the """"""""spring_rabbitmq_addresses"""""""" environment variable or the spring.rabbitmq.addresses setting in the servers.yml file.  In this case XD is ignoring this environment variable.    h3. Steps to reproduce   # set the transport by using """"""""export XD_TRANSPORT=rabbit"""""""" # set the spring_rabbitmq_addresses by """"""""export spring_rabbitmq_addresses=foo:5672"""""""" # Startup a admin container on your local machine # deploy ticktock #* this should fail #* start up a local rabbitmq #* deploy a new ticktock and stream will deploy.  """"""",,3.0
1,XD-2513,FEATURE,Done,MEDIUM,"""Add support for message compression""","""As a user, I'd like to have the option to _compress_ messages so that I can influence the performance throughput. It'd be beneficial to have support for gzip, zip compression, and decompression.""","""""""As a user, I'd like to have the option to _compress_ messages so that I can influence the performance throughput. It'd be beneficial to have support for gzip, zip compression, and decompression.""""""",,3.0
1,XD-2510,BUG,Done,MEDIUM,"""Fix classpath issues for RabbitMQ source/sink""","""RabbitMQ Sink is throwing: {quote} 09:44:16,031 1.1.0.SNAP ERROR DeploymentsPathChildrenCache-0 boot.SpringApplication - Application startup failed org.springframework.beans.factory.xml.XmlBeanDefinitionStoreException: Line 19 in XML document from class path resource [config/rabbit.xml] is invalid; nested exception is org.xml.sax.SAXParseException; lineNumber: 19; columnNumber: 53; cvc-complex-type.3.2.2: Attribute 'default-delivery-mode' is not allowed to appear in element 'int-amqp:outbound-channel-adapter'.     at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.doLoadBeanDefinitions(XmlBeanDefinitionReader.java:399)     at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.loadBeanDefinitions(XmlBeanDefinitionReader.java:336)     at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.loadBeanDefinitions(XmlBeanDefinitionReader.java:304)     at org.springframework.boot.BeanDefinitionLoader.load(BeanDefinitionLoader.java:180)     at org.springframework.boot.BeanDefinitionLoader.load(BeanDefinitionLoader.java:138)     at org.springframework.boot.BeanDefinitionLoader.load(BeanDefinitionLoader.java:127)     at org.springframework.boot.SpringApplication.load(SpringApplication.java:620)     at org.springframework.boot.SpringApplication.run(SpringApplication.java:315)     at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:139)     at org.springframework.xd.module.core.SimpleModule.initialize(SimpleModule.java:211)     at org.springframework.xd.dirt.module.ModuleDeployer.doDeploy(ModuleDeployer.java:217)     at org.springframework.xd.dirt.module.ModuleDeployer.deploy(ModuleDeployer.java:200)     at org.springframework.xd.dirt.server.DeploymentListener.deployModule(DeploymentListener.java:363)     at org.springframework.xd.dirt.server.DeploymentListener.deployStreamModule(DeploymentListener.java:332)     at org.springframework.xd.dirt.server.DeploymentListener.onChildAdded(DeploymentListener.java:179)     at org.springframework.xd.dirt.server.DeploymentListener.childEvent(DeploymentListener.java:147)     at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:509)     at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:503)     at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)     at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297)     at org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:83)     at org.apache.curator.framework.recipes.cache.PathChildrenCache.callListeners(PathChildrenCache.java:500)     at org.apache.curator.framework.recipes.cache.EventOperation.invoke(EventOperation.java:35)     at org.apache.curator.framework.recipes.cache.PathChildrenCache$10.run(PathChildrenCache.java:762)     at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)     at java.util.concurrent.FutureTask.run(FutureTask.java:262)     at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)     at java.util.concurrent.FutureTask.run(FutureTask.java:262)     at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)     at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)     at java.lang.Thread.run(Thread.java:745) Caused by: org.xml.sax.SAXParseException; lineNumber: 19; columnNumber: 53; cvc-complex-type.3.2.2: Attribute 'default-delivery-mode' is not allowed to appear in element 'int-amqp:outbound-channel-adapter'.     at com.sun.org.apache.xerces.internal.util.ErrorHandlerWrapper.createSAXParseException(ErrorHandlerWrapper.java:198)     at com.sun.org.apache.xerces.internal.util.ErrorHandlerWrapper.error(ErrorHandlerWrapper.java:134)     at com.sun.org.apache.xerces.internal.impl.XMLErrorReporter.reportError(XMLErrorReporter.java:437)     at com.sun.org.apache.xerces.internal.impl.XMLErrorReporter.reportError(XMLErrorReporter.java:368)     at com.sun.org.apache.xerces.internal.impl.XMLErrorReporter.reportError(XMLErrorReporter.java:325)     at com.sun.org.apache.xerces.internal.impl.xs.XMLSchemaValidator$XSIErrorReporter.reportError(XMLSchemaValidator.java:458)     at com.sun.org.apache.xerces.internal.impl.xs.XMLSchemaValidator.reportSchemaError(XMLSchemaValidator.java:3237)     at com.sun.org.apache.xerces.internal.impl.xs.XMLSchemaValidator.processAttributes(XMLSchemaValidator.java:2714)     at com.sun.org.apache.xerces.internal.impl.xs.XMLSchemaValidator.handleStartElement(XMLSchemaValidator.java:2056)     at com.sun.org.apache.xerces.internal.impl.xs.XMLSchemaValidator.emptyElement(XMLSchemaValidator.java:766)     at com.sun.org.apache.xerces.internal.impl.XMLNSDocumentScannerImpl.scanStartElement(XMLNSDocumentScannerImpl.java:356)     at com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl$FragmentContentDriver.next(XMLDocumentFragmentScannerImpl.java:2786)     at com.sun.org.apache.xerces.internal.impl.XMLDocumentScannerImpl.next(XMLDocumentScannerImpl.java:606)     at com.sun.org.apache.xerces.internal.impl.XMLNSDocumentScannerImpl.next(XMLNSDocumentScannerImpl.java:117)     at com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl.scanDocument(XMLDocumentFragmentScannerImpl.java:510)     at com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:848)     at com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:777)     at com.sun.org.apache.xerces.internal.parsers.XMLParser.parse(XMLParser.java:141)     at com.sun.org.apache.xerces.internal.parsers.DOMParser.parse(DOMParser.java:243)     at com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderImpl.parse(DocumentBuilderImpl.java:347)     at org.springframework.beans.factory.xml.DefaultDocumentLoader.loadDocument(DefaultDocumentLoader.java:76)     at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.doLoadDocument(XmlBeanDefinitionReader.java:429)     at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.doLoadBeanDefinitions(XmlBeanDefinitionReader.java:391)     ... 30 more 09:44:16,036 1.1.0.SNAP ERROR DeploymentsPathChildrenCache-0 server.DeploymentListener - Exception deploying module org.springframework.beans.factory.xml.XmlBeanDefinitionStoreException: Line 19 in XML document from class path resource [config/rabbit.xml] is invalid; nested exception is org.xml.sax.SAXParseException; lineNumber: 19; columnNumber: 53; cvc-complex-type.3.2.2: Attribute 'default-delivery-mode' is not allowed to appear in element 'int-amqp:outbound-channel-adapter'.     at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.doLoadBeanDefinitions(XmlBeanDefinitionReader.java:399)     at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.loadBeanDefinitions(XmlBeanDefinitionReader.java:336)     at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.loadBeanDefinitions(XmlBeanDefinitionReader.java:304)     at org.springframework.boot.BeanDefinitionLoader.load(BeanDefinitionLoader.java:180)     at org.springframework.boot.BeanDefinitionLoader.load(BeanDefinitionLoader.java:138)     at org.springframework.boot.BeanDefinitionLoader.load(BeanDefinitionLoader.java:127)     at org.springframework.boot.SpringApplication.load(SpringApplication.java:620)     at org.springframework.boot.SpringApplication.run(SpringApplication.java:315)     at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:139)     at org.springframework.xd.module.core.SimpleModule.initialize(SimpleModule.java:211)     at org.springframework.xd.dirt.module.ModuleDeployer.doDeploy(ModuleDeployer.java:217)     at org.springframework.xd.dirt.module.ModuleDeployer.deploy(ModuleDeployer.java:200)     at org.springframework.xd.dirt.server.DeploymentListener.deployModule(DeploymentListener.java:363)     at org.springframework.xd.dirt.server.DeploymentListener.deployStreamModule(DeploymentListener.java:332)     at org.springframework.xd.dirt.server.DeploymentListener.onChildAdded(DeploymentListener.java:179)     at org.springframework.xd.dirt.server.DeploymentListener.childEvent(DeploymentListener.java:147)     at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:509)     at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:503)     at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)     at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297)     at org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:83)     at org.apache.curator.framework.recipes.cache.PathChildrenCache.callListeners(PathChildrenCache.java:500)     at org.apache.curator.framework.recipes.cache.EventOperation.invoke(EventOperation.java:35)     at org.apache.curator.framework.recipes.cache.PathChildrenCache$10.run(PathChildrenCache.java:762)     at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)     at java.util.concurrent.FutureTask.run(FutureTask.java:262)     at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)     at java.util.concurrent.FutureTask.run(FutureTask.java:262)     at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)     at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)     at java.lang.Thread.run(Thread.java:745) Caused by: org.xml.sax.SAXParseException; lineNumber: 19; columnNumber: 53; cvc-complex-type.3.2.2: Attribute 'default-delivery-mode' is not allowed to appear in element 'int-amqp:outbound-channel-adapter'.     at com.sun.org.apache.xerces.internal.util.ErrorHandlerWrapper.createSAXParseException(ErrorHandlerWrapper.java:198)     at com.sun.org.apache.xerces.internal.util.ErrorHandlerWrapper.error(ErrorHandlerWrapper.java:134)     at com.sun.org.apache.xerces.internal.impl.XMLErrorReporter.reportError(XMLErrorReporter.java:437)     at com.sun.org.apache.xerces.internal.impl.XMLErrorReporter.reportError(XMLErrorReporter.java:368)     at com.sun.org.apache.xerces.internal.impl.XMLErrorReporter.reportError(XMLErrorReporter.java:325)     at com.sun.org.apache.xerces.internal.impl.xs.XMLSchemaValidator$XSIErrorReporter.reportError(XMLSchemaValidator.java:458)     at com.sun.org.apache.xerces.internal.impl.xs.XMLSchemaValidator.reportSchemaError(XMLSchemaValidator.java:3237)     at com.sun.org.apache.xerces.internal.impl.xs.XMLSchemaValidator.processAttributes(XMLSchemaValidator.java:2714)     at com.sun.org.apache.xerces.internal.impl.xs.XMLSchemaValidator.handleStartElement(XMLSchemaValidator.java:2056)     at com.sun.org.apache.xerces.internal.impl.xs.XMLSchemaValidator.emptyElement(XMLSchemaValidator.java:766)     at com.sun.org.apache.xerces.internal.impl.XMLNSDocumentScannerImpl.scanStartElement(XMLNSDocumentScannerImpl.java:356)     at com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl$FragmentContentDriver.next(XMLDocumentFragmentScannerImpl.java:2786)     at com.sun.org.apache.xerces.internal.impl.XMLDocumentScannerImpl.next(XMLDocumentScannerImpl.java:606)     at com.sun.org.apache.xerces.internal.impl.XMLNSDocumentScannerImpl.next(XMLNSDocumentScannerImpl.java:117)     at com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl.scanDocument(XMLDocumentFragmentScannerImpl.java:510)     at com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:848)     at com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:777)     at com.sun.org.apache.xerces.internal.parsers.XMLParser.parse(XMLParser.java:141)     at com.sun.org.apache.xerces.internal.parsers.DOMParser.parse(DOMParser.java:243)     at com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderImpl.parse(DocumentBuilderImpl.java:347)     at org.springframework.beans.factory.xml.DefaultDocumentLoader.loadDocument(DefaultDocumentLoader.java:76)     at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.doLoadDocument(XmlBeanDefinitionReader.java:429)     at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.doLoadBeanDefinitions(XmlBeanDefinitionReader.java:391) {quote}""","""""""RabbitMQ Sink is throwing: {quote} 09:44:16,031 1.1.0.SNAP ERROR DeploymentsPathChildrenCache-0 boot.SpringApplication - Application startup failed org.springframework.beans.factory.xml.XmlBeanDefinitionStoreException: Line 19 in XML document from class path resource [config/rabbit.xml] is invalid; nested exception is org.xml.sax.SAXParseException; lineNumber: 19; columnNumber: 53; cvc-complex-type.3.2.2: Attribute 'default-delivery-mode' is not allowed to appear in element 'int-amqp:outbound-channel-adapter'.     at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.doLoadBeanDefinitions(XmlBeanDefinitionReader.java:399)     at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.loadBeanDefinitions(XmlBeanDefinitionReader.java:336)     at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.loadBeanDefinitions(XmlBeanDefinitionReader.java:304)     at org.springframework.boot.BeanDefinitionLoader.load(BeanDefinitionLoader.java:180)     at org.springframework.boot.BeanDefinitionLoader.load(BeanDefinitionLoader.java:138)     at org.springframework.boot.BeanDefinitionLoader.load(BeanDefinitionLoader.java:127)     at org.springframework.boot.SpringApplication.load(SpringApplication.java:620)     at org.springframework.boot.SpringApplication.run(SpringApplication.java:315)     at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:139)     at org.springframework.xd.module.core.SimpleModule.initialize(SimpleModule.java:211)     at org.springframework.xd.dirt.module.ModuleDeployer.doDeploy(ModuleDeployer.java:217)     at org.springframework.xd.dirt.module.ModuleDeployer.deploy(ModuleDeployer.java:200)     at org.springframework.xd.dirt.server.DeploymentListener.deployModule(DeploymentListener.java:363)     at org.springframework.xd.dirt.server.DeploymentListener.deployStreamModule(DeploymentListener.java:332)     at org.springframework.xd.dirt.server.DeploymentListener.onChildAdded(DeploymentListener.java:179)     at org.springframework.xd.dirt.server.DeploymentListener.childEvent(DeploymentListener.java:147)     at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:509)     at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:503)     at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)     at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297)     at org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:83)     at org.apache.curator.framework.recipes.cache.PathChildrenCache.callListeners(PathChildrenCache.java:500)     at org.apache.curator.framework.recipes.cache.EventOperation.invoke(EventOperation.java:35)     at org.apache.curator.framework.recipes.cache.PathChildrenCache$10.run(PathChildrenCache.java:762)     at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)     at java.util.concurrent.FutureTask.run(FutureTask.java:262)     at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)     at java.util.concurrent.FutureTask.run(FutureTask.java:262)     at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)     at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)     at java.lang.Thread.run(Thread.java:745) Caused by: org.xml.sax.SAXParseException; lineNumber: 19; columnNumber: 53; cvc-complex-type.3.2.2: Attribute 'default-delivery-mode' is not allowed to appear in element 'int-amqp:outbound-channel-adapter'.     at com.sun.org.apache.xerces.internal.util.ErrorHandlerWrapper.createSAXParseException(ErrorHandlerWrapper.java:198)     at com.sun.org.apache.xerces.internal.util.ErrorHandlerWrapper.error(ErrorHandlerWrapper.java:134)     at com.sun.org.apache.xerces.internal.impl.XMLErrorReporter.reportError(XMLErrorReporter.java:437)     at com.sun.org.apache.xerces.internal.impl.XMLErrorReporter.reportError(XMLErrorReporter.java:368)     at com.sun.org.apache.xerces.internal.impl.XMLErrorReporter.reportError(XMLErrorReporter.java:325)     at com.sun.org.apache.xerces.internal.impl.xs.XMLSchemaValidator$XSIErrorReporter.reportError(XMLSchemaValidator.java:458)     at com.sun.org.apache.xerces.internal.impl.xs.XMLSchemaValidator.reportSchemaError(XMLSchemaValidator.java:3237)     at com.sun.org.apache.xerces.internal.impl.xs.XMLSchemaValidator.processAttributes(XMLSchemaValidator.java:2714)     at com.sun.org.apache.xerces.internal.impl.xs.XMLSchemaValidator.handleStartElement(XMLSchemaValidator.java:2056)     at com.sun.org.apache.xerces.internal.impl.xs.XMLSchemaValidator.emptyElement(XMLSchemaValidator.java:766)     at com.sun.org.apache.xerces.internal.impl.XMLNSDocumentScannerImpl.scanStartElement(XMLNSDocumentScannerImpl.java:356)     at com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl$FragmentContentDriver.next(XMLDocumentFragmentScannerImpl.java:2786)     at com.sun.org.apache.xerces.internal.impl.XMLDocumentScannerImpl.next(XMLDocumentScannerImpl.java:606)     at com.sun.org.apache.xerces.internal.impl.XMLNSDocumentScannerImpl.next(XMLNSDocumentScannerImpl.java:117)     at com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl.scanDocument(XMLDocumentFragmentScannerImpl.java:510)     at com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:848)     at com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:777)     at com.sun.org.apache.xerces.internal.parsers.XMLParser.parse(XMLParser.java:141)     at com.sun.org.apache.xerces.internal.parsers.DOMParser.parse(DOMParser.java:243)     at com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderImpl.parse(DocumentBuilderImpl.java:347)     at org.springframework.beans.factory.xml.DefaultDocumentLoader.loadDocument(DefaultDocumentLoader.java:76)     at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.doLoadDocument(XmlBeanDefinitionReader.java:429)     at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.doLoadBeanDefinitions(XmlBeanDefinitionReader.java:391)     ... 30 more 09:44:16,036 1.1.0.SNAP ERROR DeploymentsPathChildrenCache-0 server.DeploymentListener - Exception deploying module org.springframework.beans.factory.xml.XmlBeanDefinitionStoreException: Line 19 in XML document from class path resource [config/rabbit.xml] is invalid; nested exception is org.xml.sax.SAXParseException; lineNumber: 19; columnNumber: 53; cvc-complex-type.3.2.2: Attribute 'default-delivery-mode' is not allowed to appear in element 'int-amqp:outbound-channel-adapter'.     at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.doLoadBeanDefinitions(XmlBeanDefinitionReader.java:399)     at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.loadBeanDefinitions(XmlBeanDefinitionReader.java:336)     at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.loadBeanDefinitions(XmlBeanDefinitionReader.java:304)     at org.springframework.boot.BeanDefinitionLoader.load(BeanDefinitionLoader.java:180)     at org.springframework.boot.BeanDefinitionLoader.load(BeanDefinitionLoader.java:138)     at org.springframework.boot.BeanDefinitionLoader.load(BeanDefinitionLoader.java:127)     at org.springframework.boot.SpringApplication.load(SpringApplication.java:620)     at org.springframework.boot.SpringApplication.run(SpringApplication.java:315)     at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:139)     at org.springframework.xd.module.core.SimpleModule.initialize(SimpleModule.java:211)     at org.springframework.xd.dirt.module.ModuleDeployer.doDeploy(ModuleDeployer.java:217)     at org.springframework.xd.dirt.module.ModuleDeployer.deploy(ModuleDeployer.java:200)     at org.springframework.xd.dirt.server.DeploymentListener.deployModule(DeploymentListener.java:363)     at org.springframework.xd.dirt.server.DeploymentListener.deployStreamModule(DeploymentListener.java:332)     at org.springframework.xd.dirt.server.DeploymentListener.onChildAdded(DeploymentListener.java:179)     at org.springframework.xd.dirt.server.DeploymentListener.childEvent(DeploymentListener.java:147)     at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:509)     at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:503)     at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)     at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297)     at org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:83)     at org.apache.curator.framework.recipes.cache.PathChildrenCache.callListeners(PathChildrenCache.java:500)     at org.apache.curator.framework.recipes.cache.EventOperation.invoke(EventOperation.java:35)     at org.apache.curator.framework.recipes.cache.PathChildrenCache$10.run(PathChildrenCache.java:762)     at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)     at java.util.concurrent.FutureTask.run(FutureTask.java:262)     at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)     at java.util.concurrent.FutureTask.run(FutureTask.java:262)     at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)     at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)     at java.lang.Thread.run(Thread.java:745) Caused by: org.xml.sax.SAXParseException; lineNumber: 19; columnNumber: 53; cvc-complex-type.3.2.2: Attribute 'default-delivery-mode' is not allowed to appear in element 'int-amqp:outbound-channel-adapter'.     at com.sun.org.apache.xerces.internal.util.ErrorHandlerWrapper.createSAXParseException(ErrorHandlerWrapper.java:198)     at com.sun.org.apache.xerces.internal.util.ErrorHandlerWrapper.error(ErrorHandlerWrapper.java:134)     at com.sun.org.apache.xerces.internal.impl.XMLErrorReporter.reportError(XMLErrorReporter.java:437)     at com.sun.org.apache.xerces.internal.impl.XMLErrorReporter.reportError(XMLErrorReporter.java:368)     at com.sun.org.apache.xerces.internal.impl.XMLErrorReporter.reportError(XMLErrorReporter.java:325)     at com.sun.org.apache.xerces.internal.impl.xs.XMLSchemaValidator$XSIErrorReporter.reportError(XMLSchemaValidator.java:458)     at com.sun.org.apache.xerces.internal.impl.xs.XMLSchemaValidator.reportSchemaError(XMLSchemaValidator.java:3237)     at com.sun.org.apache.xerces.internal.impl.xs.XMLSchemaValidator.processAttributes(XMLSchemaValidator.java:2714)     at com.sun.org.apache.xerces.internal.impl.xs.XMLSchemaValidator.handleStartElement(XMLSchemaValidator.java:2056)     at com.sun.org.apache.xerces.internal.impl.xs.XMLSchemaValidator.emptyElement(XMLSchemaValidator.java:766)     at com.sun.org.apache.xerces.internal.impl.XMLNSDocumentScannerImpl.scanStartElement(XMLNSDocumentScannerImpl.java:356)     at com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl$FragmentContentDriver.next(XMLDocumentFragmentScannerImpl.java:2786)     at com.sun.org.apache.xerces.internal.impl.XMLDocumentScannerImpl.next(XMLDocumentScannerImpl.java:606)     at com.sun.org.apache.xerces.internal.impl.XMLNSDocumentScannerImpl.next(XMLNSDocumentScannerImpl.java:117)     at com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl.scanDocument(XMLDocumentFragmentScannerImpl.java:510)     at com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:848)     at com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:777)     at com.sun.org.apache.xerces.internal.parsers.XMLParser.parse(XMLParser.java:141)     at com.sun.org.apache.xerces.internal.parsers.DOMParser.parse(DOMParser.java:243)     at com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderImpl.parse(DocumentBuilderImpl.java:347)     at org.springframework.beans.factory.xml.DefaultDocumentLoader.loadDocument(DefaultDocumentLoader.java:76)     at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.doLoadDocument(XmlBeanDefinitionReader.java:429)     at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.doLoadBeanDefinitions(XmlBeanDefinitionReader.java:391) {quote}""""""",,1.0
1,XD-2509,BUG,Done,MEDIUM,"""Solve CP issues for the Rabbit MessageBus""","""Rabbit Message Bus is throwing:  {quote} 10:14:04,678 1.1.0.SNAP  INFO DeploymentSupervisor-0 server.StreamDeploymentListener - Deployment status for stream 'foo': DeploymentStatus{state=failed,error(s)=org.springframework.amqp.UncategorizedAmqpException: java.lang.IllegalArgumentException: interface org.springframework.amqp.rabbit.listener.SimpleMessageListenerContainer$ContainerDelegate is not visible from class loader     at org.springframework.amqp.rabbit.support.RabbitExceptionTranslator.convertRabbitAccessException(RabbitExceptionTranslator.java:66)     at org.springframework.amqp.rabbit.connection.RabbitAccessor.convertRabbitAccessException(RabbitAccessor.java:110)     at org.springframework.amqp.rabbit.listener.AbstractMessageListenerContainer.initialize(AbstractMessageListenerContainer.java:426)     at org.springframework.amqp.rabbit.listener.AbstractMessageListenerContainer.afterPropertiesSet(AbstractMessageListenerContainer.java:385)     at org.springframework.xd.dirt.integration.rabbit.RabbitMessageBus.doRegisterConsumer(RabbitMessageBus.java:367)     at org.springframework.xd.dirt.integration.rabbit.RabbitMessageBus.bindConsumer(RabbitMessageBus.java:308)     at org.springframework.xd.dirt.plugins.AbstractMessageBusBinderPlugin.bindMessageConsumer(AbstractMessageBusBinderPlugin.java:183)     at org.springframework.xd.dirt.plugins.AbstractMessageBusBinderPlugin.bindConsumerAndProducers(AbstractMessageBusBinderPlugin.java:138)     at org.springframework.xd.dirt.plugins.stream.StreamPlugin.postProcessModule(StreamPlugin.java:73)     at org.springframework.xd.dirt.module.ModuleDeployer.postProcessModule(ModuleDeployer.java:238)     at org.springframework.xd.dirt.module.ModuleDeployer.doDeploy(ModuleDeployer.java:218)     at org.springframework.xd.dirt.module.ModuleDeployer.deploy(ModuleDeployer.java:200)     at org.springframework.xd.dirt.server.DeploymentListener.deployModule(DeploymentListener.java:363)     at org.springframework.xd.dirt.server.DeploymentListener.deployStreamModule(DeploymentListener.java:332)     at org.springframework.xd.dirt.server.DeploymentListener.onChildAdded(DeploymentListener.java:179)     at org.springframework.xd.dirt.server.DeploymentListener.childEvent(DeploymentListener.java:147)     at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:509)     at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:503)     at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)     at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297)     at org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:83)     at org.apache.curator.framework.recipes.cache.PathChildrenCache.callListeners(PathChildrenCache.java:500)     at org.apache.curator.framework.recipes.cache.EventOperation.invoke(EventOperation.java:35)     at org.apache.curator.framework.recipes.cache.PathChildrenCache$10.run(PathChildrenCache.java:762)     at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)     at java.util.concurrent.FutureTask.run(FutureTask.java:262)     at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)     at java.util.concurrent.FutureTask.run(FutureTask.java:262)     at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)     at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)     at java.lang.Thread.run(Thread.java:745) Caused by: java.lang.IllegalArgumentException: interface org.springframework.amqp.rabbit.listener.SimpleMessageListenerContainer$ContainerDelegate is not visible from class loader     at java.lang.reflect.Proxy$ProxyClassFactory.apply(Proxy.java:616)     at java.lang.reflect.Proxy$ProxyClassFactory.apply(Proxy.java:592)     at java.lang.reflect.WeakCache$Factory.get(WeakCache.java:244)     at java.lang.reflect.WeakCache.get(WeakCache.java:141)     at java.lang.reflect.Proxy.getProxyClass0(Proxy.java:455)     at java.lang.reflect.Proxy.newProxyInstance(Proxy.java:738)     at org.springframework.aop.framework.JdkDynamicAopProxy.getProxy(JdkDynamicAopProxy.java:121)     at org.springframework.aop.framework.JdkDynamicAopProxy.getProxy(JdkDynamicAopProxy.java:111)     at org.springframework.aop.framework.ProxyFactory.getProxy(ProxyFactory.java:96)     at org.springframework.amqp.rabbit.listener.SimpleMessageListenerContainer.initializeProxy(SimpleMessageListenerContainer.java:586)     at org.springframework.amqp.rabbit.listener.SimpleMessageListenerContainer.doInitialize(SimpleMessageListenerContainer.java:612)     at org.springframework.amqp.rabbit.listener.AbstractMessageListenerContainer.initialize(AbstractMessageListenerContainer.java:424)     ... 28 more {quote}""","""""""Rabbit Message Bus is throwing:  {quote} 10:14:04,678 1.1.0.SNAP  INFO DeploymentSupervisor-0 server.StreamDeploymentListener - Deployment status for stream 'foo': DeploymentStatus{state=failed,error(s)=org.springframework.amqp.UncategorizedAmqpException: java.lang.IllegalArgumentException: interface org.springframework.amqp.rabbit.listener.SimpleMessageListenerContainer$ContainerDelegate is not visible from class loader     at org.springframework.amqp.rabbit.support.RabbitExceptionTranslator.convertRabbitAccessException(RabbitExceptionTranslator.java:66)     at org.springframework.amqp.rabbit.connection.RabbitAccessor.convertRabbitAccessException(RabbitAccessor.java:110)     at org.springframework.amqp.rabbit.listener.AbstractMessageListenerContainer.initialize(AbstractMessageListenerContainer.java:426)     at org.springframework.amqp.rabbit.listener.AbstractMessageListenerContainer.afterPropertiesSet(AbstractMessageListenerContainer.java:385)     at org.springframework.xd.dirt.integration.rabbit.RabbitMessageBus.doRegisterConsumer(RabbitMessageBus.java:367)     at org.springframework.xd.dirt.integration.rabbit.RabbitMessageBus.bindConsumer(RabbitMessageBus.java:308)     at org.springframework.xd.dirt.plugins.AbstractMessageBusBinderPlugin.bindMessageConsumer(AbstractMessageBusBinderPlugin.java:183)     at org.springframework.xd.dirt.plugins.AbstractMessageBusBinderPlugin.bindConsumerAndProducers(AbstractMessageBusBinderPlugin.java:138)     at org.springframework.xd.dirt.plugins.stream.StreamPlugin.postProcessModule(StreamPlugin.java:73)     at org.springframework.xd.dirt.module.ModuleDeployer.postProcessModule(ModuleDeployer.java:238)     at org.springframework.xd.dirt.module.ModuleDeployer.doDeploy(ModuleDeployer.java:218)     at org.springframework.xd.dirt.module.ModuleDeployer.deploy(ModuleDeployer.java:200)     at org.springframework.xd.dirt.server.DeploymentListener.deployModule(DeploymentListener.java:363)     at org.springframework.xd.dirt.server.DeploymentListener.deployStreamModule(DeploymentListener.java:332)     at org.springframework.xd.dirt.server.DeploymentListener.onChildAdded(DeploymentListener.java:179)     at org.springframework.xd.dirt.server.DeploymentListener.childEvent(DeploymentListener.java:147)     at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:509)     at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:503)     at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)     at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297)     at org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:83)     at org.apache.curator.framework.recipes.cache.PathChildrenCache.callListeners(PathChildrenCache.java:500)     at org.apache.curator.framework.recipes.cache.EventOperation.invoke(EventOperation.java:35)     at org.apache.curator.framework.recipes.cache.PathChildrenCache$10.run(PathChildrenCache.java:762)     at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)     at java.util.concurrent.FutureTask.run(FutureTask.java:262)     at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)     at java.util.concurrent.FutureTask.run(FutureTask.java:262)     at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)     at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)     at java.lang.Thread.run(Thread.java:745) Caused by: java.lang.IllegalArgumentException: interface org.springframework.amqp.rabbit.listener.SimpleMessageListenerContainer$ContainerDelegate is not visible from class loader     at java.lang.reflect.Proxy$ProxyClassFactory.apply(Proxy.java:616)     at java.lang.reflect.Proxy$ProxyClassFactory.apply(Proxy.java:592)     at java.lang.reflect.WeakCache$Factory.get(WeakCache.java:244)     at java.lang.reflect.WeakCache.get(WeakCache.java:141)     at java.lang.reflect.Proxy.getProxyClass0(Proxy.java:455)     at java.lang.reflect.Proxy.newProxyInstance(Proxy.java:738)     at org.springframework.aop.framework.JdkDynamicAopProxy.getProxy(JdkDynamicAopProxy.java:121)     at org.springframework.aop.framework.JdkDynamicAopProxy.getProxy(JdkDynamicAopProxy.java:111)     at org.springframework.aop.framework.ProxyFactory.getProxy(ProxyFactory.java:96)     at org.springframework.amqp.rabbit.listener.SimpleMessageListenerContainer.initializeProxy(SimpleMessageListenerContainer.java:586)     at org.springframework.amqp.rabbit.listener.SimpleMessageListenerContainer.doInitialize(SimpleMessageListenerContainer.java:612)     at org.springframework.amqp.rabbit.listener.AbstractMessageListenerContainer.initialize(AbstractMessageListenerContainer.java:424)     ... 28 more {quote}""""""",,2.0
1,XD-2508,FEATURE,Done,MEDIUM,"""MQTT: Support the New Spring Integration 4.1 Features""","""HA Configuration, async sends.  http://docs.spring.io/spring-integration/reference/html/whats-new.html#4.1-mqtt""","""""""HA Configuration, async sends.  http://docs.spring.io/spring-integration/reference/html/whats-new.html#4.1-mqtt""""""",,3.0
1,XD-2506,FEATURE,Done,MEDIUM,"""""""script"""" processor options incorrect on docs""","""The EXAMPLE in the documentation (and the paragraph preceding the example) for the """"script"""" processor uses both """"location"""" and """"properties-location"""" options, but these are in actuality """"script"""" and """"locationProperties"""" according to """"module info processor:script"""" and the text of the documentation.  See: http://docs.spring.io/spring-xd/docs/1.0.2.RELEASE/reference/html/#script   {quote}To use the module, pass the location of a Groovy script using the location attribute. If you want to pass variable values to your script, you can optionally pass the path to a properties file using the properties-location attribute. All properties in the file will be made available to the script as variables.  {code}xd:> stream create --name groovyprocessortest --definition """"http --port=9006 | script --location=custom-processor.groovy --properties-location=custom-processor.properties | log"""" --deploy{code} {quote}""","""""""The EXAMPLE in the documentation (and the paragraph preceding the example) for the """"""""script"""""""" processor uses both """"""""location"""""""" and """"""""properties-location"""""""" options, but these are in actuality """"""""script"""""""" and """"""""locationProperties"""""""" according to """"""""module info processor:script"""""""" and the text of the documentation.  See: http://docs.spring.io/spring-xd/docs/1.0.2.RELEASE/reference/html/#script   {quote}To use the module, pass the location of a Groovy script using the location attribute. If you want to pass variable values to your script, you can optionally pass the path to a properties file using the properties-location attribute. All properties in the file will be made available to the script as variables.   {quote}""""""","""xd:> stream create --name groovyprocessortest --definition """"""""http --port=9006 | script --location=custom-processor.groovy --properties-location=custom-processor.properties | log"""""""" --deploy""",1.0
1,XD-2505,BUG,Done,HIGH,"""Undeploying HDFS module closes filesystem""","""When using the hadoop namespace to create a hadoop configuration and filesystem, the FileSystemFactoryBean uses Hadoop FileSystem.get and not newInstance which will return a FileSystem from the cache.  When undeploying the module, the FileSystemFactoryBean destroy method will close the FileSystem which closes for all other deployed Hadoop modules throwing a java.io.IOException: Filesystem closed""","""""""When using the hadoop namespace to create a hadoop configuration and filesystem, the FileSystemFactoryBean uses Hadoop FileSystem.get and not newInstance which will return a FileSystem from the cache.  When undeploying the module, the FileSystemFactoryBean destroy method will close the FileSystem which closes for all other deployed Hadoop modules throwing a java.io.IOException: Filesystem closed""""""",,3.0
1,XD-2504,FEATURE,Done,MEDIUM,"""Upgrade CI Acceptance AMI to HVM""","""Replace the current paravirtual AMI used for CI tests needed to be replaced with a HVM based AMI Paravirtual is being phased out by Amazon.  Also so we can utilize VPC and placement groups in the future. ""","""""""Replace the current paravirtual AMI used for CI tests needed to be replaced with a HVM based AMI Paravirtual is being phased out by Amazon.  Also so we can utilize VPC and placement groups in the future. """"""",,5.0
1,XD-2503,BUG,Done,URGENT,"""RabbitMQ Message bus, RabbitMQ Source/Sinks are throwing exceptions""","""I believe it is being cause by the following PR: XD-2381: Split MessageBus and Analytics dependencies from DIRT PR:  1307 SHA: 8d28b2786acbdea1617d7e903b805e5af5369b90  *RabbitMQ Sink is throwing:* {noformat} 09:44:16,031 1.1.0.SNAP ERROR DeploymentsPathChildrenCache-0 boot.SpringApplication - Application startup failed org.springframework.beans.factory.xml.XmlBeanDefinitionStoreException: Line 19 in XML document from class path resource [config/rabbit.xml] is invalid; nested exception is org.xml.sax.SAXParseException; lineNumber: 19; columnNumber: 53; cvc-complex-type.3.2.2: Attribute 'default-delivery-mode' is not allowed to appear in element 'int-amqp:outbound-channel-adapter'.  at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.doLoadBeanDefinitions(XmlBeanDefinitionReader.java:399)  at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.loadBeanDefinitions(XmlBeanDefinitionReader.java:336)  at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.loadBeanDefinitions(XmlBeanDefinitionReader.java:304)  at org.springframework.boot.BeanDefinitionLoader.load(BeanDefinitionLoader.java:180)  at org.springframework.boot.BeanDefinitionLoader.load(BeanDefinitionLoader.java:138)  at org.springframework.boot.BeanDefinitionLoader.load(BeanDefinitionLoader.java:127)  at org.springframework.boot.SpringApplication.load(SpringApplication.java:620)  at org.springframework.boot.SpringApplication.run(SpringApplication.java:315)  at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:139)  at org.springframework.xd.module.core.SimpleModule.initialize(SimpleModule.java:211)  at org.springframework.xd.dirt.module.ModuleDeployer.doDeploy(ModuleDeployer.java:217)  at org.springframework.xd.dirt.module.ModuleDeployer.deploy(ModuleDeployer.java:200)  at org.springframework.xd.dirt.server.DeploymentListener.deployModule(DeploymentListener.java:363)  at org.springframework.xd.dirt.server.DeploymentListener.deployStreamModule(DeploymentListener.java:332)  at org.springframework.xd.dirt.server.DeploymentListener.onChildAdded(DeploymentListener.java:179)  at org.springframework.xd.dirt.server.DeploymentListener.childEvent(DeploymentListener.java:147)  at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:509)  at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:503)  at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)  at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297)  at org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:83)  at org.apache.curator.framework.recipes.cache.PathChildrenCache.callListeners(PathChildrenCache.java:500)  at org.apache.curator.framework.recipes.cache.EventOperation.invoke(EventOperation.java:35)  at org.apache.curator.framework.recipes.cache.PathChildrenCache$10.run(PathChildrenCache.java:762)  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)  at java.util.concurrent.FutureTask.run(FutureTask.java:262)  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)  at java.util.concurrent.FutureTask.run(FutureTask.java:262)  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)  at java.lang.Thread.run(Thread.java:745) Caused by: org.xml.sax.SAXParseException; lineNumber: 19; columnNumber: 53; cvc-complex-type.3.2.2: Attribute 'default-delivery-mode' is not allowed to appear in element 'int-amqp:outbound-channel-adapter'.  at com.sun.org.apache.xerces.internal.util.ErrorHandlerWrapper.createSAXParseException(ErrorHandlerWrapper.java:198)  at com.sun.org.apache.xerces.internal.util.ErrorHandlerWrapper.error(ErrorHandlerWrapper.java:134)  at com.sun.org.apache.xerces.internal.impl.XMLErrorReporter.reportError(XMLErrorReporter.java:437)  at com.sun.org.apache.xerces.internal.impl.XMLErrorReporter.reportError(XMLErrorReporter.java:368)  at com.sun.org.apache.xerces.internal.impl.XMLErrorReporter.reportError(XMLErrorReporter.java:325)  at com.sun.org.apache.xerces.internal.impl.xs.XMLSchemaValidator$XSIErrorReporter.reportError(XMLSchemaValidator.java:458)  at com.sun.org.apache.xerces.internal.impl.xs.XMLSchemaValidator.reportSchemaError(XMLSchemaValidator.java:3237)  at com.sun.org.apache.xerces.internal.impl.xs.XMLSchemaValidator.processAttributes(XMLSchemaValidator.java:2714)  at com.sun.org.apache.xerces.internal.impl.xs.XMLSchemaValidator.handleStartElement(XMLSchemaValidator.java:2056)  at com.sun.org.apache.xerces.internal.impl.xs.XMLSchemaValidator.emptyElement(XMLSchemaValidator.java:766)  at com.sun.org.apache.xerces.internal.impl.XMLNSDocumentScannerImpl.scanStartElement(XMLNSDocumentScannerImpl.java:356)  at com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl$FragmentContentDriver.next(XMLDocumentFragmentScannerImpl.java:2786)  at com.sun.org.apache.xerces.internal.impl.XMLDocumentScannerImpl.next(XMLDocumentScannerImpl.java:606)  at com.sun.org.apache.xerces.internal.impl.XMLNSDocumentScannerImpl.next(XMLNSDocumentScannerImpl.java:117)  at com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl.scanDocument(XMLDocumentFragmentScannerImpl.java:510)  at com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:848)  at com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:777)  at com.sun.org.apache.xerces.internal.parsers.XMLParser.parse(XMLParser.java:141)  at com.sun.org.apache.xerces.internal.parsers.DOMParser.parse(DOMParser.java:243)  at com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderImpl.parse(DocumentBuilderImpl.java:347)  at org.springframework.beans.factory.xml.DefaultDocumentLoader.loadDocument(DefaultDocumentLoader.java:76)  at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.doLoadDocument(XmlBeanDefinitionReader.java:429)  at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.doLoadBeanDefinitions(XmlBeanDefinitionReader.java:391)  ... 30 more 09:44:16,036 1.1.0.SNAP ERROR DeploymentsPathChildrenCache-0 server.DeploymentListener - Exception deploying module org.springframework.beans.factory.xml.XmlBeanDefinitionStoreException: Line 19 in XML document from class path resource [config/rabbit.xml] is invalid; nested exception is org.xml.sax.SAXParseException; lineNumber: 19; columnNumber: 53; cvc-complex-type.3.2.2: Attribute 'default-delivery-mode' is not allowed to appear in element 'int-amqp:outbound-channel-adapter'.  at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.doLoadBeanDefinitions(XmlBeanDefinitionReader.java:399)  at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.loadBeanDefinitions(XmlBeanDefinitionReader.java:336)  at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.loadBeanDefinitions(XmlBeanDefinitionReader.java:304)  at org.springframework.boot.BeanDefinitionLoader.load(BeanDefinitionLoader.java:180)  at org.springframework.boot.BeanDefinitionLoader.load(BeanDefinitionLoader.java:138)  at org.springframework.boot.BeanDefinitionLoader.load(BeanDefinitionLoader.java:127)  at org.springframework.boot.SpringApplication.load(SpringApplication.java:620)  at org.springframework.boot.SpringApplication.run(SpringApplication.java:315)  at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:139)  at org.springframework.xd.module.core.SimpleModule.initialize(SimpleModule.java:211)  at org.springframework.xd.dirt.module.ModuleDeployer.doDeploy(ModuleDeployer.java:217)  at org.springframework.xd.dirt.module.ModuleDeployer.deploy(ModuleDeployer.java:200)  at org.springframework.xd.dirt.server.DeploymentListener.deployModule(DeploymentListener.java:363)  at org.springframework.xd.dirt.server.DeploymentListener.deployStreamModule(DeploymentListener.java:332)  at org.springframework.xd.dirt.server.DeploymentListener.onChildAdded(DeploymentListener.java:179)  at org.springframework.xd.dirt.server.DeploymentListener.childEvent(DeploymentListener.java:147)  at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:509)  at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:503)  at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)  at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297)  at org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:83)  at org.apache.curator.framework.recipes.cache.PathChildrenCache.callListeners(PathChildrenCache.java:500)  at org.apache.curator.framework.recipes.cache.EventOperation.invoke(EventOperation.java:35)  at org.apache.curator.framework.recipes.cache.PathChildrenCache$10.run(PathChildrenCache.java:762)  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)  at java.util.concurrent.FutureTask.run(FutureTask.java:262)  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)  at java.util.concurrent.FutureTask.run(FutureTask.java:262)  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)  at java.lang.Thread.run(Thread.java:745) Caused by: org.xml.sax.SAXParseException; lineNumber: 19; columnNumber: 53; cvc-complex-type.3.2.2: Attribute 'default-delivery-mode' is not allowed to appear in element 'int-amqp:outbound-channel-adapter'.  at com.sun.org.apache.xerces.internal.util.ErrorHandlerWrapper.createSAXParseException(ErrorHandlerWrapper.java:198)  at com.sun.org.apache.xerces.internal.util.ErrorHandlerWrapper.error(ErrorHandlerWrapper.java:134)  at com.sun.org.apache.xerces.internal.impl.XMLErrorReporter.reportError(XMLErrorReporter.java:437)  at com.sun.org.apache.xerces.internal.impl.XMLErrorReporter.reportError(XMLErrorReporter.java:368)  at com.sun.org.apache.xerces.internal.impl.XMLErrorReporter.reportError(XMLErrorReporter.java:325)  at com.sun.org.apache.xerces.internal.impl.xs.XMLSchemaValidator$XSIErrorReporter.reportError(XMLSchemaValidator.java:458)  at com.sun.org.apache.xerces.internal.impl.xs.XMLSchemaValidator.reportSchemaError(XMLSchemaValidator.java:3237)  at com.sun.org.apache.xerces.internal.impl.xs.XMLSchemaValidator.processAttributes(XMLSchemaValidator.java:2714)  at com.sun.org.apache.xerces.internal.impl.xs.XMLSchemaValidator.handleStartElement(XMLSchemaValidator.java:2056)  at com.sun.org.apache.xerces.internal.impl.xs.XMLSchemaValidator.emptyElement(XMLSchemaValidator.java:766)  at com.sun.org.apache.xerces.internal.impl.XMLNSDocumentScannerImpl.scanStartElement(XMLNSDocumentScannerImpl.java:356)  at com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl$FragmentContentDriver.next(XMLDocumentFragmentScannerImpl.java:2786)  at com.sun.org.apache.xerces.internal.impl.XMLDocumentScannerImpl.next(XMLDocumentScannerImpl.java:606)  at com.sun.org.apache.xerces.internal.impl.XMLNSDocumentScannerImpl.next(XMLNSDocumentScannerImpl.java:117)  at com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl.scanDocument(XMLDocumentFragmentScannerImpl.java:510)  at com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:848)  at com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:777)  at com.sun.org.apache.xerces.internal.parsers.XMLParser.parse(XMLParser.java:141)  at com.sun.org.apache.xerces.internal.parsers.DOMParser.parse(DOMParser.java:243)  at com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderImpl.parse(DocumentBuilderImpl.java:347)  at org.springframework.beans.factory.xml.DefaultDocumentLoader.loadDocument(DefaultDocumentLoader.java:76)  at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.doLoadDocument(XmlBeanDefinitionReader.java:429)  at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.doLoadBeanDefinitions(XmlBeanDefinitionReader.java:391) {noformat} *Rabbit Message Bus is throwing:* {noformat} 10:14:04,678 1.1.0.SNAP  INFO DeploymentSupervisor-0 server.StreamDeploymentListener - Deployment status for stream 'foo': DeploymentStatus{state=failed,error(s)=org.springframework.amqp.UncategorizedAmqpException: java.lang.IllegalArgumentException: interface org.springframework.amqp.rabbit.listener.SimpleMessageListenerContainer$ContainerDelegate is not visible from class loader  at org.springframework.amqp.rabbit.support.RabbitExceptionTranslator.convertRabbitAccessException(RabbitExceptionTranslator.java:66)  at org.springframework.amqp.rabbit.connection.RabbitAccessor.convertRabbitAccessException(RabbitAccessor.java:110)  at org.springframework.amqp.rabbit.listener.AbstractMessageListenerContainer.initialize(AbstractMessageListenerContainer.java:426)  at org.springframework.amqp.rabbit.listener.AbstractMessageListenerContainer.afterPropertiesSet(AbstractMessageListenerContainer.java:385)  at org.springframework.xd.dirt.integration.rabbit.RabbitMessageBus.doRegisterConsumer(RabbitMessageBus.java:367)  at org.springframework.xd.dirt.integration.rabbit.RabbitMessageBus.bindConsumer(RabbitMessageBus.java:308)  at org.springframework.xd.dirt.plugins.AbstractMessageBusBinderPlugin.bindMessageConsumer(AbstractMessageBusBinderPlugin.java:183)  at org.springframework.xd.dirt.plugins.AbstractMessageBusBinderPlugin.bindConsumerAndProducers(AbstractMessageBusBinderPlugin.java:138)  at org.springframework.xd.dirt.plugins.stream.StreamPlugin.postProcessModule(StreamPlugin.java:73)  at org.springframework.xd.dirt.module.ModuleDeployer.postProcessModule(ModuleDeployer.java:238)  at org.springframework.xd.dirt.module.ModuleDeployer.doDeploy(ModuleDeployer.java:218)  at org.springframework.xd.dirt.module.ModuleDeployer.deploy(ModuleDeployer.java:200)  at org.springframework.xd.dirt.server.DeploymentListener.deployModule(DeploymentListener.java:363)  at org.springframework.xd.dirt.server.DeploymentListener.deployStreamModule(DeploymentListener.java:332)  at org.springframework.xd.dirt.server.DeploymentListener.onChildAdded(DeploymentListener.java:179)  at org.springframework.xd.dirt.server.DeploymentListener.childEvent(DeploymentListener.java:147)  at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:509)  at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:503)  at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)  at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297)  at org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:83)  at org.apache.curator.framework.recipes.cache.PathChildrenCache.callListeners(PathChildrenCache.java:500)  at org.apache.curator.framework.recipes.cache.EventOperation.invoke(EventOperation.java:35)  at org.apache.curator.framework.recipes.cache.PathChildrenCache$10.run(PathChildrenCache.java:762)  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)  at java.util.concurrent.FutureTask.run(FutureTask.java:262)  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)  at java.util.concurrent.FutureTask.run(FutureTask.java:262)  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)  at java.lang.Thread.run(Thread.java:745) Caused by: java.lang.IllegalArgumentException: interface org.springframework.amqp.rabbit.listener.SimpleMessageListenerContainer$ContainerDelegate is not visible from class loader  at java.lang.reflect.Proxy$ProxyClassFactory.apply(Proxy.java:616)  at java.lang.reflect.Proxy$ProxyClassFactory.apply(Proxy.java:592)  at java.lang.reflect.WeakCache$Factory.get(WeakCache.java:244)  at java.lang.reflect.WeakCache.get(WeakCache.java:141)  at java.lang.reflect.Proxy.getProxyClass0(Proxy.java:455)  at java.lang.reflect.Proxy.newProxyInstance(Proxy.java:738)  at org.springframework.aop.framework.JdkDynamicAopProxy.getProxy(JdkDynamicAopProxy.java:121)  at org.springframework.aop.framework.JdkDynamicAopProxy.getProxy(JdkDynamicAopProxy.java:111)  at org.springframework.aop.framework.ProxyFactory.getProxy(ProxyFactory.java:96)  at org.springframework.amqp.rabbit.listener.SimpleMessageListenerContainer.initializeProxy(SimpleMessageListenerContainer.java:586)  at org.springframework.amqp.rabbit.listener.SimpleMessageListenerContainer.doInitialize(SimpleMessageListenerContainer.java:612)  at org.springframework.amqp.rabbit.listener.AbstractMessageListenerContainer.initialize(AbstractMessageListenerContainer.java:424)  ... 28 more {noformat}""","""""""I believe it is being cause by the following PR: XD-2381: Split MessageBus and Analytics dependencies from DIRT PR:  1307 SHA: 8d28b2786acbdea1617d7e903b805e5af5369b90  *RabbitMQ Sink is throwing:*  *Rabbit Message Bus is throwing:* """"""",""" 09:44:16,031 1.1.0.SNAP ERROR DeploymentsPathChildrenCache-0 boot.SpringApplication - Application startup failed org.springframework.beans.factory.xml.XmlBeanDefinitionStoreException: Line 19 in XML document from class path resource [config/rabbit.xml] is invalid; nested exception is org.xml.sax.SAXParseException; lineNumber: 19; columnNumber: 53; cvc-complex-type.3.2.2: Attribute 'default-delivery-mode' is not allowed to appear in element 'int-amqp:outbound-channel-adapter'.  at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.doLoadBeanDefinitions(XmlBeanDefinitionReader.java:399)  at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.loadBeanDefinitions(XmlBeanDefinitionReader.java:336)  at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.loadBeanDefinitions(XmlBeanDefinitionReader.java:304)  at org.springframework.boot.BeanDefinitionLoader.load(BeanDefinitionLoader.java:180)  at org.springframework.boot.BeanDefinitionLoader.load(BeanDefinitionLoader.java:138)  at org.springframework.boot.BeanDefinitionLoader.load(BeanDefinitionLoader.java:127)  at org.springframework.boot.SpringApplication.load(SpringApplication.java:620)  at org.springframework.boot.SpringApplication.run(SpringApplication.java:315)  at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:139)  at org.springframework.xd.module.core.SimpleModule.initialize(SimpleModule.java:211)  at org.springframework.xd.dirt.module.ModuleDeployer.doDeploy(ModuleDeployer.java:217)  at org.springframework.xd.dirt.module.ModuleDeployer.deploy(ModuleDeployer.java:200)  at org.springframework.xd.dirt.server.DeploymentListener.deployModule(DeploymentListener.java:363)  at org.springframework.xd.dirt.server.DeploymentListener.deployStreamModule(DeploymentListener.java:332)  at org.springframework.xd.dirt.server.DeploymentListener.onChildAdded(DeploymentListener.java:179)  at org.springframework.xd.dirt.server.DeploymentListener.childEvent(DeploymentListener.java:147)  at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:509)  at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:503)  at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)  at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297)  at org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:83)  at org.apache.curator.framework.recipes.cache.PathChildrenCache.callListeners(PathChildrenCache.java:500)  at org.apache.curator.framework.recipes.cache.EventOperation.invoke(EventOperation.java:35)  at org.apache.curator.framework.recipes.cache.PathChildrenCache$10.run(PathChildrenCache.java:762)  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)  at java.util.concurrent.FutureTask.run(FutureTask.java:262)  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)  at java.util.concurrent.FutureTask.run(FutureTask.java:262)  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)  at java.lang.Thread.run(Thread.java:745) Caused by: org.xml.sax.SAXParseException; lineNumber: 19; columnNumber: 53; cvc-complex-type.3.2.2: Attribute 'default-delivery-mode' is not allowed to appear in element 'int-amqp:outbound-channel-adapter'.  at com.sun.org.apache.xerces.internal.util.ErrorHandlerWrapper.createSAXParseException(ErrorHandlerWrapper.java:198)  at com.sun.org.apache.xerces.internal.util.ErrorHandlerWrapper.error(ErrorHandlerWrapper.java:134)  at com.sun.org.apache.xerces.internal.impl.XMLErrorReporter.reportError(XMLErrorReporter.java:437)  at com.sun.org.apache.xerces.internal.impl.XMLErrorReporter.reportError(XMLErrorReporter.java:368)  at com.sun.org.apache.xerces.internal.impl.XMLErrorReporter.reportError(XMLErrorReporter.java:325)  at com.sun.org.apache.xerces.internal.impl.xs.XMLSchemaValidator$XSIErrorReporter.reportError(XMLSchemaValidator.java:458)  at com.sun.org.apache.xerces.internal.impl.xs.XMLSchemaValidator.reportSchemaError(XMLSchemaValidator.java:3237)  at com.sun.org.apache.xerces.internal.impl.xs.XMLSchemaValidator.processAttributes(XMLSchemaValidator.java:2714)  at com.sun.org.apache.xerces.internal.impl.xs.XMLSchemaValidator.handleStartElement(XMLSchemaValidator.java:2056)  at com.sun.org.apache.xerces.internal.impl.xs.XMLSchemaValidator.emptyElement(XMLSchemaValidator.java:766)  at com.sun.org.apache.xerces.internal.impl.XMLNSDocumentScannerImpl.scanStartElement(XMLNSDocumentScannerImpl.java:356)  at com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl$FragmentContentDriver.next(XMLDocumentFragmentScannerImpl.java:2786)  at com.sun.org.apache.xerces.internal.impl.XMLDocumentScannerImpl.next(XMLDocumentScannerImpl.java:606)  at com.sun.org.apache.xerces.internal.impl.XMLNSDocumentScannerImpl.next(XMLNSDocumentScannerImpl.java:117)  at com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl.scanDocument(XMLDocumentFragmentScannerImpl.java:510)  at com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:848)  at com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:777)  at com.sun.org.apache.xerces.internal.parsers.XMLParser.parse(XMLParser.java:141)  at com.sun.org.apache.xerces.internal.parsers.DOMParser.parse(DOMParser.java:243)  at com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderImpl.parse(DocumentBuilderImpl.java:347)  at org.springframework.beans.factory.xml.DefaultDocumentLoader.loadDocument(DefaultDocumentLoader.java:76)  at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.doLoadDocument(XmlBeanDefinitionReader.java:429)  at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.doLoadBeanDefinitions(XmlBeanDefinitionReader.java:391)  ... 30 more 09:44:16,036 1.1.0.SNAP ERROR DeploymentsPathChildrenCache-0 server.DeploymentListener - Exception deploying module org.springframework.beans.factory.xml.XmlBeanDefinitionStoreException: Line 19 in XML document from class path resource [config/rabbit.xml] is invalid; nested exception is org.xml.sax.SAXParseException; lineNumber: 19; columnNumber: 53; cvc-complex-type.3.2.2: Attribute 'default-delivery-mode' is not allowed to appear in element 'int-amqp:outbound-channel-adapter'.  at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.doLoadBeanDefinitions(XmlBeanDefinitionReader.java:399)  at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.loadBeanDefinitions(XmlBeanDefinitionReader.java:336)  at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.loadBeanDefinitions(XmlBeanDefinitionReader.java:304)  at org.springframework.boot.BeanDefinitionLoader.load(BeanDefinitionLoader.java:180)  at org.springframework.boot.BeanDefinitionLoader.load(BeanDefinitionLoader.java:138)  at org.springframework.boot.BeanDefinitionLoader.load(BeanDefinitionLoader.java:127)  at org.springframework.boot.SpringApplication.load(SpringApplication.java:620)  at org.springframework.boot.SpringApplication.run(SpringApplication.java:315)  at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:139)  at org.springframework.xd.module.core.SimpleModule.initialize(SimpleModule.java:211)  at org.springframework.xd.dirt.module.ModuleDeployer.doDeploy(ModuleDeployer.java:217)  at org.springframework.xd.dirt.module.ModuleDeployer.deploy(ModuleDeployer.java:200)  at org.springframework.xd.dirt.server.DeploymentListener.deployModule(DeploymentListener.java:363)  at org.springframework.xd.dirt.server.DeploymentListener.deployStreamModule(DeploymentListener.java:332)  at org.springframework.xd.dirt.server.DeploymentListener.onChildAdded(DeploymentListener.java:179)  at org.springframework.xd.dirt.server.DeploymentListener.childEvent(DeploymentListener.java:147)  at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:509)  at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:503)  at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)  at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297)  at org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:83)  at org.apache.curator.framework.recipes.cache.PathChildrenCache.callListeners(PathChildrenCache.java:500)  at org.apache.curator.framework.recipes.cache.EventOperation.invoke(EventOperation.java:35)  at org.apache.curator.framework.recipes.cache.PathChildrenCache$10.run(PathChildrenCache.java:762)  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)  at java.util.concurrent.FutureTask.run(FutureTask.java:262)  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)  at java.util.concurrent.FutureTask.run(FutureTask.java:262)  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)  at java.lang.Thread.run(Thread.java:745) Caused by: org.xml.sax.SAXParseException; lineNumber: 19; columnNumber: 53; cvc-complex-type.3.2.2: Attribute 'default-delivery-mode' is not allowed to appear in element 'int-amqp:outbound-channel-adapter'.  at com.sun.org.apache.xerces.internal.util.ErrorHandlerWrapper.createSAXParseException(ErrorHandlerWrapper.java:198)  at com.sun.org.apache.xerces.internal.util.ErrorHandlerWrapper.error(ErrorHandlerWrapper.java:134)  at com.sun.org.apache.xerces.internal.impl.XMLErrorReporter.reportError(XMLErrorReporter.java:437)  at com.sun.org.apache.xerces.internal.impl.XMLErrorReporter.reportError(XMLErrorReporter.java:368)  at com.sun.org.apache.xerces.internal.impl.XMLErrorReporter.reportError(XMLErrorReporter.java:325)  at com.sun.org.apache.xerces.internal.impl.xs.XMLSchemaValidator$XSIErrorReporter.reportError(XMLSchemaValidator.java:458)  at com.sun.org.apache.xerces.internal.impl.xs.XMLSchemaValidator.reportSchemaError(XMLSchemaValidator.java:3237)  at com.sun.org.apache.xerces.internal.impl.xs.XMLSchemaValidator.processAttributes(XMLSchemaValidator.java:2714)  at com.sun.org.apache.xerces.internal.impl.xs.XMLSchemaValidator.handleStartElement(XMLSchemaValidator.java:2056)  at com.sun.org.apache.xerces.internal.impl.xs.XMLSchemaValidator.emptyElement(XMLSchemaValidator.java:766)  at com.sun.org.apache.xerces.internal.impl.XMLNSDocumentScannerImpl.scanStartElement(XMLNSDocumentScannerImpl.java:356)  at com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl$FragmentContentDriver.next(XMLDocumentFragmentScannerImpl.java:2786)  at com.sun.org.apache.xerces.internal.impl.XMLDocumentScannerImpl.next(XMLDocumentScannerImpl.java:606)  at com.sun.org.apache.xerces.internal.impl.XMLNSDocumentScannerImpl.next(XMLNSDocumentScannerImpl.java:117)  at com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl.scanDocument(XMLDocumentFragmentScannerImpl.java:510)  at com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:848)  at com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:777)  at com.sun.org.apache.xerces.internal.parsers.XMLParser.parse(XMLParser.java:141)  at com.sun.org.apache.xerces.internal.parsers.DOMParser.parse(DOMParser.java:243)  at com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderImpl.parse(DocumentBuilderImpl.java:347)  at org.springframework.beans.factory.xml.DefaultDocumentLoader.loadDocument(DefaultDocumentLoader.java:76)  at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.doLoadDocument(XmlBeanDefinitionReader.java:429)  at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.doLoadBeanDefinitions(XmlBeanDefinitionReader.java:391)  10:14:04,678 1.1.0.SNAP  INFO DeploymentSupervisor-0 server.StreamDeploymentListener - Deployment status for stream 'foo': DeploymentStatus{state=failed,error(s)=org.springframework.amqp.UncategorizedAmqpException: java.lang.IllegalArgumentException: interface org.springframework.amqp.rabbit.listener.SimpleMessageListenerContainer$ContainerDelegate is not visible from class loader  at org.springframework.amqp.rabbit.support.RabbitExceptionTranslator.convertRabbitAccessException(RabbitExceptionTranslator.java:66)  at org.springframework.amqp.rabbit.connection.RabbitAccessor.convertRabbitAccessException(RabbitAccessor.java:110)  at org.springframework.amqp.rabbit.listener.AbstractMessageListenerContainer.initialize(AbstractMessageListenerContainer.java:426)  at org.springframework.amqp.rabbit.listener.AbstractMessageListenerContainer.afterPropertiesSet(AbstractMessageListenerContainer.java:385)  at org.springframework.xd.dirt.integration.rabbit.RabbitMessageBus.doRegisterConsumer(RabbitMessageBus.java:367)  at org.springframework.xd.dirt.integration.rabbit.RabbitMessageBus.bindConsumer(RabbitMessageBus.java:308)  at org.springframework.xd.dirt.plugins.AbstractMessageBusBinderPlugin.bindMessageConsumer(AbstractMessageBusBinderPlugin.java:183)  at org.springframework.xd.dirt.plugins.AbstractMessageBusBinderPlugin.bindConsumerAndProducers(AbstractMessageBusBinderPlugin.java:138)  at org.springframework.xd.dirt.plugins.stream.StreamPlugin.postProcessModule(StreamPlugin.java:73)  at org.springframework.xd.dirt.module.ModuleDeployer.postProcessModule(ModuleDeployer.java:238)  at org.springframework.xd.dirt.module.ModuleDeployer.doDeploy(ModuleDeployer.java:218)  at org.springframework.xd.dirt.module.ModuleDeployer.deploy(ModuleDeployer.java:200)  at org.springframework.xd.dirt.server.DeploymentListener.deployModule(DeploymentListener.java:363)  at org.springframework.xd.dirt.server.DeploymentListener.deployStreamModule(DeploymentListener.java:332)  at org.springframework.xd.dirt.server.DeploymentListener.onChildAdded(DeploymentListener.java:179)  at org.springframework.xd.dirt.server.DeploymentListener.childEvent(DeploymentListener.java:147)  at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:509)  at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:503)  at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)  at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297)  at org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:83)  at org.apache.curator.framework.recipes.cache.PathChildrenCache.callListeners(PathChildrenCache.java:500)  at org.apache.curator.framework.recipes.cache.EventOperation.invoke(EventOperation.java:35)  at org.apache.curator.framework.recipes.cache.PathChildrenCache$10.run(PathChildrenCache.java:762)  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)  at java.util.concurrent.FutureTask.run(FutureTask.java:262)  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)  at java.util.concurrent.FutureTask.run(FutureTask.java:262)  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)  at java.lang.Thread.run(Thread.java:745) Caused by: java.lang.IllegalArgumentException: interface org.springframework.amqp.rabbit.listener.SimpleMessageListenerContainer$ContainerDelegate is not visible from class loader  at java.lang.reflect.Proxy$ProxyClassFactory.apply(Proxy.java:616)  at java.lang.reflect.Proxy$ProxyClassFactory.apply(Proxy.java:592)  at java.lang.reflect.WeakCache$Factory.get(WeakCache.java:244)  at java.lang.reflect.WeakCache.get(WeakCache.java:141)  at java.lang.reflect.Proxy.getProxyClass0(Proxy.java:455)  at java.lang.reflect.Proxy.newProxyInstance(Proxy.java:738)  at org.springframework.aop.framework.JdkDynamicAopProxy.getProxy(JdkDynamicAopProxy.java:121)  at org.springframework.aop.framework.JdkDynamicAopProxy.getProxy(JdkDynamicAopProxy.java:111)  at org.springframework.aop.framework.ProxyFactory.getProxy(ProxyFactory.java:96)  at org.springframework.amqp.rabbit.listener.SimpleMessageListenerContainer.initializeProxy(SimpleMessageListenerContainer.java:586)  at org.springframework.amqp.rabbit.listener.SimpleMessageListenerContainer.doInitialize(SimpleMessageListenerContainer.java:612)  at org.springframework.amqp.rabbit.listener.AbstractMessageListenerContainer.initialize(AbstractMessageListenerContainer.java:424)  ... 28 more """,3.0
1,XD-2502,FEATURE,Done,MEDIUM,"""KafkaSourceSinkTests to use embedded Kafka server""","""Test is failing since Kafka isn't installed on the CI server.  Using an embedded server will make the testing more robust vs. needing an external server.""","""""""Test is failing since Kafka isn't installed on the CI server.  Using an embedded server will make the testing more robust vs. needing an external server.""""""",,2.0
1,XD-2501,FEATURE,Done,MEDIUM,"""Upgrade to Boot 1.2.0 RELEASE and the dependencies""","""As a XD Admin, I'd like to upgrade to Spring Boot 1.2.0 RELEASE and the associated dependencies so that we can catch up with the latest features, bug-fixes and enhancements.   *Following XD dependencies needs upgraded to sync-up with Boot 1.2.0 RELEASE:*  <activemq.version>5.10.0</activemq.version> <aspectj.version>1.8.4</aspectj.version> <commons-dbcp2.version>2.0.1</commons-dbcp2.version> <h2.version>1.4.182</h2.version> <hibernate.version>dd4.3.7.Final</hibernate.version> <hibernate-validator.version>5.1.3.Final</hibernate-validator.version> <hikaricp.version>2.2.5</hikaricp.version> <hornetq.version>2.4.5.Final</hornetq.version> <httpasyncclient.version>4.0.2</httpasyncclient.version> <httpclient.version>4.3.6</httpclient.version> <jackson.version>2.4.4</jackson.version> <janino.version>2.6.1</janino.version> <jetty.version>9.2.4.v20141103</jetty.version> <jetty-jsp.version>2.2.0.v201112011158</jetty-jsp.version> <joda-time.version>2.5</joda-time.version> <jolokia.version>1.2.3</jolokia.version> <junit.version>4.12</junit.version> <liquibase.version>3.3.0</liquibase.version> <log4j.version>1.2.17</log4j.version> <log4j2.version>2.1</log4j2.version> <mockito.version>1.10.8</mockito.version> <mongodb.version>2.12.4</mongodb.version> <mysql.version>5.1.34</mysql.version> <reactor.version>1.1.5.RELEASE</reactor.version> <reactor-spring.version>1.1.3.RELEASE</reactor-spring.version> <servlet-api.version>3.1.0</servlet-api.version> <spring.version>4.1.3.RELEASE</spring.version> <spring-batch.version>3.0.2.RELEASE</spring-batch.version> <spring-data-releasetrain.version>Evans-SR1</spring-data-releasetrain.version> <spring-hateoas.version>0.16.0.RELEASE</spring-hateoas.version> <spring-mobile.version>1.1.3.RELEASE</spring-mobile.version> <spring-security.version>3.2.5.RELEASE</spring-security.version> <tomcat.version>8.0.15</tomcat.version> <undertow.version>1.1.1.Final</undertow.version>""","""""""As a XD Admin, I'd like to upgrade to Spring Boot 1.2.0 RELEASE and the associated dependencies so that we can catch up with the latest features, bug-fixes and enhancements.   *Following XD dependencies needs upgraded to sync-up with Boot 1.2.0 RELEASE:*  <activemq.version>5.10.0</activemq.version> <aspectj.version>1.8.4</aspectj.version> <commons-dbcp2.version>2.0.1</commons-dbcp2.version> <h2.version>1.4.182</h2.version> <hibernate.version>dd4.3.7.Final</hibernate.version> <hibernate-validator.version>5.1.3.Final</hibernate-validator.version> <hikaricp.version>2.2.5</hikaricp.version> <hornetq.version>2.4.5.Final</hornetq.version> <httpasyncclient.version>4.0.2</httpasyncclient.version> <httpclient.version>4.3.6</httpclient.version> <jackson.version>2.4.4</jackson.version> <janino.version>2.6.1</janino.version> <jetty.version>9.2.4.v20141103</jetty.version> <jetty-jsp.version>2.2.0.v201112011158</jetty-jsp.version> <joda-time.version>2.5</joda-time.version> <jolokia.version>1.2.3</jolokia.version> <junit.version>4.12</junit.version> <liquibase.version>3.3.0</liquibase.version> <log4j.version>1.2.17</log4j.version> <log4j2.version>2.1</log4j2.version> <mockito.version>1.10.8</mockito.version> <mongodb.version>2.12.4</mongodb.version> <mysql.version>5.1.34</mysql.version> <reactor.version>1.1.5.RELEASE</reactor.version> <reactor-spring.version>1.1.3.RELEASE</reactor-spring.version> <servlet-api.version>3.1.0</servlet-api.version> <spring.version>4.1.3.RELEASE</spring.version> <spring-batch.version>3.0.2.RELEASE</spring-batch.version> <spring-data-releasetrain.version>Evans-SR1</spring-data-releasetrain.version> <spring-hateoas.version>0.16.0.RELEASE</spring-hateoas.version> <spring-mobile.version>1.1.3.RELEASE</spring-mobile.version> <spring-security.version>3.2.5.RELEASE</spring-security.version> <tomcat.version>8.0.15</tomcat.version> <undertow.version>1.1.1.Final</undertow.version>""""""",,5.0
1,XD-2499,FEATURE,Done,MEDIUM,"""Document 'partitionResultsTimeout' metadata attribute""","""As a user, I'd like to use _partitionResultsTimeout_ attribute for jobs that inherit singlestep-partitioning strategy but it is not exposed as a metadata attribute in the wiki.   *Note:* The property should be available for all the jobs that import; 3 OOTB jobs have it imported (ref. attachment)""","""""""As a user, I'd like to use _partitionResultsTimeout_ attribute for jobs that inherit singlestep-partitioning strategy but it is not exposed as a metadata attribute in the wiki.   *Note:* The property should be available for all the jobs that import; 3 OOTB jobs have it imported (ref. attachment)""""""",,1.0
1,XD-2497,FEATURE,Done,MEDIUM,"""Investigate lack of falling back to origin/master when building docs on a branch""","""When building on a branch, the docs should be defaulting to build from origin/master, but that doesn't seem to be happening.  Instead an explicit -Pwikibranch=origin/master is required to be specified on the command line.  ""","""""""When building on a branch, the docs should be defaulting to build from origin/master, but that doesn't seem to be happening.  Instead an explicit -Pwikibranch=origin/master is required to be specified on the command line.  """"""",,2.0
1,XD-2496,FEATURE,Done,MEDIUM,"""Refactor use of getContainerHostForSource in integration tests""","""Some cleanup to make the tests a bit easer to read.""","""""""Some cleanup to make the tests a bit easer to read.""""""",,1.0
1,XD-2495,BUG,Done,URGENT,"""Add Request/Reply support to Kafka message bus""","""* Environment: ** Can be reproduced on local machine with Admin and a single container. * create the following job ** job create ogg --definition """"filejdbc --resources=file:filejdbctest//filejdbctestpartition* --names=data --tableName=filejdbctest --initializeDatabase=true """" --deploy * note: this works on Rabbit and Redis as a message bus * The following exception is thrown on the admin: 6:54:22,856 1.1.0.SNAP  INFO DeploymentSupervisor-0 server.JobDeploymentListener - Deployment status for job 'ogg': DeploymentStatus{state=failed,error(s)=java.lang.UnsupportedOperationException: Auto-generated method stub  at org.springframework.xd.dirt.integration.kafka.KafkaMessageBus.bindRequestor(KafkaMessageBus.java:289)  at org.springframework.xd.dirt.plugins.job.JobPartitionerPlugin.processPartitionedJob(JobPartitionerPlugin.java:69)  at org.springframework.xd.dirt.plugins.job.JobPartitionerPlugin.postProcessModule(JobPartitionerPlugin.java:53)  at org.springframework.xd.dirt.module.ModuleDeployer.postProcessModule(ModuleDeployer.java:238)  at org.springframework.xd.dirt.module.ModuleDeployer.doDeploy(ModuleDeployer.java:218)  at org.springframework.xd.dirt.module.ModuleDeployer.deploy(ModuleDeployer.java:200)  at org.springframework.xd.dirt.server.DeploymentListener.deployModule(DeploymentListener.java:363)  at org.springframework.xd.dirt.server.DeploymentListener.deployJobModule(DeploymentListener.java:289)  at org.springframework.xd.dirt.server.DeploymentListener.onChildAdded(DeploymentListener.java:179)  at org.springframework.xd.dirt.server.DeploymentListener.childEvent(DeploymentListener.java:147)  at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:509)  at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:503)  at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)  at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297)  at org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:83)  at org.apache.curator.framework.recipes.cache.PathChildrenCache.callListeners(PathChildrenCache.java:500)  at org.apache.curator.framework.recipes.cache.EventOperation.invoke(EventOperation.java:35)  at org.apache.curator.framework.recipes.cache.PathChildrenCache$10.run(PathChildrenCache.java:762)  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)  at java.util.concurrent.FutureTask.run(FutureTask.java:262)  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)  at java.util.concurrent.FutureTask.run(FutureTask.java:262)  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)  at java.lang.Thread.run(Thread.java:745) }  * The following exception is thrown on the container 21:08:14,721 1.1.0.SNAP  WARN DeploymentsPathChildrenCache-0 config.ReleaseStrategyFactoryBean - No annotated method found; falling back to SequenceSizeReleaseStrategy, target:org.springframework.batch.integration.partition.MessageChannelPartitionHandler@692ee39f, methodName:null 21:08:15,946 1.1.0.SNAP ERROR DeploymentsPathChildrenCache-0 server.DeploymentListener - Exception deploying module java.lang.UnsupportedOperationException: Auto-generated method stub  at org.springframework.xd.dirt.integration.kafka.KafkaMessageBus.bindRequestor(KafkaMessageBus.java:289)  at org.springframework.xd.dirt.plugins.job.JobPartitionerPlugin.processPartitionedJob(JobPartitionerPlugin.java:69)  at org.springframework.xd.dirt.plugins.job.JobPartitionerPlugin.postProcessModule(JobPartitionerPlugin.java:53)  at org.springframework.xd.dirt.module.ModuleDeployer.postProcessModule(ModuleDeployer.java:238)  at org.springframework.xd.dirt.module.ModuleDeployer.doDeploy(ModuleDeployer.java:218)  at org.springframework.xd.dirt.module.ModuleDeployer.deploy(ModuleDeployer.java:200)  at org.springframework.xd.dirt.server.DeploymentListener.deployModule(DeploymentListener.java:363)  at org.springframework.xd.dirt.server.DeploymentListener.deployJobModule(DeploymentListener.java:289)  at org.springframework.xd.dirt.server.DeploymentListener.onChildAdded(DeploymentListener.java:179)  at org.springframework.xd.dirt.server.DeploymentListener.childEvent(DeploymentListener.java:147)  at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:509)  at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:503)  at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)  at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297)  at org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:83)  at org.apache.curator.framework.recipes.cache.PathChildrenCache.callListeners(PathChildrenCache.java:500)  at org.apache.curator.framework.recipes.cache.EventOperation.invoke(EventOperation.java:35)  at org.apache.curator.framework.recipes.cache.PathChildrenCache$10.run(PathChildrenCache.java:762)  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)  at java.util.concurrent.FutureTask.run(FutureTask.java:262)  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)  at java.util.concurrent.FutureTask.run(FutureTask.java:262)  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)  at java.lang.Thread.run(Thread.java:745)""","""""""* Environment: ** Can be reproduced on local machine with Admin and a single container. * create the following job ** job create ogg --definition """"""""filejdbc --resources=file:filejdbctest//filejdbctestpartition* --names=data --tableName=filejdbctest --initializeDatabase=true """""""" --deploy * note: this works on Rabbit and Redis as a message bus * The following exception is thrown on the admin: 6:54:22,856 1.1.0.SNAP  INFO DeploymentSupervisor-0 server.JobDeploymentListener - Deployment status for job 'ogg': DeploymentStatus{state=failed,error(s)=java.lang.UnsupportedOperationException: Auto-generated method stub  at org.springframework.xd.dirt.integration.kafka.KafkaMessageBus.bindRequestor(KafkaMessageBus.java:289)  at org.springframework.xd.dirt.plugins.job.JobPartitionerPlugin.processPartitionedJob(JobPartitionerPlugin.java:69)  at org.springframework.xd.dirt.plugins.job.JobPartitionerPlugin.postProcessModule(JobPartitionerPlugin.java:53)  at org.springframework.xd.dirt.module.ModuleDeployer.postProcessModule(ModuleDeployer.java:238)  at org.springframework.xd.dirt.module.ModuleDeployer.doDeploy(ModuleDeployer.java:218)  at org.springframework.xd.dirt.module.ModuleDeployer.deploy(ModuleDeployer.java:200)  at org.springframework.xd.dirt.server.DeploymentListener.deployModule(DeploymentListener.java:363)  at org.springframework.xd.dirt.server.DeploymentListener.deployJobModule(DeploymentListener.java:289)  at org.springframework.xd.dirt.server.DeploymentListener.onChildAdded(DeploymentListener.java:179)  at org.springframework.xd.dirt.server.DeploymentListener.childEvent(DeploymentListener.java:147)  at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:509)  at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:503)  at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)  at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297)  at org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:83)  at org.apache.curator.framework.recipes.cache.PathChildrenCache.callListeners(PathChildrenCache.java:500)  at org.apache.curator.framework.recipes.cache.EventOperation.invoke(EventOperation.java:35)  at org.apache.curator.framework.recipes.cache.PathChildrenCache$10.run(PathChildrenCache.java:762)  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)  at java.util.concurrent.FutureTask.run(FutureTask.java:262)  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)  at java.util.concurrent.FutureTask.run(FutureTask.java:262)  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)  at java.lang.Thread.run(Thread.java:745) }  * The following exception is thrown on the container 21:08:14,721 1.1.0.SNAP  WARN DeploymentsPathChildrenCache-0 config.ReleaseStrategyFactoryBean - No annotated method found; falling back to SequenceSizeReleaseStrategy, target:org.springframework.batch.integration.partition.MessageChannelPartitionHandler@692ee39f, methodName:null 21:08:15,946 1.1.0.SNAP ERROR DeploymentsPathChildrenCache-0 server.DeploymentListener - Exception deploying module java.lang.UnsupportedOperationException: Auto-generated method stub  at org.springframework.xd.dirt.integration.kafka.KafkaMessageBus.bindRequestor(KafkaMessageBus.java:289)  at org.springframework.xd.dirt.plugins.job.JobPartitionerPlugin.processPartitionedJob(JobPartitionerPlugin.java:69)  at org.springframework.xd.dirt.plugins.job.JobPartitionerPlugin.postProcessModule(JobPartitionerPlugin.java:53)  at org.springframework.xd.dirt.module.ModuleDeployer.postProcessModule(ModuleDeployer.java:238)  at org.springframework.xd.dirt.module.ModuleDeployer.doDeploy(ModuleDeployer.java:218)  at org.springframework.xd.dirt.module.ModuleDeployer.deploy(ModuleDeployer.java:200)  at org.springframework.xd.dirt.server.DeploymentListener.deployModule(DeploymentListener.java:363)  at org.springframework.xd.dirt.server.DeploymentListener.deployJobModule(DeploymentListener.java:289)  at org.springframework.xd.dirt.server.DeploymentListener.onChildAdded(DeploymentListener.java:179)  at org.springframework.xd.dirt.server.DeploymentListener.childEvent(DeploymentListener.java:147)  at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:509)  at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:503)  at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)  at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297)  at org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:83)  at org.apache.curator.framework.recipes.cache.PathChildrenCache.callListeners(PathChildrenCache.java:500)  at org.apache.curator.framework.recipes.cache.EventOperation.invoke(EventOperation.java:35)  at org.apache.curator.framework.recipes.cache.PathChildrenCache$10.run(PathChildrenCache.java:762)  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)  at java.util.concurrent.FutureTask.run(FutureTask.java:262)  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)  at java.util.concurrent.FutureTask.run(FutureTask.java:262)  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)  at java.lang.Thread.run(Thread.java:745)""""""",,3.0
1,XD-2492,IMPROVEMENT,Done,MEDIUM,"""Increase Exit Description Text field on Job Execution Process step page""","""The Step Execution Process (http://localhost:9393/admin-ui/#/jobs/executions/38/52) page should list more lines of text for 'Exit Description' field to make sense of error messages.  *Scope:* Investigate how much information can be collected directly from the ExecutionContext. It may be dependent on the error types. Let's have the observation documented to decide next steps.  ""","""""""The Step Execution Process (http://localhost:9393/admin-ui/#/jobs/executions/38/52) page should list more lines of text for 'Exit Description' field to make sense of error messages.  *Scope:* Investigate how much information can be collected directly from the ExecutionContext. It may be dependent on the error types. Let's have the observation documented to decide next steps.  """"""",,2.0
1,XD-2491,BUG,Done,HIGH,"""JDBCHDFS Master Process Timeout error""","""The JDBCHDFS Master process fails with a timeout error while the child process is still processing data.  The error message on the error message on the master process is:  org.springframework.integration.MessageTimeoutException: Timeout occurred before all partitions returned  at org.springframework.batch.integration.partition.MessageChannelPartitionHandler.handle(MessageChannelPartitionHandler.java:141)  at org.springframework.batch.core.partition.support.PartitionStep.doExecute(PartitionStep.java:106)  at org.springframework.batch.core.step.AbstractStep.execute(AbstractStep.java:198)  at org.springframework.batch.core.job.SimpleStepHandler.handleStep(SimpleStepHandler.java:148)  at org.springframework.batch.core.job.flow.JobFlowExecutor.executeStep(JobFlowExecutor.java:64)  at org.springframework.batch.core.job.flow.support.state.StepState.handle(StepState.java:67)  at org.springframework.batch.core.job.flow.support.SimpleFlow.resume(SimpleFlow.java:162)  at org.springframework.batch.core.job.flow.support.SimpleFlow.start(SimpleFlow.java:141)  at org.springframework.batch.core.job.flow.FlowJob.doExecute(FlowJob.java:134)  at org.springframework.batch.core.job.AbstractJob.execute(AbstractJob.java:304)  at org.springframework.batch.core.launch.support.SimpleJobLauncher$1.run(SimpleJobLauncher.java:135)  at org.springframework.core.task.SyncTaskExecutor.execute(SyncTaskExecutor.java:50)  at org.springframework.batch.core.launch.support.SimpleJobLauncher.run(SimpleJobLauncher.java:128)  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)  at java.lang.reflect.Method.invoke(Method.java:606)  at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)  at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)  at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)  at org.springframework.batch.core.configuration.annotation.SimpleBatchConfiguration$PassthruAdvice.invoke(SimpleBatchConfiguration.java:127)  at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)  at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)  at com.sun.proxy.$Proxy47.run(Unknown Source)  at org.springframework.batch.integration.launch.JobLaunchingMessageHandler.launch(JobLaunchingMessageHandler.java:50)  at sun.reflect.NativeMethodAccessorImpl""","""""""The JDBCHDFS Master process fails with a timeout error while the child process is still processing data.  The error message on the error message on the master process is:  org.springframework.integration.MessageTimeoutException: Timeout occurred before all partitions returned  at org.springframework.batch.integration.partition.MessageChannelPartitionHandler.handle(MessageChannelPartitionHandler.java:141)  at org.springframework.batch.core.partition.support.PartitionStep.doExecute(PartitionStep.java:106)  at org.springframework.batch.core.step.AbstractStep.execute(AbstractStep.java:198)  at org.springframework.batch.core.job.SimpleStepHandler.handleStep(SimpleStepHandler.java:148)  at org.springframework.batch.core.job.flow.JobFlowExecutor.executeStep(JobFlowExecutor.java:64)  at org.springframework.batch.core.job.flow.support.state.StepState.handle(StepState.java:67)  at org.springframework.batch.core.job.flow.support.SimpleFlow.resume(SimpleFlow.java:162)  at org.springframework.batch.core.job.flow.support.SimpleFlow.start(SimpleFlow.java:141)  at org.springframework.batch.core.job.flow.FlowJob.doExecute(FlowJob.java:134)  at org.springframework.batch.core.job.AbstractJob.execute(AbstractJob.java:304)  at org.springframework.batch.core.launch.support.SimpleJobLauncher$1.run(SimpleJobLauncher.java:135)  at org.springframework.core.task.SyncTaskExecutor.execute(SyncTaskExecutor.java:50)  at org.springframework.batch.core.launch.support.SimpleJobLauncher.run(SimpleJobLauncher.java:128)  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)  at java.lang.reflect.Method.invoke(Method.java:606)  at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)  at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)  at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)  at org.springframework.batch.core.configuration.annotation.SimpleBatchConfiguration$PassthruAdvice.invoke(SimpleBatchConfiguration.java:127)  at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)  at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)  at com.sun.proxy.$Proxy47.run(Unknown Source)  at org.springframework.batch.integration.launch.JobLaunchingMessageHandler.launch(JobLaunchingMessageHandler.java:50)  at sun.reflect.NativeMethodAccessorImpl""""""",,3.0
1,XD-2490,FEATURE,Done,MEDIUM,"""Update Reactor Stream processor to use latest snapshots""","""The code base is changing a bit, so using 2.0 M1 for development is stable up until all major JIRA issues have bee completed.  Then we should track snapshots in preparation to move to 2.0. M2 when it gets released.""","""""""The code base is changing a bit, so using 2.0 M1 for development is stable up until all major JIRA issues have bee completed.  Then we should track snapshots in preparation to move to 2.0. M2 when it gets released.""""""",,2.0
1,XD-2489,FEATURE,Done,MEDIUM,"""Reference documentation on creating Reactive Stream processor/sink""","""Reference documentation on creating Reactive Stream processor/sink""","""Reference documentation on creating Reactive Stream processor/sink""",,3.0
1,XD-2488,FEATURE,Done,MEDIUM,"""Create sample module in spring-xd-modules for a Reactor Stream processor""","""A sample, perhaps taken from Pivotal Labs use-case in Denver, that would calculate some time window averages for a many individual senor values .""","""""""A sample, perhaps taken from Pivotal Labs use-case in Denver, that would calculate some time window averages for a many individual senor values .""""""",,3.0
1,XD-2487,FEATURE,Done,MEDIUM,"""Create ReactorMessageHandler for Reactor based XD processor/sink modules""","""The module should be flexible to act as a sink as well as a processor.  ErrorHandling will be considered as part of another JIRA""","""""""The module should be flexible to act as a sink as well as a processor.  ErrorHandling will be considered as part of another JIRA""""""",,5.0
1,XD-2486,BUG,Done,MEDIUM,"""Context Deserialize Doesn't Use Parent First Classloader""","""If a class is added to a batch execution context that is located in an isolated context, an exception will be thrown when that object is deserialized.  It appears the serialize doesn't use the ParentFirstClassloader during deserialization.""","""""""If a class is added to a batch execution context that is located in an isolated context, an exception will be thrown when that object is deserialized.  It appears the serialize doesn't use the ParentFirstClassloader during deserialization.""""""",,8.0
1,XD-2485,FEATURE,Done,MEDIUM,"""Update spring-data-hadoop version to 2.0.4 for XD 1.0.3""","""Update spring-data-hadoop version to 2.0.4 for XD 1.0.3""","""Update spring-data-hadoop version to 2.0.4 for XD 1.0.3""",,1.0
1,XD-2484,FEATURE,Done,MEDIUM,"""Update spring-data-hadoop version to 2.1.0.M3""","""Update spring-data-hadoop version to 2.1.0.M3""","""Update spring-data-hadoop version to 2.1.0.M3""",,1.0
1,XD-2483,FEATURE,Done,MEDIUM,"""Add codec option to hdfs-dataset sink""","""As a user, I would like to be able disable snappy compression when using hdfs-dataset sink with Avro files. I'd also like to be able to provide a different codec.""","""""""As a user, I would like to be able disable snappy compression when using hdfs-dataset sink with Avro files. I'd also like to be able to provide a different codec.""""""",,1.0
1,XD-2482,IMPROVEMENT,Done,HIGH,"""Add """"initialDelay"""" to """"source:trigger""""""","""Currently, the {{source:trigger}} module is based on 3 profiles: {{date}}, {{cron}} or {{fixedDelay}}, where the latter has precedence over the former in {{TriggerSourceOptionsMetadata}}:  {code:java} @Override public String[] profilesToActivate() {     if (cron != null) {         return new String[] { """"use-cron"""" };     }     else if (fixedDelay != null) {         return new String[] { """"use-delay"""" };     }     else {         return new String[] { """"use-date"""" };     } } {code}  Therefore it is not possible to combine {{date}} and {{fixedDelay}} to start off at a specific point in time, and then repeat every X seconds.  This is a request to provide another parameter to {{source:trigger}} such as *{{initialDelay}}* to be able to achieve the desired behaviour.""","""""""Currently, the {{source:trigger}} module is based on 3 profiles: {{date}}, {{cron}} or {{fixedDelay}}, where the latter has precedence over the former in {{TriggerSourceOptionsMetadata}}:    Therefore it is not possible to combine {{date}} and {{fixedDelay}} to start off at a specific point in time, and then repeat every X seconds.  This is a request to provide another parameter to {{source:trigger}} such as *{{initialDelay}}* to be able to achieve the desired behaviour.""""""",""" @Override public String[] profilesToActivate() {     if (cron != null) {         return new String[] { """"""""use-cron"""""""" };     }     else if (fixedDelay != null) {         return new String[] { """"""""use-delay"""""""" };     }     else {         return new String[] { """"""""use-date"""""""" };     } } """,1.0
1,XD-2481,FEATURE,Done,HIGH,"""Define developer facing interfaces for Reactor Stream processors""","""What is the core interface contract users will be exposed to when creating a processor module that uses Reactor's Stream API.   Some consideration for error handling should be considered as it maybe outside normal exception throwing signatures.""","""""""What is the core interface contract users will be exposed to when creating a processor module that uses Reactor's Stream API.   Some consideration for error handling should be considered as it maybe outside normal exception throwing signatures.""""""",,2.0
1,XD-2480,FEATURE,Done,MEDIUM,"""Benchmark: Sqoop vs. jdbchdfs""","""As a QA, I'd like to benchmark _Sqoop_ vs. _jdbchdfs_ batch job so that I can compare and contrast performance stats. ""","""""""As a QA, I'd like to benchmark _Sqoop_ vs. _jdbchdfs_ batch job so that I can compare and contrast performance stats. """"""",,5.0
1,XD-2478,FEATURE,Done,MEDIUM,"""Add support to access Sqoop logs""","""As a user, I'd like to access Sqoop logs so that I can troubleshoot or evaluate the errors or current state respectively.   We will have to identify how to capture the Sqoop logs and stream them to our logging mechanism.""","""""""As a user, I'd like to access Sqoop logs so that I can troubleshoot or evaluate the errors or current state respectively.   We will have to identify how to capture the Sqoop logs and stream them to our logging mechanism.""""""",,5.0
1,XD-2477,FEATURE,Done,MEDIUM,"""Add support to stop existing Sqoop jobs""","""As a user, I'd like to have the option to _stop_ an existing Sqoop job so that I can clean-up resources at the time of completion.""","""""""As a user, I'd like to have the option to _stop_ an existing Sqoop job so that I can clean-up resources at the time of completion.""""""",,8.0
1,XD-2475,FEATURE,Done,MEDIUM,"""Add batching support to Spring AMQP/Rabbit""","""As a user, I'd like to have the option to setup _batching_ so that I can ingest data in batches as opposed to payload-at-a-time.""","""""""As a user, I'd like to have the option to setup _batching_ so that I can ingest data in batches as opposed to payload-at-a-time.""""""",,8.0
1,XD-2474,FEATURE,Done,MEDIUM,"""Add support for bindRequestor and bindReplier""","""As a user, I'd like to have the option to implement _bindRequestor_ and _bindReplier_ so that I can """"bind a producer that expects async replies"""" and """"bind a consumer that handles requests from a requestor and asynchronously sends replies"""" respectively.   ""","""""""As a user, I'd like to have the option to implement _bindRequestor_ and _bindReplier_ so that I can """"""""bind a producer that expects async replies"""""""" and """"""""bind a consumer that handles requests from a requestor and asynchronously sends replies"""""""" respectively.   """"""",,3.0
1,XD-2473,FEATURE,Done,MEDIUM,"""Kafka Bus: Add support for ACK mode""","""As a user, I'd like to have the option to _ACK_ messages so that I can guarantee that the message/request sent is successful. ""","""""""As a user, I'd like to have the option to _ACK_ messages so that I can guarantee that the message/request sent is successful. """"""",,3.0
1,XD-2472,IMPROVEMENT,Done,MEDIUM,"""Kafka Bus: Add suppor for async vs. sync producer""","""As a user, I'd like to have the option to choose between async vs. sync Kafka producer so that I can decide what algorithm better suits for sending messages. ""","""""""As a user, I'd like to have the option to choose between async vs. sync Kafka producer so that I can decide what algorithm better suits for sending messages. """"""",,0.0
1,XD-2471,FEATURE,Done,MEDIUM,"""Kafka Bus: Concurrency and compression support""","""As a user, I'd like to have concurrency and compression support for Kafka so that I can increase performance throughput and/or increase responsiveness  *Things to consider:* * make global configuration options be """"defaults"""" and allow per-deployment overrides * add options for  ** concurrency ** compression support""","""""""As a user, I'd like to have concurrency and compression support for Kafka so that I can increase performance throughput and/or increase responsiveness  *Things to consider:* * make global configuration options be """"""""defaults"""""""" and allow per-deployment overrides * add options for  ** concurrency ** compression support""""""",,3.0
1,XD-2469,FEATURE,Done,MEDIUM,"""Parent Modules""","""Thinking about the UBS(?) scenario that MP described. They want all their Cassandra modules to share a connection, leading to the need for a parent context for those modules. We could introduce a parent module for this purpose. The module declares a parent in its properties which likely ends up as a parent module definition in the module definition. When the child module is deployed, its parent must be deployed first if it is not already (i.e., parent is a singleton per container). The module sets that as it's parent context.  We would have to make sure things happen in the correct order so the global context is the parent of the parent (ad infinitum). The alternative is to add a module parent context to the XD hierarchy which is extensible, but this is more elegant IMHO.  Also, in cases that don't require singleton bean definitions, the parent could package and provide common jars to children, e.g., we set the parent module's classloader as the parent classloader, eliminating the need to install common jars in an HDFS path (basically the approach I described https://jira.spring.io/browse/XD-2420)   MP : >> Also, in cases that don't require singleton bean definitions, the parent could package and provide common jars to children, e.g., we set the parent module's classloader as the parent classloader This approach does not offer some of the advantages offered by a central yaml config noted in the JIRA . But it is simpler in many ways. The developer just installs a parent module containing dependent jars and sets that as a parent in the child modules. This requires no additional infrastructure. To address the hadoop scenario we discussed, we would need another level of indirection so the parent of the hdfs modules is bound to the configured hadoop distro.   e.g. something like  parent = ${xd.hadoop.distro} in module properties  MF: Possibly the parent modules could go in the """"common"""" directory? They should be considered """"abstract"""" also - in the same sense as abstract bean definitions in a Spring context (and they should only be started on demand when needed by at least one concrete child module - then destroyed when the last child module is destroyed).  Maybe this would also allow us to wrap up those xml files that currently live in """"common"""" so that they are treated as parent modules?  MF: Yea, that sounds good wrt to common.  Also it might be a good idea to enable spring to throw an error if it finds more than one bean of the same name in the application context - i think that applies to searching in parent contexts as well.  this would avoid the 'last one wins' rule and give more deterministic behavior. ""","""""""Thinking about the UBS(?) scenario that MP described. They want all their Cassandra modules to share a connection, leading to the need for a parent context for those modules. We could introduce a parent module for this purpose. The module declares a parent in its properties which likely ends up as a parent module definition in the module definition. When the child module is deployed, its parent must be deployed first if it is not already (i.e., parent is a singleton per container). The module sets that as it's parent context.  We would have to make sure things happen in the correct order so the global context is the parent of the parent (ad infinitum). The alternative is to add a module parent context to the XD hierarchy which is extensible, but this is more elegant IMHO.  Also, in cases that don't require singleton bean definitions, the parent could package and provide common jars to children, e.g., we set the parent module's classloader as the parent classloader, eliminating the need to install common jars in an HDFS path (basically the approach I described https://jira.spring.io/browse/XD-2420)   MP : >> Also, in cases that don't require singleton bean definitions, the parent could package and provide common jars to children, e.g., we set the parent module's classloader as the parent classloader This approach does not offer some of the advantages offered by a central yaml config noted in the JIRA . But it is simpler in many ways. The developer just installs a parent module containing dependent jars and sets that as a parent in the child modules. This requires no additional infrastructure. To address the hadoop scenario we discussed, we would need another level of indirection so the parent of the hdfs modules is bound to the configured hadoop distro.   e.g. something like  parent = ${xd.hadoop.distro} in module properties  MF: Possibly the parent modules could go in the """"""""common"""""""" directory? They should be considered """"""""abstract"""""""" also - in the same sense as abstract bean definitions in a Spring context (and they should only be started on demand when needed by at least one concrete child module - then destroyed when the last child module is destroyed).  Maybe this would also allow us to wrap up those xml files that currently live in """"""""common"""""""" so that they are treated as parent modules?  MF: Yea, that sounds good wrt to common.  Also it might be a good idea to enable spring to throw an error if it finds more than one bean of the same name in the application context - i think that applies to searching in parent contexts as well.  this would avoid the 'last one wins' rule and give more deterministic behavior. """"""",,8.0
1,XD-2468,FEATURE,Done,MEDIUM,"""Kafka Profiling for Base & Distributed base benchmarks""","""Kafka Profiling for Base & Distributed base benchmarks""","""Kafka Profiling for Base & Distributed base benchmarks""",,5.0
1,XD-2467,FEATURE,Done,MEDIUM,"""Implement a Spark Streaming Driver application that can be controlled as an XD module instance""","""This should include lifecycle management, so that when the module's stream is undeployed, the Spark Streaming application should be stopped, etc.  Deploying a number of module instances should result in multiple receiver tasks, and those should bind to the bus using the consumer side partitioning metadata. ""","""""""This should include lifecycle management, so that when the module's stream is undeployed, the Spark Streaming application should be stopped, etc.  Deploying a number of module instances should result in multiple receiver tasks, and those should bind to the bus using the consumer side partitioning metadata. """"""",,8.0
1,XD-2466,FEATURE,Done,MEDIUM,"""Implement a dirt plugin for Spark Streaming support""","""Implement a dirt plugin for Spark Streaming support""","""Implement a dirt plugin for Spark Streaming support""",,5.0
1,XD-2462,FEATURE,Done,MEDIUM,"""Acceptance test for Kafka as a message bus""","""As a QA, I'd like to include acceptance test coverage for _Kafka_ as a message bus so that I can validate the functionality as part of every CI build.""","""""""As a QA, I'd like to include acceptance test coverage for _Kafka_ as a message bus so that I can validate the functionality as part of every CI build.""""""",,5.0
1,XD-2458,MAINTENANCE,Done,MEDIUM,"""Update Base AMI to be a HVM""","""Update Base AMI to be a HVM""","""Update Base AMI to be a HVM""",,3.0
1,XD-2456,FEATURE,Done,MEDIUM,"""Acceptance test for """"spark-app"""" batch job""","""As a QA, I'd like to include acceptance test coverage for _spark-app_ batch job so that I can validate the functionality as part of every CI build. ""","""""""As a QA, I'd like to include acceptance test coverage for _spark-app_ batch job so that I can validate the functionality as part of every CI build. """"""",,5.0
1,XD-2433,FEATURE,Done,MEDIUM,"""Implement a Spark Streaming Receiver that binds to the MessageBus""","""Implement a Spark Streaming Receiver that binds to the MessageBus""","""Implement a Spark Streaming Receiver that binds to the MessageBus""",,8.0
1,XD-2432,FEATURE,Done,MEDIUM,"""Define developer-facing interfaces for Spark Streaming modules""","""Define developer-facing interfaces for Spark Streaming modules""","""Define developer-facing interfaces for Spark Streaming modules""",,3.0
1,XD-2431,FEATURE,Done,MEDIUM,"""Workaround latest boot snapshot issue""","""* The workaround explicitly updates spring-core (latest boot needs it) * merges all application.yml documents that are not profile-specific under on spring: key (the latest boot requires it, at least for now. Boot may go back, see spring-projects/spring-boot#2022""","""""""* The workaround explicitly updates spring-core (latest boot needs it) * merges all application.yml documents that are not profile-specific under on spring: key (the latest boot requires it, at least for now. Boot may go back, see spring-projects/spring-boot#2022""""""",,3.0
1,XD-2430,FEATURE,Done,MEDIUM,"""Create a Sqoop job and required batch tasklet integration code""","""Based on the POC from XD-2124 we should create the actual implementation.  Things to consider to store in step context: - capture Log output/MapReduce job counters - capture last-value from incremental imports ""","""""""Based on the POC from XD-2124 we should create the actual implementation.  Things to consider to store in step context: - capture Log output/MapReduce job counters - capture last-value from incremental imports """"""",,8.0
1,XD-2429,BUG,Done,MEDIUM,"""Bind Producer Before Consumer""","""{quote}   Here is the full exception:   org.springframework.messaging.MessageDeliveryException: Dispatcher has no subscribers for channel 'ResourceConfiguredModule [name=filter, type=processor, group=request-rate, index=0 @58b0f318]:use-expression,default,admin,singlenode,hsqldbServer:9393.output'.; nested exception is org.springframework.integration.MessageDispatchingException: Dispatcher has no subscribers   and here is that stream:   topic:httpstartstop > filter --expression=payload.getHttpStartStop().getPeerType().name().equals('Client') | requestRateAggregator | appMetricsSplitter | router --expression='topic:app-request-rate-'+#jsonPath(payload,'$.appId') [2:59 PM] <USER> @MarkFisher  @IlayaperumalGopinathan @PatrickPeralta  This looks like another (not fixed by the previous fix) timing problem with taps when using singlenode. The tap is started before the tap stream is deployed. But it's not clear to me how the filter module could be deployed/bound as a consumer before the requestRateAggregator [3:08 PM] <USER> I see the problem: AbstractMessageBusBinderPlugin.bindConsumerAndProducers() binds the consumer before the producer - this is the wrong order for a passive component such as the filter. /cc @DavidTuranski {quote}""","""""""{quote}   Here is the full exception:   org.springframework.messaging.MessageDeliveryException: Dispatcher has no subscribers for channel 'ResourceConfiguredModule [name=filter, type=processor, group=request-rate, index=0 @58b0f318]:use-expression,default,admin,singlenode,hsqldbServer:9393.output'.; nested exception is org.springframework.integration.MessageDispatchingException: Dispatcher has no subscribers   and here is that stream:   topic:httpstartstop > filter --expression=payload.getHttpStartStop().getPeerType().name().equals('Client') | requestRateAggregator | appMetricsSplitter | router --expression='topic:app-request-rate-'+#jsonPath(payload,'$.appId') [2:59 PM] <USER> @MarkFisher  @IlayaperumalGopinathan @PatrickPeralta  This looks like another (not fixed by the previous fix) timing problem with taps when using singlenode. The tap is started before the tap stream is deployed. But it's not clear to me how the filter module could be deployed/bound as a consumer before the requestRateAggregator [3:08 PM] <USER> I see the problem: AbstractMessageBusBinderPlugin.bindConsumerAndProducers() binds the consumer before the producer - this is the wrong order for a passive component such as the filter. /cc @DavidTuranski {quote}""""""",,1.0
1,XD-2428,FEATURE,Done,MEDIUM,"""Mysql Libraries not shipped with XD by default""","""The reference states the following:  """"The JDBC driver jars for the HSQLDB, MySql, and Postgres are already on the XD classpath""""  It looks like this is true for Postgres and HSQLDB, but I can't see a driver for MySQL shipped with the distribution.""","""""""The reference states the following:  """"""""The JDBC driver jars for the HSQLDB, MySql, and Postgres are already on the XD classpath""""""""  It looks like this is true for Postgres and HSQLDB, but I can't see a driver for MySQL shipped with the distribution.""""""",,0.0
1,XD-2427,FEATURE,Done,MEDIUM,"""Use repo.spring.io as NPM repository""","""In order to improve the build reliability, we should be using the NPM repo provided by *repo.spring.io*   See *spring-xd-ui/README.md* for further details.""","""""""In order to improve the build reliability, we should be using the NPM repo provided by *repo.spring.io*   See *spring-xd-ui/README.md* for further details.""""""",,1.0
1,XD-2426,FEATURE,Done,MEDIUM,"""Travis CI improvements""","""Travis CI recently introduced docker based builds. This prevents root access (which we don't need), but allows caching (which we could not use before) and seems to come with beefier machine specs""","""""""Travis CI recently introduced docker based builds. This prevents root access (which we don't need), but allows caching (which we could not use before) and seems to come with beefier machine specs""""""",,2.0
1,XD-2425,BUG,Done,MEDIUM,"""SpringXD's syslog source does not fully support syslog RFC5424""","""SpringXD's syslog source cannot parse rfc5424 messages into a Map. For the messages we get in RFC 3164, springXD converts these to a Map.  Since the rfc5424 data cannot be interpreted then the map contains just one key called 'UNDECODED'. The result of this is that we get a string that looks like this (when we convert the message to a String) {code}  {UNDECODED=<182>Dec 02 2014 07:56:35: %ASA-6-113008: AAA transaction status ACCEPT : user = jbloggs} {code}  Should be something like this (note the values below are for illustrative purposes only and should not be used as test data)  {code}  {FACILITY=22, SEVERITY=6, TIMESTAMP=Tue Dec 02 07:56:35, HOST=the-hostname-that-sent-the-data, TAG=%ASA-6-113008, MESSAGE=........} {code}  h3. Root Cause Spring integration does not parse these messages. There is a JIRA for SI here: https://jira.spring.io/browse/INT-3450 ""","""""""SpringXD's syslog source cannot parse rfc5424 messages into a Map. For the messages we get in RFC 3164, springXD converts these to a Map.  Since the rfc5424 data cannot be interpreted then the map contains just one key called 'UNDECODED'. The result of this is that we get a string that looks like this (when we convert the message to a String)   Should be something like this (note the values below are for illustrative purposes only and should not be used as test data)    h3. Root Cause Spring integration does not parse these messages. There is a JIRA for SI here: https://jira.spring.io/browse/INT-3450 """"""","""  {UNDECODED=<182>Dec 02 2014 07:56:35: %ASA-6-113008: AAA transaction status ACCEPT : user = jbloggs}   {FACILITY=22, SEVERITY=6, TIMESTAMP=Tue Dec 02 07:56:35, HOST=the-hostname-that-sent-the-data, TAG=%ASA-6-113008, MESSAGE=........} """,5.0
1,XD-2424,FEATURE,Done,MEDIUM,"""Profile / Improve performance of TupleBuilder""","""See discussion at https://github.com/spring-projects/spring-xd/pull/1311  1) there seems to be unused SimpleDateFormat in TupleBuilder which hurst perf 2) More generally, should take some time to profile / micro-benchmark TupleBuilder""","""""""See discussion at https://github.com/spring-projects/spring-xd/pull/1311  1) there seems to be unused SimpleDateFormat in TupleBuilder which hurst perf 2) More generally, should take some time to profile / micro-benchmark TupleBuilder""""""",,5.0
1,XD-2423,BUG,Done,MEDIUM,"""WireTap is Applied to OutputChannel Before the Tap Channel has been Bound To The Bus""","""When establishing the tap, we create the tap channel and add the WireTap before the tap channel has been bound to the bus.  {quote} 17:00:23,918 ERROR task-scheduler-8 handler.LoggingHandler - org.springframework.messaging.MessageDeliveryException: Dispatcher has no subscribers for channel 'tap:stream:foo.time.0.tap.bridge'.; nested exception is org.springframework.integration.MessageDispatchingException: Dispatcher has no subscribers  at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:81)  at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:277)  at org.springframework.integration.channel.interceptor.WireTap.preSend(WireTap.java:129) {quote}""","""""""When establishing the tap, we create the tap channel and add the WireTap before the tap channel has been bound to the bus.  {quote} 17:00:23,918 ERROR task-scheduler-8 handler.LoggingHandler - org.springframework.messaging.MessageDeliveryException: Dispatcher has no subscribers for channel 'tap:stream:foo.time.0.tap.bridge'.; nested exception is org.springframework.integration.MessageDispatchingException: Dispatcher has no subscribers  at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:81)  at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:277)  at org.springframework.integration.channel.interceptor.WireTap.preSend(WireTap.java:129) {quote}""""""",,1.0
1,XD-2422,FEATURE,Done,MEDIUM,"""UI Provide fixed version numbers for NPM and Bower dependencies""","""UI Provide fixed version numbers for NPM and Bower dependencies""","""UI Provide fixed version numbers for NPM and Bower dependencies""",,1.0
1,XD-2421,BUG,Done,URGENT,"""UI: List of Streams causes """"undefined is not an option""""""","""See Screenshot.  The error is caused when loading all stream definitions in method *loadStreamDefinitions*.   Only 1 or two streams exist in the system.  ""","""""""See Screenshot.  The error is caused when loading all stream definitions in method *loadStreamDefinitions*.   Only 1 or two streams exist in the system.  """"""",,2.0
1,XD-2418,FEATURE,Done,MEDIUM,"""Kafka Sink: Support async Producer""","""The kafka sink supports properties for an async producer (e.g. {{queue.buffering.max.ms}} ) but you cannot enable such a producer (only {{sync}} ). Async producers batch messages (at the risk of message loss).  Add a new property {{async}} default {{false}} and add the corresponding attribute to the {{<int-kafka:producer-configuration/>}} element  {{async=""""$\{async\}""""}}""","""""""The kafka sink supports properties for an async producer (e.g. {{queue.buffering.max.ms}} ) but you cannot enable such a producer (only {{sync}} ). Async producers batch messages (at the risk of message loss).  Add a new property {{async}} default {{false}} and add the corresponding attribute to the {{<int-kafka:producer-configuration/>}} element  {{async=""""""""$\{async\}""""""""}}""""""",,1.0
1,XD-2416,BUG,Done,MEDIUM,"""SpelParseException is thrown when using empty string ("""""""") inside of an expression""","""I can only reproduce this when using single quotes around the expression:  {code} stream create test --definition """"http | transform --expression='payload.replace(\""""abc\"""", \""""\"""")' | log"""" --deploy true {code}  The following two alternatives work fine though: {code} # Using trim on a single space stream create test --definition """"http | transform --expression='payload.replace(\""""abc\"""", \"""" \"""".trim())' | log"""" --deploy true  # Not using single quotes or spaces in the expression stream create test --definition """"http | transform --expression=payload.replace(\""""abc\"""",\""""\"""") | log"""" --deploy true {code}""","""""""I can only reproduce this when using single quotes around the expression:    The following two alternatives work fine though: """"""",""" stream create test --definition """"""""http | transform --expression='payload.replace(\""""""""abc\"""""""", \""""""""\"""""""")' | log"""""""" --deploy true  # Using trim on a single space stream create test --definition """"""""http | transform --expression='payload.replace(\""""""""abc\"""""""", \"""""""" \"""""""".trim())' | log"""""""" --deploy true  # Not using single quotes or spaces in the expression stream create test --definition """"""""http | transform --expression=payload.replace(\""""""""abc\"""""""",\""""""""\"""""""") | log"""""""" --deploy true """,1.0
1,XD-2415,BUG,Done,HIGH,"""Using custom classes for module properties leads to ClassNotFoundException""","""Attached is module properties file. Both custom Java classes referenced in the properties are available in the JAR file under _SPRING_XD_HOME/xd/module/<the-module>/lib_ directory.  Following exception is thrown: {code}6:26:03,064 1.0.2.RELEASE ERROR http-nio-9393-exec-4 rest.RestControllerAdvice - Caught exception while handling a request java.lang.IllegalStateException: Can't find class used for type of option 'binding': com.emc.it.ds.rtd.springxd.binding.BindingStrategy  at org.springframework.xd.module.options.DefaultModuleOptionsMetadataResolver.makeSimpleModuleOptions(DefaultModuleOptionsMetadataResolver.java:137)  at org.springframework.xd.module.options.DefaultModuleOptionsMetadataResolver.resolveNormalMetadata(DefaultModuleOptionsMetadataResolver.java:193)  at org.springframework.xd.module.options.DefaultModuleOptionsMetadataResolver.resolve(DefaultModuleOptionsMetadataResolver.java:154)  at org.springframework.xd.module.options.DelegatingModuleOptionsMetadataResolver.resolve(DelegatingModuleOptionsMetadataResolver.java:44)  at org.springframework.xd.module.options.EnvironmentAwareModuleOptionsMetadataResolver.resolve(EnvironmentAwareModuleOptionsMetadataResolver.java:127)  at org.springframework.xd.dirt.stream.XDStreamParser.parse(XDStreamParser.java:173)  at org.springframework.xd.dirt.stream.AbstractDeployer.save(AbstractDeployer.java:95)  at org.springframework.xd.dirt.rest.XDController.save(XDController.java:223)  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)  at java.lang.reflect.Method.invoke(Method.java:606)  at org.springframework.web.method.support.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:215)  at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:132)  at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:104)  at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandleMethod(RequestMappingHandlerAdapter.java:749)  at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:689)  at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:83)  at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:938)  at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:870)  at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:961)  at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:863)  at javax.servlet.http.HttpServlet.service(HttpServlet.java:646)  at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:837)  at javax.servlet.http.HttpServlet.service(HttpServlet.java:727)  at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:303)  at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)  at org.springframework.boot.actuate.trace.WebRequestTraceFilter.doFilterInternal(WebRequestTraceFilter.java:110)  at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)  at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)  at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)  at org.springframework.boot.actuate.autoconfigure.EndpointWebMvcAutoConfiguration$ApplicationContextHeaderFilter.doFilterInternal(EndpointWebMvcAutoConfiguration.java:280)  at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)  at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)  at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)  at org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:186)  at org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:160)  at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)  at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)  at org.springframework.web.filter.HiddenHttpMethodFilter.doFilterInternal(HiddenHttpMethodFilter.java:77)  at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)  at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)  at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)  at org.springframework.web.filter.HttpPutFormContentFilter.doFilterInternal(HttpPutFormContentFilter.java:88)  at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)  at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)  at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)  at org.springframework.boot.actuate.autoconfigure.MetricFilterAutoConfiguration$MetricsFilter.doFilterInternal(MetricFilterAutoConfiguration.java:89)  at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)  at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)  at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)  at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:220)  at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:122)  at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:501)  at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:171)  at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:103)  at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:116)  at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:408)  at org.apache.coyote.http11.AbstractHttp11Processor.process(AbstractHttp11Processor.java:1070)  at org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process(AbstractProtocol.java:611)  at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1736)  at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.run(NioEndpoint.java:1695)  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)  at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)  at java.lang.Thread.run(Thread.java:745){code}  Please see attached patch file, this seems to be enough to resolve the problem. ""","""""""Attached is module properties file. Both custom Java classes referenced in the properties are available in the JAR file under _SPRING_XD_HOME/xd/module/<the-module>/lib_ directory.  Following exception is thrown:   Please see attached patch file, this seems to be enough to resolve the problem. """"""","""6:26:03,064 1.0.2.RELEASE ERROR http-nio-9393-exec-4 rest.RestControllerAdvice - Caught exception while handling a request java.lang.IllegalStateException: Can't find class used for type of option 'binding': com.emc.it.ds.rtd.springxd.binding.BindingStrategy  at org.springframework.xd.module.options.DefaultModuleOptionsMetadataResolver.makeSimpleModuleOptions(DefaultModuleOptionsMetadataResolver.java:137)  at org.springframework.xd.module.options.DefaultModuleOptionsMetadataResolver.resolveNormalMetadata(DefaultModuleOptionsMetadataResolver.java:193)  at org.springframework.xd.module.options.DefaultModuleOptionsMetadataResolver.resolve(DefaultModuleOptionsMetadataResolver.java:154)  at org.springframework.xd.module.options.DelegatingModuleOptionsMetadataResolver.resolve(DelegatingModuleOptionsMetadataResolver.java:44)  at org.springframework.xd.module.options.EnvironmentAwareModuleOptionsMetadataResolver.resolve(EnvironmentAwareModuleOptionsMetadataResolver.java:127)  at org.springframework.xd.dirt.stream.XDStreamParser.parse(XDStreamParser.java:173)  at org.springframework.xd.dirt.stream.AbstractDeployer.save(AbstractDeployer.java:95)  at org.springframework.xd.dirt.rest.XDController.save(XDController.java:223)  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)  at java.lang.reflect.Method.invoke(Method.java:606)  at org.springframework.web.method.support.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:215)  at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:132)  at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:104)  at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandleMethod(RequestMappingHandlerAdapter.java:749)  at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:689)  at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:83)  at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:938)  at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:870)  at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:961)  at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:863)  at javax.servlet.http.HttpServlet.service(HttpServlet.java:646)  at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:837)  at javax.servlet.http.HttpServlet.service(HttpServlet.java:727)  at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:303)  at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)  at org.springframework.boot.actuate.trace.WebRequestTraceFilter.doFilterInternal(WebRequestTraceFilter.java:110)  at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)  at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)  at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)  at org.springframework.boot.actuate.autoconfigure.EndpointWebMvcAutoConfiguration$ApplicationContextHeaderFilter.doFilterInternal(EndpointWebMvcAutoConfiguration.java:280)  at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)  at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)  at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)  at org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:186)  at org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:160)  at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)  at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)  at org.springframework.web.filter.HiddenHttpMethodFilter.doFilterInternal(HiddenHttpMethodFilter.java:77)  at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)  at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)  at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)  at org.springframework.web.filter.HttpPutFormContentFilter.doFilterInternal(HttpPutFormContentFilter.java:88)  at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)  at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)  at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)  at org.springframework.boot.actuate.autoconfigure.MetricFilterAutoConfiguration$MetricsFilter.doFilterInternal(MetricFilterAutoConfiguration.java:89)  at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)  at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)  at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)  at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:220)  at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:122)  at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:501)  at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:171)  at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:103)  at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:116)  at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:408)  at org.apache.coyote.http11.AbstractHttp11Processor.process(AbstractHttp11Processor.java:1070)  at org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process(AbstractProtocol.java:611)  at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1736)  at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.run(NioEndpoint.java:1695)  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)  at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)  at java.lang.Thread.run(Thread.java:745)""",1.0
1,XD-2414,BUG,Done,MEDIUM,"""Incorrect """"directory"""" option described in hdfs-dataset docs""","""Please see [hdfs-dataset 1.0.2.RELEASE docs|http://docs.spring.io/autorepo/docs/spring-xd/1.0.2.RELEASE/reference/html/#hdfs-dataset-avroparquet].  According to docs there should be """"directory"""" option in this sink but in code """"basePath"""" is used.""","""""""Please see [hdfs-dataset 1.0.2.RELEASE docs|http://docs.spring.io/autorepo/docs/spring-xd/1.0.2.RELEASE/reference/html/#hdfs-dataset-avroparquet].  According to docs there should be """"""""directory"""""""" option in this sink but in code """"""""basePath"""""""" is used.""""""",,1.0
1,XD-2413,IMPROVEMENT,Done,MEDIUM,"""Rollover support in hdfs-datasink""","""hdfs-datasink should support rollover option, just like hdfs sink.  This might be mutually exclusive with batchSize option which also performs rollover.""","""""""hdfs-datasink should support rollover option, just like hdfs sink.  This might be mutually exclusive with batchSize option which also performs rollover.""""""",,5.0
1,XD-2412,FEATURE,Done,MEDIUM,"""Fix Redis FieldValueCounter repo save() method""","""That method is actually currently never called, but : - The case where a mapping already exists is not covered (outstanding TODO comment) - the semantics of the method should just be to """"save and override""""  ""","""""""That method is actually currently never called, but : - The case where a mapping already exists is not covered (outstanding TODO comment) - the semantics of the method should just be to """"""""save and override""""""""  """"""",,2.0
1,XD-2411,FEATURE,Done,HIGH,"""Make Redis RichGauge repository """"cluster safe""""""","""The current implementation makes individual reads from redis and then writes back the average, so in a cluster environment the reads and writes are not serialized, client reads and writes for specific keys can interfere with each other.  Investigate options, such as use of redis transactions or use of lua scripting to solve this problem.""","""""""The current implementation makes individual reads from redis and then writes back the average, so in a cluster environment the reads and writes are not serialized, client reads and writes for specific keys can interfere with each other.  Investigate options, such as use of redis transactions or use of lua scripting to solve this problem.""""""",,5.0
1,XD-2410,FEATURE,Done,MEDIUM,"""Rename metrics repositories setValue(x, y, z) to something less """"javabean""""""","""Rename metrics repositories setValue(x, y, z) to something less """"javabean""""""","""Rename metrics repositories setValue(x, y, z) to something less """"javabean""""""",,1.0
1,XD-2409,BUG,Done,MEDIUM,"""hdfs-dataset sink with getName() method in Pojo""","""Having a pojo: {code} public class User{  private String name;  public String getName() {   return user;  }  public void setName(String name) {   this.name = name;  } } {code}  with: {code} hdfs-dataset --inputType='application/x-java-object;type=test.User' {code}  throws exception: {code} 12:43:27,698 1.1.0.SNAP ERROR task-scheduler-1 handler.LoggingHandler - org.springframework.messaging.MessageHandlingException: Expression evaluation failed: payload.getClass().getName(); nested exception is org.springframework.expression.AccessException: Problem invoking method: public java.lang.String test.User.getName() {code}  Which I believe is caused by `correlation-strategy-expression` spel in aggregator: {code}  <int:aggregator    input-channel=""""input""""    correlation-strategy-expression=""""payload.getClass().getName()""""    release-strategy-expression=""""size() == ${batchSize}""""    expire-groups-upon-completion=""""true""""    send-partial-result-on-expiry=""""true""""    message-store=""""messageStore""""    output-channel=""""objects""""/> {code}  Changing `getName()` method in pojo to something else works.""","""""""Having a pojo:   with:   throws exception:   Which I believe is caused by `correlation-strategy-expression` spel in aggregator:   Changing `getName()` method in pojo to something else works.""""""",""" public class User{  private String name;  public String getName() {   return user;  }  public void setName(String name) {   this.name = name;  } }  hdfs-dataset --inputType='application/x-java-object;type=test.User'  12:43:27,698 1.1.0.SNAP ERROR task-scheduler-1 handler.LoggingHandler - org.springframework.messaging.MessageHandlingException: Expression evaluation failed: payload.getClass().getName(); nested exception is org.springframework.expression.AccessException: Problem invoking method: public java.lang.String test.User.getName()   <int:aggregator    input-channel=""""""""input""""""""    correlation-strategy-expression=""""""""payload.getClass().getName()""""""""    release-strategy-expression=""""""""size() == ${batchSize}""""""""    expire-groups-upon-completion=""""""""true""""""""    send-partial-result-on-expiry=""""""""true""""""""    message-store=""""""""messageStore""""""""    output-channel=""""""""objects""""""""/> """,1.0
1,XD-2408,BUG,Done,HIGH,"""When a tap is re-deployed after undeploy, it doesn't work""","""When a tap on a stream is undeployed and re-deployed, it stops working. To make it work, the main stream associated with the tap needs to be undeployed and re-deployed. ""","""""""When a tap on a stream is undeployed and re-deployed, it stops working. To make it work, the main stream associated with the tap needs to be undeployed and re-deployed. """"""",,1.0
1,XD-2407,FEATURE,Done,MEDIUM,"""Enhance """"module upload"""" to support exploded dirs (on the shell side)""","""Would be nice to have the shell zip the contents of a directory if not already in zipped form. This way, the development cycle (if one decided to use upload) is quicker and edits can be done in place.""","""""""Would be nice to have the shell zip the contents of a directory if not already in zipped form. This way, the development cycle (if one decided to use upload) is quicker and edits can be done in place.""""""",,3.0
1,XD-2406,FEATURE,Done,MEDIUM,"""Create Sample Module projects""","""Create one or more Sample module projects in the Spring XD Examples repo to serve as templates for Spring XD module projects. Similar to https://github.com/<USER>siDslModule, these should include unit and single node integration tests, and demonstrate the use of Spring XD build and packaging tools, and other module development support. This may be split out into separate tasks, but should include a sample for source, processor, sink, and job, using @Configuration or XML configuration (either as separate samples or using build profiles). ""","""""""Create one or more Sample module projects in the Spring XD Examples repo to serve as templates for Spring XD module projects. Similar to https://github.com/<USER>siDslModule, these should include unit and single node integration tests, and demonstrate the use of Spring XD build and packaging tools, and other module development support. This may be split out into separate tasks, but should include a sample for source, processor, sink, and job, using @Configuration or XML configuration (either as separate samples or using build profiles). """"""",,3.0
1,XD-2404,FEATURE,Done,MEDIUM,"""Provide an XD Starter POM for module projects""","""Provide A maven pom to support module projects that will declare the Spring XD dependencies as provided, configure the boot plugin for 'MODULE' layout and other boilerplate build configuration. This should include a similar feature for gradle.""","""""""Provide A maven pom to support module projects that will declare the Spring XD dependencies as provided, configure the boot plugin for 'MODULE' layout and other boilerplate build configuration. This should include a similar feature for gradle.""""""",,3.0
1,XD-2403,FEATURE,Done,MEDIUM,"""Spike: Study elastic instances and scheduling in Bamboo to create Windows CI infrastructure""","""As a build manager, I'd like to setup CI infrastructure so that I can run integration tests in Windows OS automatically as we commit-trigger new builds.   *Scope:* * Use the environment where Bamboo is running * Gain access to powershell  * Setup services (redis, rabbit, etc.) * Kick-off CI task""","""""""As a build manager, I'd like to setup CI infrastructure so that I can run integration tests in Windows OS automatically as we commit-trigger new builds.   *Scope:* * Use the environment where Bamboo is running * Gain access to powershell  * Setup services (redis, rabbit, etc.) * Kick-off CI task""""""",,5.0
1,XD-2401,FEATURE,Done,MEDIUM,"""EC2 CI build improvements""","""As a developer, I'd like to include the following improvements as part of the EC2 CI infrastructure, so that we can reliably run the CI builds and also assert over feature functionalities.  *Scope:* * Enable 'distributed jvm test' * Change from using artifactory gradle task to a command task (that calls ./gradlew) * Test w/ embedded hadoop off * Turn on maxParallelForks ""","""""""As a developer, I'd like to include the following improvements as part of the EC2 CI infrastructure, so that we can reliably run the CI builds and also assert over feature functionalities.  *Scope:* * Enable 'distributed jvm test' * Change from using artifactory gradle task to a command task (that calls ./gradlew) * Test w/ embedded hadoop off * Turn on maxParallelForks """"""",,5.0
1,XD-2400,FEATURE,Done,MEDIUM,"""Research options to improve CI reliability""","""As a build master, I'd like to research CI options so that I can improve CI build stability and reliability.  *Potential Option:* [npm-cache|https://github.com/swarajban/npm-cache]  Caching previously installed dependencies, _npm-cache_ doesn't require downloading the internet each time we build.""","""""""As a build master, I'd like to research CI options so that I can improve CI build stability and reliability.  *Potential Option:* [npm-cache|https://github.com/swarajban/npm-cache]  Caching previously installed dependencies, _npm-cache_ doesn't require downloading the internet each time we build.""""""",,0.0
1,XD-2397,FEATURE,Done,HIGH,"""TCP-Client source module throws ClassNotFoundException""","""*Version:* XD: 1.1 M1  *Problem:* Trying to use tcp-client source module and observing an exception while deploying the stream.  *Stream Definition:* {code:xml}  curl --data name=dummy-firehose --data definition='tcp-client --decoder=LF --port=8080 | log' --data deploy=true http://localhost:9393/streams/definitions {""""name"""":""""dummy-firehose"""",""""status"""":null,""""definition"""":""""tcp-client --decoder=LF --port=8080 | log"""",""""_links"""":{""""self"""":{""""href"""":""""http://localhost:9393/streams/dummy-firehose""""}}} {code}  The same curl command works fine against XD 1.0.1 release. ""","""""""*Version:* XD: 1.1 M1  *Problem:* Trying to use tcp-client source module and observing an exception while deploying the stream.  *Stream Definition:*   The same curl command works fine against XD 1.0.1 release. """"""","""  curl --data name=dummy-firehose --data definition='tcp-client --decoder=LF --port=8080 | log' --data deploy=true http://localhost:9393/streams/definitions {""""""""name"""""""":""""""""dummy-firehose"""""""",""""""""status"""""""":null,""""""""definition"""""""":""""""""tcp-client --decoder=LF --port=8080 | log"""""""",""""""""_links"""""""":{""""""""self"""""""":{""""""""href"""""""":""""""""http://localhost:9393/streams/dummy-firehose""""""""}}} """,3.0
1,XD-2395,IMPROVEMENT,Done,MEDIUM,"""Need a way to specify a specific namenode for a given hdfs based job""","""A scenario where I have multiple jobs deployed to one singlenode or distributed instance of SpringXD that need to use different namenodes can easily exist.   The ability to specify a namenode, much the same way I can specify a directory would solve this problem.  The desired behavior would be to specify a namenode that wasn't set using 'hadoop config fs <namenode>' in the job description and have that value used instead of the value set at the SpringXD global level.""","""""""A scenario where I have multiple jobs deployed to one singlenode or distributed instance of SpringXD that need to use different namenodes can easily exist.   The ability to specify a namenode, much the same way I can specify a directory would solve this problem.  The desired behavior would be to specify a namenode that wasn't set using 'hadoop config fs <namenode>' in the job description and have that value used instead of the value set at the SpringXD global level.""""""",,3.0
1,XD-2392,MAINTENANCE,Done,MEDIUM,"""Add integration tests""","""Scope is to have integration test coverage for source and sink modules. ""","""""""Scope is to have integration test coverage for source and sink modules. """"""",,5.0
1,XD-2390,MAINTENANCE,Done,MEDIUM,"""Add regression test""","""Verify that network interruptions will not negatively affect the XD cluster.   Verify that a container that looses connectivity will be able to rejoin the cluster cleanly. Modules will redploy when the network is back up. ""","""""""Verify that network interruptions will not negatively affect the XD cluster.   Verify that a container that looses connectivity will be able to rejoin the cluster cleanly. Modules will redploy when the network is back up. """"""",,3.0
1,XD-2389,IMPROVEMENT,Done,MEDIUM,"""Streams section of doc should explicitly mention that labels are required for ambiguous modules""","""Currently I believe we only mention labels in this section of the doc: https://github.com/spring-projects/spring-xd/wiki/DSL-Reference#labels  And it is not even clear there that they are *required* when 2 or more module names would otherwise be ambiguous. It was probably written before we made that a mandatory part of the definition.  We should mention this somewhere in the 'streams' section of the manual. Even if none of the examples there currently have more than one occurrence of the same module, we should add one to illustrate this point. ""","""""""Currently I believe we only mention labels in this section of the doc: https://github.com/spring-projects/spring-xd/wiki/DSL-Reference#labels  And it is not even clear there that they are *required* when 2 or more module names would otherwise be ambiguous. It was probably written before we made that a mandatory part of the definition.  We should mention this somewhere in the 'streams' section of the manual. Even if none of the examples there currently have more than one occurrence of the same module, we should add one to illustrate this point. """"""",,2.0
1,XD-2388,FEATURE,Done,MEDIUM,"""Add support to host custom module in HDFS""","""As a user, I'd like to have the custom module (built as uber-jar) hosted in HDFS so that I can deploy the module to newly arriving containers. ""","""""""As a user, I'd like to have the custom module (built as uber-jar) hosted in HDFS so that I can deploy the module to newly arriving containers. """"""",,8.0
1,XD-2387,FEATURE,Done,MEDIUM,"""Acceptance test for Kafka source and sink""","""Acceptance test for Kafka source and sink""","""Acceptance test for Kafka source and sink""",,5.0
1,XD-2386,FEATURE,Done,MEDIUM,"""Need TCP-Client Source Acceptance test""","""Need TCP-Client Source Acceptance test""","""Need TCP-Client Source Acceptance test""",,3.0
1,XD-2384,FEATURE,Done,MEDIUM,"""Document custom module install procedures""","""As a user, I'd like to refer to documentation so that I can build the custom module based on recommended standards and patterns.""","""""""As a user, I'd like to refer to documentation so that I can build the custom module based on recommended standards and patterns.""""""",,3.0
1,XD-2383,MAINTENANCE,Done,MEDIUM,"""Add a Shell command to push custom module""","""As a user, I'd like to have a Shell command so that I can point to the custom-built _module_ archive and push it to the runtime for immediate usage. ""","""""""As a user, I'd like to have a Shell command so that I can point to the custom-built _module_ archive and push it to the runtime for immediate usage. """"""",,3.0
1,XD-2382,FEATURE,Done,MEDIUM,"""Re-run Kafka baseline tests in new infrastructure""","""As a developer, I'd like to setup a performance testing infrastructure (rackspace), so I can start benching Kafka baselines and continue with XD use-cases.""","""""""As a developer, I'd like to setup a performance testing infrastructure (rackspace), so I can start benching Kafka baselines and continue with XD use-cases.""""""",,8.0
1,XD-2381,FEATURE,Done,MEDIUM,"""Decouple messagebus dependencies""","""*Refactoring scope:* (_spring-xd-dirt_) * Message bus dependencies   The goal is to decouple them from startup phase to further enhance initialization time. ""","""""""*Refactoring scope:* (_spring-xd-dirt_) * Message bus dependencies   The goal is to decouple them from startup phase to further enhance initialization time. """"""",,8.0
1,XD-2378,FEATURE,Done,MEDIUM,"""Add ability to logout using the Admin UI""","""While there is a server endpoint to logout, we don't have that ability yet from the UI. As indicated by XD-2122 we will also need a meta-data REST endpoint  so we can interrogate whether security is enabled, whether the user is logged etc. So we can fulfill the requirements:   * Show a logout button only if a) security is enabled and b) user is logged in * Show the username and/or full name of the user being logged in  ""","""""""While there is a server endpoint to logout, we don't have that ability yet from the UI. As indicated by XD-2122 we will also need a meta-data REST endpoint  so we can interrogate whether security is enabled, whether the user is logged etc. So we can fulfill the requirements:   * Show a logout button only if a) security is enabled and b) user is logged in * Show the username and/or full name of the user being logged in  """"""",,5.0
1,XD-2377,BUG,Done,MEDIUM,"""Update """"About"""" section in UI with relevant release links""","""As a user, I'd like to have API and Documentation links in the [""""About""""|https://github.com/spring-projects/spring-xd/blob/master/spring-xd-ui/app/scripts/shared/views/about.html] section within _admin-ui_.   It would be ideal to have the version # dynamically replaced for every release. ""","""""""As a user, I'd like to have API and Documentation links in the [""""""""About""""""""|https://github.com/spring-projects/spring-xd/blob/master/spring-xd-ui/app/scripts/shared/views/about.html] section within _admin-ui_.   It would be ideal to have the version # dynamically replaced for every release. """"""",,1.0
1,XD-2376,FEATURE,Done,MEDIUM,"""Add batching support for Rabbit Message Bus""","""As a user, I'd like to have _microbatching_ capability so that I can ingest based on batch intervals for enhanced performance throughput.   *Example:* """"http --batchInterval=10 | log""""""","""""""As a user, I'd like to have _microbatching_ capability so that I can ingest based on batch intervals for enhanced performance throughput.   *Example:* """"""""http --batchInterval=10 | log""""""""""""""",,5.0
1,XD-2375,FEATURE,Done,MEDIUM,"""Research reactor-stream integration options""","""As a user, I'd like to have a _reactor-stream_ processor module so that I can ingest data using XD source modules and process them as time-window operations.   *Example 1:* http | reactor-stream --timeWindow=10s --field=payload.sensorData --expressions=min,avg  This would give you 10 second time window of the min and avg values.  *Example 2:* Reactor as a module  *Example 3:* Integration with Spark streaming and reactor""","""""""As a user, I'd like to have a _reactor-stream_ processor module so that I can ingest data using XD source modules and process them as time-window operations.   *Example 1:* http | reactor-stream --timeWindow=10s --field=payload.sensorData --expressions=min,avg  This would give you 10 second time window of the min and avg values.  *Example 2:* Reactor as a module  *Example 3:* Integration with Spark streaming and reactor""""""",,8.0
1,XD-2370,FEATURE,Done,MEDIUM,"""Remove Test Scripts From XD""","""The acceptance tests cover the entire suite of script tests.  Thus they are no longer needed.  The only test that was remaining was posting 10 messages to a http source and writing to a long and making sure we didn't get an error.  This test (httpbash) was never called from the scripts CI build.""","""""""The acceptance tests cover the entire suite of script tests.  Thus they are no longer needed.  The only test that was remaining was posting 10 messages to a http source and writing to a long and making sure we didn't get an error.  This test (httpbash) was never called from the scripts CI build.""""""",,1.0
1,XD-2368,FEATURE,Done,MEDIUM,"""Research Spark integration options (phase #2)""","""As a continuation, we would like to further investigate Spark, develop POC and identify the best appropriate design and implementation for XD.""","""""""As a continuation, we would like to further investigate Spark, develop POC and identify the best appropriate design and implementation for XD.""""""",,8.0
1,XD-2367,FEATURE,Done,MEDIUM,"""Investigate TypeConvertingStreamTests.testBasicTypeConversionWithTap test failure in CI builds""","""TypeConvertingStreamTests.testBasicTypeConversionWithTap() is failing intermittently. Why?""","""""""TypeConvertingStreamTests.testBasicTypeConversionWithTap() is failing intermittently. Why?""""""",,1.0
1,XD-2366,IMPROVEMENT,Done,MEDIUM,"""Doc generation accesses http://docbook.sourceforge.net""","""When generating docs, the build tries to access  http://docbook.sourceforge.net/release/images/draft.png  You will observe output like:  {code} Error with opening URL 'http://docbook.sourceforge.net/release/images/draft.png': docbook.sourceforge.net Background image not available: http://docbook.sourceforge.net/release/images/draft.png Background image not available: http://docbook.sourceforge.net/release/images/draft.png Background image not available: http://docbook.sourceforge.net/release/images/draft.png Background image not available: http://docbook.sourceforge.net/release/images/draft.png Background image not available: http://docbook.sourceforge.net/release/images/draft.png Background image not available: http://docbook.sourceforge.net/release/images/draft.png {code}""","""""""When generating docs, the build tries to access  http://docbook.sourceforge.net/release/images/draft.png  You will observe output like:  """"""",""" Error with opening URL 'http://docbook.sourceforge.net/release/images/draft.png': docbook.sourceforge.net Background image not available: http://docbook.sourceforge.net/release/images/draft.png Background image not available: http://docbook.sourceforge.net/release/images/draft.png Background image not available: http://docbook.sourceforge.net/release/images/draft.png Background image not available: http://docbook.sourceforge.net/release/images/draft.png Background image not available: http://docbook.sourceforge.net/release/images/draft.png Background image not available: http://docbook.sourceforge.net/release/images/draft.png """,2.0
1,XD-2364,FEATURE,Done,MEDIUM,"""Remove usage of <context:property-placeholder location=.../> in module defitions""","""This doesn't follow the conventions we have with other modules and it also means it isn't easy to override via environment variables etc.  This is in HDFS and some others.""","""""""This doesn't follow the conventions we have with other modules and it also means it isn't easy to override via environment variables etc.  This is in HDFS and some others.""""""",,5.0
1,XD-2363,BUG,Done,MEDIUM,"""The word """"that"""" is written in duplicate""","""The word """"that"""" is written in duplicate. See the attached PNG file.  ========================================================== Caveats Note that that inputType and outputType parameters only apply to payloads that require type conversion. For example, if a module produces an XML string with outputType=application/json, the payload will not be converted from XML to JSON. This is because the payload at the modules output channel is already a String so no conversion will be applied at runtime. ==========================================================  http://docs.spring.io/spring-xd/docs/1.0.1.RELEASE/reference/html/ ""","""""""The word """"""""that"""""""" is written in duplicate. See the attached PNG file.  ========================================================== Caveats Note that that inputType and outputType parameters only apply to payloads that require type conversion. For example, if a module produces an XML string with outputType=application/json, the payload will not be converted from XML to JSON. This is because the payload at the modules output channel is already a String so no conversion will be applied at runtime. ==========================================================  http://docs.spring.io/spring-xd/docs/1.0.1.RELEASE/reference/html/ """"""",,0.0
1,XD-2362,FEATURE,Done,MEDIUM,"""Created Acceptance CI test environment for 1.0.x""","""* Create the infrastructure (Mongo, Hadoop, ActiveMQ, Gemfire, Mysql, etc) in EC2 for the 1.0.2 acceptance tests * Retrofit the 1.0.2 to use the new infrastructure * Create a 1.0.2 branch for XD-EC2""","""""""* Create the infrastructure (Mongo, Hadoop, ActiveMQ, Gemfire, Mysql, etc) in EC2 for the 1.0.2 acceptance tests * Retrofit the 1.0.2 to use the new infrastructure * Create a 1.0.2 branch for XD-EC2""""""",,5.0
1,XD-2361,FEATURE,Done,MEDIUM,"""Pre-allocate partitions for Kafka message bus""","""As a user, I want Spring XDs message bus to be able to pre-allocate partitions between nodes when a stream is deployed, so that rebalancing doesnt happen when a container crashes and/or its redeployed.""","""""""As a user, I want Spring XDs message bus to be able to pre-allocate partitions between nodes when a stream is deployed, so that rebalancing doesnt happen when a container crashes and/or its redeployed.""""""",,8.0
1,XD-2360,FEATURE,Done,MEDIUM,"""Pre-allocate partitions for Kafka source""","""As a user, I want Spring XD to pre-allocate a set of partitions between the Kafka source modules when a stream is deployed, so that deployment is simpler, and rebalancing doesnt take place. ""","""""""As a user, I want Spring XD to pre-allocate a set of partitions between the Kafka source modules when a stream is deployed, so that deployment is simpler, and rebalancing doesnt take place. """"""",,8.0
1,XD-2359,MAINTENANCE,Done,MEDIUM,"""Add partition allocation support for Kafka source""","""As a user, I want to be able to control the partition allocation for the Kafka source modules when a stream is deployed, so that I can colocate with other data sources.""","""""""As a user, I want to be able to control the partition allocation for the Kafka source modules when a stream is deployed, so that I can colocate with other data sources.""""""",,8.0
1,XD-2358,FEATURE,Done,MEDIUM,"""Add starting offset support for Kafka source""","""As a user, I want to be able to control the starting offset of the Kafka source when a stream is deployed, so that I can replay a topic if necessary.  Note: - starting offset is only considered when the stream is deployed - progress made by modules must survive their crash for a running stream - undeploying and redeploying a stream with a specific start offset will cause the stream to read again from the start   TBD: what happens when streams are undeployed/redeployed - where do they resume from?""","""""""As a user, I want to be able to control the starting offset of the Kafka source when a stream is deployed, so that I can replay a topic if necessary.  Note: - starting offset is only considered when the stream is deployed - progress made by modules must survive their crash for a running stream - undeploying and redeploying a stream with a specific start offset will cause the stream to read again from the start   TBD: what happens when streams are undeployed/redeployed - where do they resume from?""""""",,8.0
1,XD-2354,FEATURE,Done,MEDIUM,"""EC2 Integration Tests fail after Boot 1.2 upgrade""","""Many of the tests fail with:  {code} java.lang.IllegalStateException: Cannot find template location: class path resource [templates/] (please add some templates, check your Groovy configuration, or set spring.groovy.template.check-template-location=false) {code}  Somehow we need to disable this check, using the property suggested.""","""""""Many of the tests fail with:    Somehow we need to disable this check, using the property suggested.""""""",""" java.lang.IllegalStateException: Cannot find template location: class path resource [templates/] (please add some templates, check your Groovy configuration, or set spring.groovy.template.check-template-location=false) """,1.0
1,XD-2353,BUG,Done,URGENT,"""Boot upgrade caused test failures""","""spring.groovy.template.check-template-location=false must now be set in the properties file. ""","""""""spring.groovy.template.check-template-location=false must now be set in the properties file. """"""",,3.0
1,XD-2352,FEATURE,Done,HIGH,"""Property replacement does not happen in XML class attribute""","""After updating to Boot 1.2 RC1 the following replacement doesn't work.    {code:xml}  <bean id=""""messageConverter"""" class=""""${converterClass}"""" /> {code}  which appears in the rabbit source and sink.  Feels like a core spring thing, but that was updated earlier.  Current workaround that was commited already so the build can pass is  {code:xml}  <bean id=""""clazz"""" class=""""java.lang.Class"""" factory-method=""""forName"""">    <constructor-arg value=""""${converterClass}""""/>  </bean>   <bean id=""""messageConverter"""" factory-bean=""""clazz"""" factory-method=""""newInstance""""/> {code}  ""","""""""After updating to Boot 1.2 RC1 the following replacement doesn't work.      which appears in the rabbit source and sink.  Feels like a core spring thing, but that was updated earlier.  Current workaround that was commited already so the build can pass is    """"""","""  <bean id=""""""""messageConverter"""""""" class=""""""""${converterClass}"""""""" />   <bean id=""""""""clazz"""""""" class=""""""""java.lang.Class"""""""" factory-method=""""""""forName"""""""">    <constructor-arg value=""""""""${converterClass}""""""""/>  </bean>   <bean id=""""""""messageConverter"""""""" factory-bean=""""""""clazz"""""""" factory-method=""""""""newInstance""""""""/> """,0.0
1,XD-2351,FEATURE,Done,MEDIUM,"""POM generation creates the correct dependency list""","""We are referencing Spring.IO deps when we shouldn't (since we moved to a different version of boot than in in the platform).""","""""""We are referencing Spring.IO deps when we shouldn't (since we moved to a different version of boot than in in the platform).""""""",,8.0
1,XD-2349,MAINTENANCE,Done,MEDIUM,"""Document Kafka source/sink""","""As a user, I'd like to refer to documentation in wiki so that I can setup and configure Kafka as a source or a sink as recommended. ""","""""""As a user, I'd like to refer to documentation in wiki so that I can setup and configure Kafka as a source or a sink as recommended. """"""",,1.0
1,XD-2348,MAINTENANCE,Done,MEDIUM,"""Document Spark job""","""As a user, I'd like to refer to documentation in wiki so that I can setup and configure Spark as a Batch job as recommended. ""","""""""As a user, I'd like to refer to documentation in wiki so that I can setup and configure Spark as a Batch job as recommended. """"""",,1.0
1,XD-2347,MAINTENANCE,Done,MEDIUM,"""Document Kafka message bus""","""As a user, I'd like to refer to documentation in wiki so that I can setup and configure Kafka as a message bus as recommended. ""","""""""As a user, I'd like to refer to documentation in wiki so that I can setup and configure Kafka as a message bus as recommended. """"""",,2.0
1,XD-2346,FEATURE,Done,MEDIUM,"""Document 1.1 major features""","""As a user, I'd like to refer to documentation in wiki so that I can configure the new sources, sinks and processor modules and as well as any new features. ""","""""""As a user, I'd like to refer to documentation in wiki so that I can configure the new sources, sinks and processor modules and as well as any new features. """"""",,0.0
1,XD-2345,BUG,Done,URGENT,"""XD UI not usable with IE 11""","""Trying to use the XD UI with Internet Explorer (version 11.0.9600.17031) is difficult. The screen doesn't refresh when streams/jobs are created or deployed. Had to erase the browsing history continuously to get state updates to show in the UI.""","""""""Trying to use the XD UI with Internet Explorer (version 11.0.9600.17031) is difficult. The screen doesn't refresh when streams/jobs are created or deployed. Had to erase the browsing history continuously to get state updates to show in the UI.""""""",,5.0
1,XD-2344,BUG,Done,URGENT,"""UI should quote parameters containing a space""","""Trying to deploy the `timestampfile` job using the UI.  Seems the UI doesn't quote string parameters that contains a space so the job creation fails.  Keeping all the defaults I get the following """"Resulting Definition"""" in the UI:  timestampfile --restartable=false --directory=/tmp/xd/output/ --fileExtension=txt --fileName=${xd.job.name} --format=yyyy-MM-dd HH:mm:ss --dateFormat=yyyy-MM-dd --makeUnique=true  (note: the --format parameter has a space)  which causes:  XD100E:(pos 128): Found unexpected data after stream definition: 'HH' timestampfile --restartable=false --directory=/tmp/xd/output/ --fileExtension=txt --fileName=${xd.job.name} --format=yyyy-MM-dd HH:mm:ss --dateFormat=yyyy-MM-dd --makeUnique=true *^ ""","""""""Trying to deploy the `timestampfile` job using the UI.  Seems the UI doesn't quote string parameters that contains a space so the job creation fails.  Keeping all the defaults I get the following """"""""Resulting Definition"""""""" in the UI:  timestampfile --restartable=false --directory=/tmp/xd/output/ --fileExtension=txt --fileName=${xd.job.name} --format=yyyy-MM-dd HH:mm:ss --dateFormat=yyyy-MM-dd --makeUnique=true  (note: the --format parameter has a space)  which causes:  XD100E:(pos 128): Found unexpected data after stream definition: 'HH' timestampfile --restartable=false --directory=/tmp/xd/output/ --fileExtension=txt --fileName=${xd.job.name} --format=yyyy-MM-dd HH:mm:ss --dateFormat=yyyy-MM-dd --makeUnique=true *^ """"""",,3.0
1,XD-2343,FEATURE,Done,MEDIUM,"""Allow """"module compose"""" to specify an explicit type""","""Currently, module composition always guesses the correct type because we don't have a module with a given name N that is both a source and a processor, or a processor and a sink (we only have the case source and sink, as in jdbc/jdbc or file/file).  If it were the case, then the heuristics for guessing the resulting type of a composition would break.  This issue is about adding the option for the user to explicitly specify the expected type of the composition, /if needed/.""","""""""Currently, module composition always guesses the correct type because we don't have a module with a given name N that is both a source and a processor, or a processor and a sink (we only have the case source and sink, as in jdbc/jdbc or file/file).  If it were the case, then the heuristics for guessing the resulting type of a composition would break.  This issue is about adding the option for the user to explicitly specify the expected type of the composition, /if needed/.""""""",,8.0
1,XD-2342,BUG,Done,MEDIUM,"""JDBCHDFS Job Password issue""","""Password for 'jdbchdfs' job definition is only hashing the initial portion of the password not the entire password (See attached image).  The password has an '_' char but it shouldn't matter. The entire password should be masked with '*' instead.""","""""""Password for 'jdbchdfs' job definition is only hashing the initial portion of the password not the entire password (See attached image).  The password has an '_' char but it shouldn't matter. The entire password should be masked with '*' instead.""""""",,3.0
1,XD-2341,FEATURE,Done,MEDIUM,"""Deleting a job and then re-adding a new definition with the same name fails""","""Using single-node deployment of Spring XD 1.0 GA, we needed to redefine several batch jobs. We deleted the jobs (""""job destroy all""""). When attempting to re-add, we received an error that a job with the name already exists. Performing """"job list"""" confirms the jobs were gone. To workaround, I needed to terminate the instance (server) of Spring XD and restart it. Since this was the single-node deployment without a live stream of data coming in this was okay, but would have been a major problem if bouncing the Spring XD server was not acceptable (i.e., live data being actively received).""","""""""Using single-node deployment of Spring XD 1.0 GA, we needed to redefine several batch jobs. We deleted the jobs (""""""""job destroy all""""""""). When attempting to re-add, we received an error that a job with the name already exists. Performing """"""""job list"""""""" confirms the jobs were gone. To workaround, I needed to terminate the instance (server) of Spring XD and restart it. Since this was the single-node deployment without a live stream of data coming in this was okay, but would have been a major problem if bouncing the Spring XD server was not acceptable (i.e., live data being actively received).""""""",,5.0
1,XD-2340,FEATURE,Done,MEDIUM,"""Ensure that branch-specific documentation is pulled and generated""","""Ensure that branch-specific documentation is pulled and generated""","""Ensure that branch-specific documentation is pulled and generated""",,3.0
1,XD-2339,FEATURE,Done,MEDIUM,"""Remove external config properties for modules""","""There are some modules that use external config properties (kafka producer/consumer, hadoop properties etc.,). We need to avoid using such properties and have them configured inside module so that module and its properties are self contained.  ""","""""""There are some modules that use external config properties (kafka producer/consumer, hadoop properties etc.,). We need to avoid using such properties and have them configured inside module so that module and its properties are self contained.  """"""",,5.0
1,XD-2338,FEATURE,Done,MEDIUM,"""Upgrade to Gradle 2.2""","""Looks like upgrade to Gradle 2.2 is not a simple version change, e.g. I see:   {code} FAILURE: Build failed with an exception.  * Where: Build file '/Users/<USER>dev/git/spring-xd/build.gradle' line: 219  * What went wrong: A problem occurred evaluating root project 'spring-xd'. > Could not find method forceDependencyVersions() for arguments [project ':documentation-toolchain'] on root project 'spring-xd'. {code}""","""""""Looks like upgrade to Gradle 2.2 is not a simple version change, e.g. I see:   """"""",""" FAILURE: Build failed with an exception.  * Where: Build file '/Users/hillert/dev/git/spring-xd/build.gradle' line: 219  * What went wrong: A problem occurred evaluating root project 'spring-xd'. > Could not find method forceDependencyVersions() for arguments [project ':documentation-toolchain'] on root project 'spring-xd'. """,3.0
1,XD-2335,FEATURE,Done,MEDIUM,"""Update Performance AMI to include Kafka""","""Create an AMI that will contain the Kafka Executable as well as the Kafka performance test tools.""","""""""Create an AMI that will contain the Kafka Executable as well as the Kafka performance test tools.""""""",,1.0
1,XD-2334,FEATURE,Done,MEDIUM,"""Create base perf test criteria""","""Since Kafka and Rabbit have different strategies on how a message system is implemented, we will need to update the tests used on rabbit to work with Kafka.  While they will not be exactly the same as before, they should exercise the same principles. This story covers:  * Create the consumer and producer execution configurations for kafka-producer-perf-test.sh and kafka-consumer-perf-test.sh.  * Record the tests a spreadsheet much like the Rabbit Base test spreadsheet  ""","""""""Since Kafka and Rabbit have different strategies on how a message system is implemented, we will need to update the tests used on rabbit to work with Kafka.  While they will not be exactly the same as before, they should exercise the same principles. This story covers:  * Create the consumer and producer execution configurations for kafka-producer-perf-test.sh and kafka-consumer-perf-test.sh.  * Record the tests a spreadsheet much like the Rabbit Base test spreadsheet  """"""",,2.0
1,XD-2333,FEATURE,Done,MEDIUM,"""Add test coverage for Kafka source and sink modules""","""As a PM, I'd like to have test coverage for both Kafka source and sink modules so that we can assert its functionality as part of the CI builds. ""","""""""As a PM, I'd like to have test coverage for both Kafka source and sink modules so that we can assert its functionality as part of the CI builds. """"""",,8.0
1,XD-2332,FEATURE,Done,MEDIUM,"""AdminUI - Provide Server-Side Cron Expression Validation""","""It is easy to get a cron expression wrong.   Provide validation of the cron expression on the Schedule Job page using async validation.   * Submit the cron expression to the server-side - and validate that the expression is valid. * Send a success message back (we may even send back some meta data  e.g. when is the next execution going to take place) ""","""""""It is easy to get a cron expression wrong.   Provide validation of the cron expression on the Schedule Job page using async validation.   * Submit the cron expression to the server-side - and validate that the expression is valid. * Send a success message back (we may even send back some meta data  e.g. when is the next execution going to take place) """"""",,5.0
1,XD-2331,BUG,Done,HIGH,"""Job deployment list returns 404 after Laptop wakes up""","""*Version:* XD 1.0.1 Mac OSX 10.9.5  *Problem:* - Deployed a simple batch job in 'singlenode' - Laptop put to sleep mode - After login: notice that ZK is establishing connection  - Continues to clean-up prior to redeployment, but never goes through successfully - Listing job both in UI and Shell states it is """"undeployed""""  *Gunnar's experiment:* - System is running in Single Node - Laptop goes to sleep - After waking up your laptop from sleep, you cannot retrieve the list of deployed jobs anymore (in AdminUI)  *Error:* Only getting back a *404* - """"NoSuchBatchJobException"""", """"Batch Job with the name abcd doesn't exist""""""","""""""*Version:* XD 1.0.1 Mac OSX 10.9.5  *Problem:* - Deployed a simple batch job in 'singlenode' - Laptop put to sleep mode - After login: notice that ZK is establishing connection  - Continues to clean-up prior to redeployment, but never goes through successfully - Listing job both in UI and Shell states it is """"""""undeployed""""""""  *Gunnar's experiment:* - System is running in Single Node - Laptop goes to sleep - After waking up your laptop from sleep, you cannot retrieve the list of deployed jobs anymore (in AdminUI)  *Error:* Only getting back a *404* - """"""""NoSuchBatchJobException"""""""", """"""""Batch Job with the name abcd doesn't exist""""""""""""""",,5.0
1,XD-2330,FEATURE,Done,URGENT,"""Zip created by Publish 1.1 only contains the shell.""","""XD, gemfire, directories in the zip file are missing.""","""""""XD, gemfire, directories in the zip file are missing.""""""",,1.0
1,XD-2329,FEATURE,Done,MEDIUM,"""Update Hadoop in CI machines""","""Placeholder to update to Apache 2.5.1 on CI machines.  *Next steps:* * It looks like the dependencies.properties still file has 2.2.0; update it to 2.5.1 * I guess we just bumped the spring-data-hadoop dependency; check for relevant other dependencies ""","""""""Placeholder to update to Apache 2.5.1 on CI machines.  *Next steps:* * It looks like the dependencies.properties still file has 2.2.0; update it to 2.5.1 * I guess we just bumped the spring-data-hadoop dependency; check for relevant other dependencies """"""",,0.0
1,XD-2328,MAINTENANCE,Done,MEDIUM,"""Create a 1.0.x 'Docs' branch ""","""Placeholder for Doc generation and supporting multiple doc branches.""","""""""Placeholder for Doc generation and supporting multiple doc branches.""""""",,0.0
1,XD-2327,MAINTENANCE,Done,MEDIUM,"""Update wiki page with release version""","""Placeholder to update [Wiki|https://github.com/spring-projects/spring-xd/wiki] with release version number.""","""""""Placeholder to update [Wiki|https://github.com/spring-projects/spring-xd/wiki] with release version number.""""""",,0.0
1,XD-2326,BUG,Done,URGENT,"""Can't create stream running on Windows""","""Trying to test on Windows and getting the following exception when createing a stream - 'stream create --name tictoc --definition """"time | log'  {code} 09:34:20,789 1.1.0.SNAP  INFO main util.XdConfigLoggingInitializer - XD Home: C:\Test\spring-xd-1.1.0.BUILD-SNAPSHOT\xd\bin\.. 09:34:20,790 1.1.0.SNAP  INFO main util.XdConfigLoggingInitializer - Transport: local 09:34:20,790 1.1.0.SNAP  INFO main util.XdConfigLoggingInitializer - XD config location: file:C:\Test\spring-xd-1.1.0.BUILD-SNAPSHOT\xd\bin\../config// 09:34:20,790 1.1.0.SNAP  INFO main util.XdConfigLoggingInitializer - XD config names: servers,application 09:34:20,793 1.1.0.SNAP  INFO main util.XdConfigLoggingInitializer - XD module config location: file:C:\Test\spring-xd-1.1.0.BUILD-SNAPSHOT\xd\bin\../config//mo dules/ 09:34:20,794 1.1.0.SNAP  INFO main util.XdConfigLoggingInitializer - XD module config name: modules 09:34:20,795 1.1.0.SNAP  INFO main util.XdConfigLoggingInitializer - Admin web UI: http://Seattle:9393/admin-ui 09:34:20,797 1.1.0.SNAP  INFO main util.XdConfigLoggingInitializer - Zookeeper at: localhost:64424 09:34:20,798 1.1.0.SNAP  INFO main util.XdConfigLoggingInitializer - Zookeeper namespace: xd 09:34:20,799 1.1.0.SNAP  INFO main util.XdConfigLoggingInitializer - Analytics: memory 09:34:20,913 1.1.0.SNAP  INFO LeaderSelector-0 server.DeploymentSupervisor - Leader Admin singlenode:default,admin,singlenode,hsqldbServer:9393 is watching for stream/job deployment requests. 09:34:21,013 1.1.0.SNAP  INFO DeploymentSupervisor-0 server.ContainerListener - Path cache event: type=INITIALIZED 09:34:21,070 1.1.0.SNAP  INFO main server.AdminServerApplication - Started AdminServerApplication in 6.364 seconds (JVM running for 18.031) 09:34:22,593 1.1.0.SNAP  INFO main server.ContainerRegistrar - Container {ip=192.168.0.120, host=Seattle, groups=, pid=1108, id=08c72e88-66d4-4b47-bd4a-8f5e5849 099f} joined cluster 09:34:22,594 1.1.0.SNAP  INFO main util.XdConfigLoggingInitializer - XD Home: C:\Test\spring-xd-1.1.0.BUILD-SNAPSHOT\xd\bin\.. 09:34:22,594 1.1.0.SNAP  INFO main util.XdConfigLoggingInitializer - Transport: local 09:34:22,595 1.1.0.SNAP  INFO main util.XdConfigLoggingInitializer - XD config location: file:C:\Test\spring-xd-1.1.0.BUILD-SNAPSHOT\xd\bin\../config// 09:34:22,596 1.1.0.SNAP  INFO main util.XdConfigLoggingInitializer - XD config names: servers,application 09:34:22,596 1.1.0.SNAP  INFO main util.XdConfigLoggingInitializer - XD module config location: file:C:\Test\spring-xd-1.1.0.BUILD-SNAPSHOT\xd\bin\../config//mo dules/ 09:34:22,596 1.1.0.SNAP  INFO main util.XdConfigLoggingInitializer - XD module config name: modules 09:34:22,596 1.1.0.SNAP  INFO main util.XdConfigLoggingInitializer - Container IP address: 192.168.0.120 09:34:22,596 1.1.0.SNAP  INFO main util.XdConfigLoggingInitializer - Container hostname:   Seattle 09:34:22,596 1.1.0.SNAP  INFO main util.XdConfigLoggingInitializer - Hadoop Distro: hadoop22 09:34:22,597 1.1.0.SNAP  INFO DeploymentSupervisor-0 server.ContainerListener - Path cache event: path=/containers/08c72e88-66d4-4b47-bd4a-8f5e5849099f, type=CH ILD_ADDED 09:34:22,600 1.1.0.SNAP  INFO DeploymentsPathChildrenCache-0 server.DeploymentListener - Path cache event: type=INITIALIZED 09:34:22,607 1.1.0.SNAP  INFO DeploymentSupervisor-0 server.ContainerListener - Container arrived: Container{name='08c72e88-66d4-4b47-bd4a-8f5e5849099f', attrib utes={ip=192.168.0.120, host=Seattle, groups=, pid=1108, id=08c72e88-66d4-4b47-bd4a-8f5e5849099f}} 09:34:22,609 1.1.0.SNAP  INFO DeploymentSupervisor-0 server.ContainerListener - Scheduling deployments to new container(s) in 15000 ms 09:34:22,611 1.1.0.SNAP  INFO main util.XdConfigLoggingInitializer - Hadoop version detected from classpath: 2.2.0 09:34:22,612 1.1.0.SNAP  INFO main util.XdConfigLoggingInitializer - Zookeeper at: localhost:64424 09:34:22,613 1.1.0.SNAP  INFO main util.XdConfigLoggingInitializer - Zookeeper namespace: xd 09:34:22,615 1.1.0.SNAP  INFO main util.XdConfigLoggingInitializer - Analytics: memory 09:34:22,616 1.1.0.SNAP  INFO main server.ContainerServerApplication - Started ContainerServerApplication in 0.61 seconds (JVM running for 19.576) 09:36:15,837 1.1.0.SNAP ERROR http-nio-9393-exec-3 rest.RestControllerAdvice - Caught exception while handling a request java.lang.StringIndexOutOfBoundsException: String index out of range: -1         at java.lang.String.substring(String.java:1954)         at org.springframework.xd.dirt.module.ArchiveModuleRegistry.fromResource(ArchiveModuleRegistry.java:140)         at org.springframework.xd.dirt.module.ArchiveModuleRegistry.findDefinition(ArchiveModuleRegistry.java:68)         at org.springframework.xd.dirt.module.DelegatingModuleRegistry.findDefinition(DelegatingModuleRegistry.java:48)         at org.springframework.xd.dirt.module.store.ZooKeeperModuleDefinitionRepository.findByNameAndType(ZooKeeperModuleDefinitionRepository.java:78)         at org.springframework.xd.dirt.stream.XDStreamParser.resolveModuleType(XDStreamParser.java:317)         at org.springframework.xd.dirt.stream.XDStreamParser.determineType(XDStreamParser.java:212)         at org.springframework.xd.dirt.stream.XDStreamParser.parse(XDStreamParser.java:168)         at org.springframework.xd.dirt.stream.AbstractDeployer.save(AbstractDeployer.java:96)         at org.springframework.xd.dirt.rest.XDController.save(XDController.java:223)         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)         at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)         at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)         at java.lang.reflect.Method.invoke(Method.java:483)         at org.springframework.web.method.support.InvocableHandalerMethod.invoke(InvocableHandlerMethod.java:215)         at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:132)         at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:104)         at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandleMethod(RequestMappingHandlerAdapter.java:781)         at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:721)         at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:83)         at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:943)         at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:877)         at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:966)         at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:868)         at javax.servlet.http.HttpServlet.service(HttpServlet.java:646)         at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:842)         at javax.servlet.http.HttpServlet.service(HttpServlet.java:727)         at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:303)         at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)         at org.springframework.boot.actuate.trace.WebRequestTraceFilter.doFilterInternal(WebRequestTraceFilter.java:110)         at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)         at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)         at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)         at org.springframework.boot.actuate.autoconfigure.EndpointWebMvcAutoConfiguration$ApplicationContextHeaderFilter.doFilterInternal(EndpointWebMvcAutoConf iguration.java:280)         at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)         at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)         at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)         at org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:186)         at org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:160)         at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)         at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)         at org.springframework.web.filter.HiddenHttpMethodFilter.doFilterInternal(HiddenHttpMethodFilter.java:77)         at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)         at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)         at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)         at org.springframework.web.filter.HttpPutFormContentFilter.doFilterInternal(HttpPutFormContentFilter.java:88)         at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)         at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)         at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)         at org.springframework.boot.actuate.autoconfigure.MetricFilterAutoConfiguration$MetricsFilter.doFilterInternal(MetricFilterAutoConfiguration.java:89)         at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)         at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)         at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)         at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:220)         at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:122)         at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:501)         at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:171)         at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:103)         at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:116)         at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:408)         at org.apache.coyote.http11.AbstractHttp11Processor.process(AbstractHttp11Processor.java:1070)         at org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process(AbstractProtocol.java:611)         at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1736)         at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.run(NioEndpoint.java:1695)         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)         at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)         at java.lang.Thread.run(Thread.java:745) {code}""","""""""Trying to test on Windows and getting the following exception when createing a stream - 'stream create --name tictoc --definition """"""""time | log'  """"""",""" 09:34:20,789 1.1.0.SNAP  INFO main util.XdConfigLoggingInitializer - XD Home: C:\Test\spring-xd-1.1.0.BUILD-SNAPSHOT\xd\bin\.. 09:34:20,790 1.1.0.SNAP  INFO main util.XdConfigLoggingInitializer - Transport: local 09:34:20,790 1.1.0.SNAP  INFO main util.XdConfigLoggingInitializer - XD config location: file:C:\Test\spring-xd-1.1.0.BUILD-SNAPSHOT\xd\bin\../config// 09:34:20,790 1.1.0.SNAP  INFO main util.XdConfigLoggingInitializer - XD config names: servers,application 09:34:20,793 1.1.0.SNAP  INFO main util.XdConfigLoggingInitializer - XD module config location: file:C:\Test\spring-xd-1.1.0.BUILD-SNAPSHOT\xd\bin\../config//mo dules/ 09:34:20,794 1.1.0.SNAP  INFO main util.XdConfigLoggingInitializer - XD module config name: modules 09:34:20,795 1.1.0.SNAP  INFO main util.XdConfigLoggingInitializer - Admin web UI: http://Seattle:9393/admin-ui 09:34:20,797 1.1.0.SNAP  INFO main util.XdConfigLoggingInitializer - Zookeeper at: localhost:64424 09:34:20,798 1.1.0.SNAP  INFO main util.XdConfigLoggingInitializer - Zookeeper namespace: xd 09:34:20,799 1.1.0.SNAP  INFO main util.XdConfigLoggingInitializer - Analytics: memory 09:34:20,913 1.1.0.SNAP  INFO LeaderSelector-0 server.DeploymentSupervisor - Leader Admin singlenode:default,admin,singlenode,hsqldbServer:9393 is watching for stream/job deployment requests. 09:34:21,013 1.1.0.SNAP  INFO DeploymentSupervisor-0 server.ContainerListener - Path cache event: type=INITIALIZED 09:34:21,070 1.1.0.SNAP  INFO main server.AdminServerApplication - Started AdminServerApplication in 6.364 seconds (JVM running for 18.031) 09:34:22,593 1.1.0.SNAP  INFO main server.ContainerRegistrar - Container {ip=192.168.0.120, host=Seattle, groups=, pid=1108, id=08c72e88-66d4-4b47-bd4a-8f5e5849 099f} joined cluster 09:34:22,594 1.1.0.SNAP  INFO main util.XdConfigLoggingInitializer - XD Home: C:\Test\spring-xd-1.1.0.BUILD-SNAPSHOT\xd\bin\.. 09:34:22,594 1.1.0.SNAP  INFO main util.XdConfigLoggingInitializer - Transport: local 09:34:22,595 1.1.0.SNAP  INFO main util.XdConfigLoggingInitializer - XD config location: file:C:\Test\spring-xd-1.1.0.BUILD-SNAPSHOT\xd\bin\../config// 09:34:22,596 1.1.0.SNAP  INFO main util.XdConfigLoggingInitializer - XD config names: servers,application 09:34:22,596 1.1.0.SNAP  INFO main util.XdConfigLoggingInitializer - XD module config location: file:C:\Test\spring-xd-1.1.0.BUILD-SNAPSHOT\xd\bin\../config//mo dules/ 09:34:22,596 1.1.0.SNAP  INFO main util.XdConfigLoggingInitializer - XD module config name: modules 09:34:22,596 1.1.0.SNAP  INFO main util.XdConfigLoggingInitializer - Container IP address: 192.168.0.120 09:34:22,596 1.1.0.SNAP  INFO main util.XdConfigLoggingInitializer - Container hostname:   Seattle 09:34:22,596 1.1.0.SNAP  INFO main util.XdConfigLoggingInitializer - Hadoop Distro: hadoop22 09:34:22,597 1.1.0.SNAP  INFO DeploymentSupervisor-0 server.ContainerListener - Path cache event: path=/containers/08c72e88-66d4-4b47-bd4a-8f5e5849099f, type=CH ILD_ADDED 09:34:22,600 1.1.0.SNAP  INFO DeploymentsPathChildrenCache-0 server.DeploymentListener - Path cache event: type=INITIALIZED 09:34:22,607 1.1.0.SNAP  INFO DeploymentSupervisor-0 server.ContainerListener - Container arrived: Container{name='08c72e88-66d4-4b47-bd4a-8f5e5849099f', attrib utes={ip=192.168.0.120, host=Seattle, groups=, pid=1108, id=08c72e88-66d4-4b47-bd4a-8f5e5849099f}} 09:34:22,609 1.1.0.SNAP  INFO DeploymentSupervisor-0 server.ContainerListener - Scheduling deployments to new container(s) in 15000 ms 09:34:22,611 1.1.0.SNAP  INFO main util.XdConfigLoggingInitializer - Hadoop version detected from classpath: 2.2.0 09:34:22,612 1.1.0.SNAP  INFO main util.XdConfigLoggingInitializer - Zookeeper at: localhost:64424 09:34:22,613 1.1.0.SNAP  INFO main util.XdConfigLoggingInitializer - Zookeeper namespace: xd 09:34:22,615 1.1.0.SNAP  INFO main util.XdConfigLoggingInitializer - Analytics: memory 09:34:22,616 1.1.0.SNAP  INFO main server.ContainerServerApplication - Started ContainerServerApplication in 0.61 seconds (JVM running for 19.576) 09:36:15,837 1.1.0.SNAP ERROR http-nio-9393-exec-3 rest.RestControllerAdvice - Caught exception while handling a request java.lang.StringIndexOutOfBoundsException: String index out of range: -1         at java.lang.String.substring(String.java:1954)         at org.springframework.xd.dirt.module.ArchiveModuleRegistry.fromResource(ArchiveModuleRegistry.java:140)         at org.springframework.xd.dirt.module.ArchiveModuleRegistry.findDefinition(ArchiveModuleRegistry.java:68)         at org.springframework.xd.dirt.module.DelegatingModuleRegistry.findDefinition(DelegatingModuleRegistry.java:48)         at org.springframework.xd.dirt.module.store.ZooKeeperModuleDefinitionRepository.findByNameAndType(ZooKeeperModuleDefinitionRepository.java:78)         at org.springframework.xd.dirt.stream.XDStreamParser.resolveModuleType(XDStreamParser.java:317)         at org.springframework.xd.dirt.stream.XDStreamParser.determineType(XDStreamParser.java:212)         at org.springframework.xd.dirt.stream.XDStreamParser.parse(XDStreamParser.java:168)         at org.springframework.xd.dirt.stream.AbstractDeployer.save(AbstractDeployer.java:96)         at org.springframework.xd.dirt.rest.XDController.save(XDController.java:223)         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)         at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)         at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)         at java.lang.reflect.Method.invoke(Method.java:483)         at org.springframework.web.method.support.InvocableHandalerMethod.invoke(InvocableHandlerMethod.java:215)         at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:132)         at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:104)         at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandleMethod(RequestMappingHandlerAdapter.java:781)         at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:721)         at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:83)         at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:943)         at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:877)         at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:966)         at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:868)         at javax.servlet.http.HttpServlet.service(HttpServlet.java:646)         at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:842)         at javax.servlet.http.HttpServlet.service(HttpServlet.java:727)         at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:303)         at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)         at org.springframework.boot.actuate.trace.WebRequestTraceFilter.doFilterInternal(WebRequestTraceFilter.java:110)         at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)         at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)         at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)         at org.springframework.boot.actuate.autoconfigure.EndpointWebMvcAutoConfiguration$ApplicationContextHeaderFilter.doFilterInternal(EndpointWebMvcAutoConf iguration.java:280)         at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)         at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)         at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)         at org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:186)         at org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:160)         at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)         at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)         at org.springframework.web.filter.HiddenHttpMethodFilter.doFilterInternal(HiddenHttpMethodFilter.java:77)         at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)         at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)         at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)         at org.springframework.web.filter.HttpPutFormContentFilter.doFilterInternal(HttpPutFormContentFilter.java:88)         at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)         at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)         at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)         at org.springframework.boot.actuate.autoconfigure.MetricFilterAutoConfiguration$MetricsFilter.doFilterInternal(MetricFilterAutoConfiguration.java:89)         at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)         at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)         at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)         at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:220)         at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:122)         at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:501)         at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:171)         at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:103)         at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:116)         at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:408)         at org.apache.coyote.http11.AbstractHttp11Processor.process(AbstractHttp11Processor.java:1070)         at org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process(AbstractProtocol.java:611)         at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1736)         at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.run(NioEndpoint.java:1695)         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)         at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)         at java.lang.Thread.run(Thread.java:745) """,3.0
1,XD-2325,FEATURE,Done,MEDIUM,"""Set 'auto-startup' to false in Kafka source""","""We have to explicitly set it to false, in order to avoid an early start of the poller and the associated DistpatcherHasNoSubscribersException.""","""""""We have to explicitly set it to false, in order to avoid an early start of the poller and the associated DistpatcherHasNoSubscribersException.""""""",,1.0
1,XD-2323,BUG,Done,HIGH,"""Filejdbc jobs status shows """"STARTED"""" even when job is complete""","""SHA: 67473dc71332c0727516b6f3fd11a55561b2472e Deployment: 1 Admin, 2 Containers JobStore: HSQLDB OS: Mac OSX & Ubuntu Reproducible: Yes Job: job create foo \-\-definition """"filejdbc \-\-resources=file:filejdbctest/filejdbctest.out \-\-names=data --tableName=filejdbctest \-\-initializeDatabase=true """"\-\-deploy  When using Rabbit as a transport with more than one container and launching the job above.  The Job execution stays as """"STARTED"""" status, even though the job is actually finished.   We expect it to reach a state of """"COMPLETED"""".  Using Redis as a transport the job execution status does reach """"COMPLETED"""".     The execution step list shows:  Id  Step Name                Job Exec ID  Start Time               End Time                 Status   --  -----------------------  -----------  -----------------------  -----------------------  ---------   8   step1-master             4            2014-11-06 15:28:29,820                           STARTED   9   step1-master:partition0  4            2014-11-06 15:28:29,854  2014-11-06 15:28:29,890  COMPLETED""","""""""SHA: 67473dc71332c0727516b6f3fd11a55561b2472e Deployment: 1 Admin, 2 Containers JobStore: HSQLDB OS: Mac OSX & Ubuntu Reproducible: Yes Job: job create foo \-\-definition """"""""filejdbc \-\-resources=file:filejdbctest/filejdbctest.out \-\-names=data --tableName=filejdbctest \-\-initializeDatabase=true """"""""\-\-deploy  When using Rabbit as a transport with more than one container and launching the job above.  The Job execution stays as """"""""STARTED"""""""" status, even though the job is actually finished.   We expect it to reach a state of """"""""COMPLETED"""""""".  Using Redis as a transport the job execution status does reach """"""""COMPLETED"""""""".     The execution step list shows:  Id  Step Name                Job Exec ID  Start Time               End Time                 Status   --  -----------------------  -----------  -----------------------  -----------------------  ---------   8   step1-master             4            2014-11-06 15:28:29,820                           STARTED   9   step1-master:partition0  4            2014-11-06 15:28:29,854  2014-11-06 15:28:29,890  COMPLETED""""""",,3.0
1,XD-2322,BUG,Done,MEDIUM,"""Enable configuration of replication factor on the Kafka message bus""","""The field exists and it is referred to in application.yml, but it does not have a setter and the bus will always use the configured default, which is 1.""","""""""The field exists and it is referred to in application.yml, but it does not have a setter and the bus will always use the configured default, which is 1.""""""",,3.0
1,XD-2321,FEATURE,Done,MEDIUM,"""UI: Create a dedicated scheduling page for Jobs""","""Create a dedicated Scheduling Page for Jobs. Currently we create a form underneath the deployments table. That is a bit unwieldy when many deployed jobs are shown in the table.  Similar to XD-2320""","""""""Create a dedicated Scheduling Page for Jobs. Currently we create a form underneath the deployments table. That is a bit unwieldy when many deployed jobs are shown in the table.  Similar to XD-2320""""""",,4.0
1,XD-2320,FEATURE,Done,MEDIUM,"""UI: Create a dedicated Launch Page for Jobs""","""Create a dedicated Launch Page for Jobs. Currently we create a launch form underneath the deployments table. That is a bit unwieldy when many deployed jobs are shown in the table.""","""""""Create a dedicated Launch Page for Jobs. Currently we create a launch form underneath the deployments table. That is a bit unwieldy when many deployed jobs are shown in the table.""""""",,4.0
1,XD-2319,FEATURE,Done,MEDIUM,"""Add spring-xd-python to the distribution""","""Add spring-xd-python to the distribution""","""Add spring-xd-python to the distribution""",,1.0
1,XD-2318,FEATURE,Done,MEDIUM,"""Create Boot Starters for Modules""","""Create various boot starter projects for module developers. This should include templates for source, processor, sink, and job and ideally different options for each. For example, a processor configured with XML, SI Java DSL, or SI Java DSL with lambdas.  ""","""""""Create various boot starter projects for module developers. This should include templates for source, processor, sink, and job and ideally different options for each. For example, a processor configured with XML, SI Java DSL, or SI Java DSL with lambdas.  """"""",,5.0
1,XD-2317,FEATURE,Done,HIGH,"""Add Greenplum Sink""","""User should have the option of Greenplum DB sink so they can write data directly to Greenplum DB via the pgfdist/gploader (Greenplum bulk loader).  The existing JDBC sinks are not suitable for high volume loads. The JDBC approach utilizes the master segment of Greenplum for loading datasets instead of the bulk loader utility.""","""""""User should have the option of Greenplum DB sink so they can write data directly to Greenplum DB via the pgfdist/gploader (Greenplum bulk loader).  The existing JDBC sinks are not suitable for high volume loads. The JDBC approach utilizes the master segment of Greenplum for loading datasets instead of the bulk loader utility.""""""",,3.0
1,XD-2316,BUG,Done,MEDIUM,"""REST: """"jobs/configurations"""" returns 404 if one job has error""","""There is a bug in the deployments rest end-point.   *How to reproduce:*   * Deploy a Batch job (success) that for example does not all necessary libraries in the class-patch and thus causes a java.lang.ClassNotFoundException  *Result:*  You cannot retrieve the list of deployments list anymore using:  * http://localhost:9393/jobs/configurations  The rest endpoint will now report:  [{""""links"""":[],""""logref"""":""""NoSuchBatchJobException"""",""""message"""":""""Batch Job with the name myJob doesn't exist""""}]  This message is not entirely wrongbut extremely misleading. I think we should still return the entire list and rather mark the job as having an error.  Also returning an 404 Not Found is misleading as well. ""","""""""There is a bug in the deployments rest end-point.   *How to reproduce:*   * Deploy a Batch job (success) that for example does not all necessary libraries in the class-patch and thus causes a java.lang.ClassNotFoundException  *Result:*  You cannot retrieve the list of deployments list anymore using:  * http://localhost:9393/jobs/configurations  The rest endpoint will now report:  [{""""""""links"""""""":[],""""""""logref"""""""":""""""""NoSuchBatchJobException"""""""",""""""""message"""""""":""""""""Batch Job with the name myJob doesn't exist""""""""}]  This message is not entirely wrongbut extremely misleading. I think we should still return the entire list and rather mark the job as having an error.  Also returning an 404 Not Found is misleading as well. """"""",,3.0
1,XD-2315,FEATURE,Done,MEDIUM,"""UI: Add support for stoppable notifications""","""* Update Angular Growl to v2 * Allowing for stoppable notifications (in case you want to see it for longer than 5 secs) ""","""""""* Update Angular Growl to v2 * Allowing for stoppable notifications (in case you want to see it for longer than 5 secs) """"""",,3.0
1,XD-2310,BUG,Done,MEDIUM,"""Parsing issues with kafka-bus.xml""","""Using Kafka as a transport option yields:  [2014-11-04 12:18:30.528] boot - 24061 ERROR [main] --- SpringApplication: Application startup failed org.springframework.beans.factory.parsing.BeanDefinitionParsingException: Configuration problem: Failed to import bean definitions from URL location [classpath*:/META-INF/spring-xd/transports/kafka-bus.xml] Offending resource: class path resource [META-INF/spring-xd/bus/message-bus.xml]; nested exception is org.springframework.beans.factory.xml.XmlBeanDefinitionStoreException: Line 9 in XML document from URL [jar:file:/Users/<USER>.gradle/caches/modules-2/files-2.1/org.springframework.xd/spring-xd-dirt/1.1.0.BUILD-SNAPSHOT/cf6a9a013dbde49d2925e2b5177d01a028379758/spring-xd-dirt-1.1.0.BUILD-SNAPSHOT.jar!/META-INF/spring-xd/transports/kafka-bus.xml] is invalid; nested exception is org.xml.sax.SAXParseException; lineNumber: 9; columnNumber: 26; Open quote is expected for attribute """"{1}"""" associated with an  element type  """"value"""".  at org.springframework.beans.factory.parsing.FailFastProblemReporter.error(FailFastProblemReporter.java:70)  at org.springframework.beans.factory.parsing.ReaderContext.error(ReaderContext.java:85)  at org.springframework.beans.factory.parsing.ReaderContext.error(ReaderContext.java:76)  at org.springframework.beans.factory.xml.DefaultBeanDefinitionDocumentReader.importBeanDefinitionResource(DefaultBeanDefinitionDocumentReader.java:248)  at org.springframework.beans.factory.xml.DefaultBeanDefinitionDocumentReader.parseDefaultElement(DefaultBeanDefinitionDocumentReader.java:199)  at org.springframework.beans.factory.xml.DefaultBeanDefinitionDocumentReader.parseBeanDefinitions(DefaultBeanDefinitionDocumentReader.java:184)  at org.springframework.beans.factory.xml.DefaultBeanDefinitionDocumentReader.doRegisterBeanDefinitions(DefaultBeanDefinitionDocumentReader.java:141)  at org.springframework.beans.factory.xml.DefaultBeanDefinitionDocumentReader.registerBeanDefinitions(DefaultBeanDefinitionDocumentReader.java:110)  at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.registerBeanDefinitions(XmlBeanDefinitionReader.java:508)  at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.doLoadBeanDefinitions(XmlBeanDefinitionReader.java:391)  at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.loadBeanDefinitions(XmlBeanDefinitionReader.java:335)  at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.loadBeanDefinitions(XmlBeanDefinitionReader.java:303)  at org.springframework.beans.factory.support.AbstractBeanDefinitionReader.loadBeanDefinitions(AbstractBeanDefinitionReader.java:180)  at org.springframework.beans.factory.support.AbstractBeanDefinitionReader.loadBeanDefinitions(AbstractBeanDefinitionReader.java:216)  at org.springframework.beans.factory.support.AbstractBeanDefinitionReader.loadBeanDefinitions(AbstractBeanDefinitionReader.java:187)  at org.springframework.context.annotation.ConfigurationClassBeanDefinitionReader.loadBeanDefinitionsFromImportedResources(ConfigurationClassBeanDefinitionReader.java:313)  at org.springframework.context.annotation.ConfigurationClassBeanDefinitionReader.loadBeanDefinitionsForConfigurationClass(ConfigurationClassBeanDefinitionReader.java:138)  at org.springframework.context.annotation.ConfigurationClassBeanDefinitionReader.loadBeanDefinitions(ConfigurationClassBeanDefinitionReader.java:116)  at org.springframework.context.annotation.ConfigurationClassPostProcessor.processConfigBeanDefinitions(ConfigurationClassPostProcessor.java:330)  at org.springframework.context.annotation.ConfigurationClassPostProcessor.postProcessBeanDefinitionRegistry(ConfigurationClassPostProcessor.java:243)  at org.springframework.context.support.PostProcessorRegistrationDelegate.invokeBeanDefinitionRegistryPostProcessors(PostProcessorRegistrationDelegate.java:254)  at org.springframework.context.support.PostProcessorRegistrationDelegate.invokeBeanFactoryPostProcessors(PostProcessorRegistrationDelegate.java:94)  at org.springframework.context.support.AbstractApplicationContext.invokeBeanFactoryPostProcessors(AbstractApplicationContext.java:609)  at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:464)  at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:691)  at org.springframework.boot.SpringApplication.run(SpringApplication.java:320)  at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:142)  at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:129)  at org.springframework.xd.dirt.server.SingleNodeApplication.run(SingleNodeApplication.java:63)  at org.springframework.xd.demo.kafka.KafkaDemo.main(KafkaDemo.java:28)  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)  at java.lang.reflect.Method.invoke(Method.java:606)  at com.intellij.rt.execution.application.AppMain.main(AppMain.java:134) Caused by: org.springframework.beans.factory.xml.XmlBeanDefinitionStoreException: Line 9 in XML document from URL [jar:file:/Users/<USER>.gradle/caches/modules-2/files-2.1/org.springframework.xd/spring-xd-dirt/1.1.0.BUILD-SNAPSHOT/cf6a9a013dbde49d2925e2b5177d01a028379758/spring-xd-dirt-1.1.0.BUILD-SNAPSHOT.jar!/META-INF/spring-xd/transports/kafka-bus.xml] is invalid; nested exception is org.xml.sax.SAXParseException; lineNumber: 9; columnNumber: 26; Open quote is expected for attribute """"{1}"""" associated with an  element type  """"value"""".  at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.doLoadBeanDefinitions(XmlBeanDefinitionReader.java:398)  at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.loadBeanDefinitions(XmlBeanDefinitionReader.java:335)  at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.loadBeanDefinitions(XmlBeanDefinitionReader.java:303)  at org.springframework.beans.factory.support.AbstractBeanDefinitionReader.loadBeanDefinitions(AbstractBeanDefinitionReader.java:180)  at org.springframework.beans.factory.support.AbstractBeanDefinitionReader.loadBeanDefinitions(AbstractBeanDefinitionReader.java:216)  at org.springframework.beans.factory.xml.DefaultBeanDefinitionDocumentReader.importBeanDefinitionResource(DefaultBeanDefinitionDocumentReader.java:242)  ... 31 more Caused by: org.xml.sax.SAXParseException; lineNumber: 9; columnNumber: 26; Open quote is expected for attribute """"{1}"""" associated with an  element type  """"value"""".  at com.sun.org.apache.xerces.internal.util.ErrorHandlerWrapper.createSAXParseException(ErrorHandlerWrapper.java:198)  at com.sun.org.apache.xerces.internal.util.ErrorHandlerWrapper.fatalError(ErrorHandlerWrapper.java:177)  at com.sun.org.apache.xerces.internal.impl.XMLErrorReporter.reportError(XMLErrorReporter.java:441)  at com.sun.org.apache.xerces.internal.impl.XMLErrorReporter.reportError(XMLErrorReporter.java:368)  at com.sun.org.apache.xerces.internal.impl.XMLScanner.reportFatalError(XMLScanner.java:1436)  at com.sun.org.apache.xerces.internal.impl.XMLScanner.scanAttributeValue(XMLScanner.java:829)  at com.sun.org.apache.xerces.internal.impl.XMLNSDocumentScannerImpl.scanAttribute(XMLNSDocumentScannerImpl.java:439)  at com.sun.org.apache.xerces.internal.impl.XMLNSDocumentScannerImpl.scanStartElement(XMLNSDocumentScannerImpl.java:255)  at com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl$FragmentContentDriver.next(XMLDocumentFragmentScannerImpl.java:2786)  at com.sun.org.apache.xerces.internal.impl.XMLDocumentScannerImpl.next(XMLDocumentScannerImpl.java:606)  at com.sun.org.apache.xerces.internal.impl.XMLNSDocumentScannerImpl.next(XMLNSDocumentScannerImpl.java:117)  at com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl.scanDocument(XMLDocumentFragmentScannerImpl.java:510)  at com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:848)  at com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:777)  at com.sun.org.apache.xerces.internal.parsers.XMLParser.parse(XMLParser.java:141)  at com.sun.org.apache.xerces.internal.parsers.DOMParser.parse(DOMParser.java:243)  at com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderImpl.parse(DocumentBuilderImpl.java:347)  at org.springframework.beans.factory.xml.DefaultDocumentLoader.loadDocument(DefaultDocumentLoader.java:76)  at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.doLoadDocument(XmlBeanDefinitionReader.java:428)  at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.doLoadBeanDefinitions(XmlBeanDefinitionReader.java:390)  ... 36 more""","""""""Using Kafka as a transport option yields:  [2014-11-04 12:18:30.528] boot - 24061 ERROR [main] --- SpringApplication: Application startup failed org.springframework.beans.factory.parsing.BeanDefinitionParsingException: Configuration problem: Failed to import bean definitions from URL location [classpath*:/META-INF/spring-xd/transports/kafka-bus.xml] Offending resource: class path resource [META-INF/spring-xd/bus/message-bus.xml]; nested exception is org.springframework.beans.factory.xml.XmlBeanDefinitionStoreException: Line 9 in XML document from URL [jar:file:/Users/<USER>.gradle/caches/modules-2/files-2.1/org.springframework.xd/spring-xd-dirt/1.1.0.BUILD-SNAPSHOT/cf6a9a013dbde49d2925e2b5177d01a028379758/spring-xd-dirt-1.1.0.BUILD-SNAPSHOT.jar!/META-INF/spring-xd/transports/kafka-bus.xml] is invalid; nested exception is org.xml.sax.SAXParseException; lineNumber: 9; columnNumber: 26; Open quote is expected for attribute """"""""{1}"""""""" associated with an  element type  """"""""value"""""""".  at org.springframework.beans.factory.parsing.FailFastProblemReporter.error(FailFastProblemReporter.java:70)  at org.springframework.beans.factory.parsing.ReaderContext.error(ReaderContext.java:85)  at org.springframework.beans.factory.parsing.ReaderContext.error(ReaderContext.java:76)  at org.springframework.beans.factory.xml.DefaultBeanDefinitionDocumentReader.importBeanDefinitionResource(DefaultBeanDefinitionDocumentReader.java:248)  at org.springframework.beans.factory.xml.DefaultBeanDefinitionDocumentReader.parseDefaultElement(DefaultBeanDefinitionDocumentReader.java:199)  at org.springframework.beans.factory.xml.DefaultBeanDefinitionDocumentReader.parseBeanDefinitions(DefaultBeanDefinitionDocumentReader.java:184)  at org.springframework.beans.factory.xml.DefaultBeanDefinitionDocumentReader.doRegisterBeanDefinitions(DefaultBeanDefinitionDocumentReader.java:141)  at org.springframework.beans.factory.xml.DefaultBeanDefinitionDocumentReader.registerBeanDefinitions(DefaultBeanDefinitionDocumentReader.java:110)  at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.registerBeanDefinitions(XmlBeanDefinitionReader.java:508)  at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.doLoadBeanDefinitions(XmlBeanDefinitionReader.java:391)  at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.loadBeanDefinitions(XmlBeanDefinitionReader.java:335)  at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.loadBeanDefinitions(XmlBeanDefinitionReader.java:303)  at org.springframework.beans.factory.support.AbstractBeanDefinitionReader.loadBeanDefinitions(AbstractBeanDefinitionReader.java:180)  at org.springframework.beans.factory.support.AbstractBeanDefinitionReader.loadBeanDefinitions(AbstractBeanDefinitionReader.java:216)  at org.springframework.beans.factory.support.AbstractBeanDefinitionReader.loadBeanDefinitions(AbstractBeanDefinitionReader.java:187)  at org.springframework.context.annotation.ConfigurationClassBeanDefinitionReader.loadBeanDefinitionsFromImportedResources(ConfigurationClassBeanDefinitionReader.java:313)  at org.springframework.context.annotation.ConfigurationClassBeanDefinitionReader.loadBeanDefinitionsForConfigurationClass(ConfigurationClassBeanDefinitionReader.java:138)  at org.springframework.context.annotation.ConfigurationClassBeanDefinitionReader.loadBeanDefinitions(ConfigurationClassBeanDefinitionReader.java:116)  at org.springframework.context.annotation.ConfigurationClassPostProcessor.processConfigBeanDefinitions(ConfigurationClassPostProcessor.java:330)  at org.springframework.context.annotation.ConfigurationClassPostProcessor.postProcessBeanDefinitionRegistry(ConfigurationClassPostProcessor.java:243)  at org.springframework.context.support.PostProcessorRegistrationDelegate.invokeBeanDefinitionRegistryPostProcessors(PostProcessorRegistrationDelegate.java:254)  at org.springframework.context.support.PostProcessorRegistrationDelegate.invokeBeanFactoryPostProcessors(PostProcessorRegistrationDelegate.java:94)  at org.springframework.context.support.AbstractApplicationContext.invokeBeanFactoryPostProcessors(AbstractApplicationContext.java:609)  at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:464)  at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:691)  at org.springframework.boot.SpringApplication.run(SpringApplication.java:320)  at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:142)  at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:129)  at org.springframework.xd.dirt.server.SingleNodeApplication.run(SingleNodeApplication.java:63)  at org.springframework.xd.demo.kafka.KafkaDemo.main(KafkaDemo.java:28)  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)  at java.lang.reflect.Method.invoke(Method.java:606)  at com.intellij.rt.execution.application.AppMain.main(AppMain.java:134) Caused by: org.springframework.beans.factory.xml.XmlBeanDefinitionStoreException: Line 9 in XML document from URL [jar:file:/Users/<USER>.gradle/caches/modules-2/files-2.1/org.springframework.xd/spring-xd-dirt/1.1.0.BUILD-SNAPSHOT/cf6a9a013dbde49d2925e2b5177d01a028379758/spring-xd-dirt-1.1.0.BUILD-SNAPSHOT.jar!/META-INF/spring-xd/transports/kafka-bus.xml] is invalid; nested exception is org.xml.sax.SAXParseException; lineNumber: 9; columnNumber: 26; Open quote is expected for attribute """"""""{1}"""""""" associated with an  element type  """"""""value"""""""".  at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.doLoadBeanDefinitions(XmlBeanDefinitionReader.java:398)  at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.loadBeanDefinitions(XmlBeanDefinitionReader.java:335)  at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.loadBeanDefinitions(XmlBeanDefinitionReader.java:303)  at org.springframework.beans.factory.support.AbstractBeanDefinitionReader.loadBeanDefinitions(AbstractBeanDefinitionReader.java:180)  at org.springframework.beans.factory.support.AbstractBeanDefinitionReader.loadBeanDefinitions(AbstractBeanDefinitionReader.java:216)  at org.springframework.beans.factory.xml.DefaultBeanDefinitionDocumentReader.importBeanDefinitionResource(DefaultBeanDefinitionDocumentReader.java:242)  ... 31 more Caused by: org.xml.sax.SAXParseException; lineNumber: 9; columnNumber: 26; Open quote is expected for attribute """"""""{1}"""""""" associated with an  element type  """"""""value"""""""".  at com.sun.org.apache.xerces.internal.util.ErrorHandlerWrapper.createSAXParseException(ErrorHandlerWrapper.java:198)  at com.sun.org.apache.xerces.internal.util.ErrorHandlerWrapper.fatalError(ErrorHandlerWrapper.java:177)  at com.sun.org.apache.xerces.internal.impl.XMLErrorReporter.reportError(XMLErrorReporter.java:441)  at com.sun.org.apache.xerces.internal.impl.XMLErrorReporter.reportError(XMLErrorReporter.java:368)  at com.sun.org.apache.xerces.internal.impl.XMLScanner.reportFatalError(XMLScanner.java:1436)  at com.sun.org.apache.xerces.internal.impl.XMLScanner.scanAttributeValue(XMLScanner.java:829)  at com.sun.org.apache.xerces.internal.impl.XMLNSDocumentScannerImpl.scanAttribute(XMLNSDocumentScannerImpl.java:439)  at com.sun.org.apache.xerces.internal.impl.XMLNSDocumentScannerImpl.scanStartElement(XMLNSDocumentScannerImpl.java:255)  at com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl$FragmentContentDriver.next(XMLDocumentFragmentScannerImpl.java:2786)  at com.sun.org.apache.xerces.internal.impl.XMLDocumentScannerImpl.next(XMLDocumentScannerImpl.java:606)  at com.sun.org.apache.xerces.internal.impl.XMLNSDocumentScannerImpl.next(XMLNSDocumentScannerImpl.java:117)  at com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl.scanDocument(XMLDocumentFragmentScannerImpl.java:510)  at com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:848)  at com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:777)  at com.sun.org.apache.xerces.internal.parsers.XMLParser.parse(XMLParser.java:141)  at com.sun.org.apache.xerces.internal.parsers.DOMParser.parse(DOMParser.java:243)  at com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderImpl.parse(DocumentBuilderImpl.java:347)  at org.springframework.beans.factory.xml.DefaultDocumentLoader.loadDocument(DefaultDocumentLoader.java:76)  at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.doLoadDocument(XmlBeanDefinitionReader.java:428)  at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.doLoadBeanDefinitions(XmlBeanDefinitionReader.java:390)  ... 36 more""""""",,1.0
1,XD-2309,IMPROVEMENT,Done,HIGH,"""Incremental data import with jdbchdfs job""","""Enhance the current jdbchdfs job in spring-xd to have an incremental load / delta load feature similar to sqoop. See sqoop documentation [here|http://sqoop.apache.org/docs/1.4.5/SqoopUserGuide.html#_incremental_imports]. The job will need to maintain some state between executions in order to decide the start point for the next data load.   The jdbchdfs job definition could take the following 2 new options.   h5. checkColumn  optional Specifies the column to be examined when determining which rows to import. (the column should not be of type CHAR/NCHAR/VARCHAR/VARNCHAR/ LONGVARCHAR/LONGNVARCHAR). Column should be numeric or timestamp. h5. lastValue  optional If specified this will override any data saved from previous job runs. If not specified will take the saved max-value from the last job run. If no last job run data is available then it will not be an incremental load and all the data which satisfies the query will be used.  Sqoop provides 2 modes of operation for incremental load, 'append' and 'lastModified'. For jdbchdfs the job will always append as it is writing to a hdfs file.  Example: To import data from the database table some_table which has a last update column called lastUpdated, you could use. {code} xd:> job create myjob --definition """"jdbchdfs --sql='select col1,col2,col3 from some_table' --checkColumn=lastUpdated"""" --deploy {code}  The batch job should also be capable of being partitioned to run in parallel across multiple containers""","""""""Enhance the current jdbchdfs job in spring-xd to have an incremental load / delta load feature similar to sqoop. See sqoop documentation [here|http://sqoop.apache.org/docs/1.4.5/SqoopUserGuide.html#_incremental_imports]. The job will need to maintain some state between executions in order to decide the start point for the next data load.   The jdbchdfs job definition could take the following 2 new options.   h5. checkColumn  optional Specifies the column to be examined when determining which rows to import. (the column should not be of type CHAR/NCHAR/VARCHAR/VARNCHAR/ LONGVARCHAR/LONGNVARCHAR). Column should be numeric or timestamp. h5. lastValue  optional If specified this will override any data saved from previous job runs. If not specified will take the saved max-value from the last job run. If no last job run data is available then it will not be an incremental load and all the data which satisfies the query will be used.  Sqoop provides 2 modes of operation for incremental load, 'append' and 'lastModified'. For jdbchdfs the job will always append as it is writing to a hdfs file.  Example: To import data from the database table some_table which has a last update column called lastUpdated, you could use.   The batch job should also be capable of being partitioned to run in parallel across multiple containers""""""",""" xd:> job create myjob --definition """"""""jdbchdfs --sql='select col1,col2,col3 from some_table' --checkColumn=lastUpdated"""""""" --deploy """,5.0
1,XD-2308,FEATURE,Done,MEDIUM,"""Create sample app to demonstrate Kafka integration""","""As a user, I'd like to have a sample app (GitHub project) so that I can use it as a reference while provisioning Spring XD cluster with Kafka.  Consider: * Kafka as message bus * Kafka as source  ""","""""""As a user, I'd like to have a sample app (GitHub project) so that I can use it as a reference while provisioning Spring XD cluster with Kafka.  Consider: * Kafka as message bus * Kafka as source  """"""",,8.0
1,XD-2307,FEATURE,Done,MEDIUM,"""Add support for PHD 2.1 (XD 1.1 M1 Release)""","""*XD 1.1 M1 Release + PHD 2.1 Upgrade - Action Items:*  * Update to SHDP 2.1.M2,  * Add Hadoop 2.5 (hadoop25) * Remove hadoop22 * Remove PHD 1.0 (phd1)  * Change PHD 2.x from phd20 to phd21 * Test PHD 2.0 with phd21""","""""""*XD 1.1 M1 Release + PHD 2.1 Upgrade - Action Items:*  * Update to SHDP 2.1.M2,  * Add Hadoop 2.5 (hadoop25) * Remove hadoop22 * Remove PHD 1.0 (phd1)  * Change PHD 2.x from phd20 to phd21 * Test PHD 2.0 with phd21""""""",,3.0
1,XD-2306,FEATURE,Done,MEDIUM,"""Add support to install custom module archive""","""As a user, I'd like to push the custom module (built as uber-jar) via a REST API so that I can install the custom module in cluster. ""","""""""As a user, I'd like to push the custom module (built as uber-jar) via a REST API so that I can install the custom module in cluster. """"""",,8.0
1,XD-2305,MAINTENANCE,Done,MEDIUM,"""POC for Spark Integration""","""*Spike Scope:*  * Experiment with identified options * POC with the logical integration choice""","""""""*Spike Scope:*  * Experiment with identified options * POC with the logical integration choice""""""",,8.0
1,XD-2304,FEATURE,Done,MEDIUM,"""Research refactoring effort for Kafka source to use simple consumer instead of high-level API""","""As a user, I'd like to use Kafka source through simple consumer API (as opposed to high-level) so that I can gain full control to offsets and partition assignment deterministically.  *Spike scope*: - Study simple consumer API functionality - Document findings, approach and next steps""","""""""As a user, I'd like to use Kafka source through simple consumer API (as opposed to high-level) so that I can gain full control to offsets and partition assignment deterministically.  *Spike scope*: - Study simple consumer API functionality - Document findings, approach and next steps""""""",,8.0
1,XD-2302,FEATURE,Done,MEDIUM,"""UI: Update AngularJS to v1.3""",""" * https://docs.angularjs.org/guide/migration#migrating-from-1-2-to-1-3 * http://angularjs.blogspot.com/2014/10/ng-europe-angular-13-and-beyond.html * http://angularjs.blogspot.com/2014/10/angularjs-130-superluminal-nudge.html  """,""""""" * https://docs.angularjs.org/guide/migration#migrating-from-1-2-to-1-3 * http://angularjs.blogspot.com/2014/10/ng-europe-angular-13-and-beyond.html * http://angularjs.blogspot.com/2014/10/angularjs-130-superluminal-nudge.html  """"""",,4.0
1,XD-2301,FEATURE,Done,MEDIUM,"""Research Spark integration options""","""*Spike scope:*  * Brainstorm * Identify options * Document ""","""""""*Spike scope:*  * Brainstorm * Identify options * Document """"""",,8.0
1,XD-2300,MAINTENANCE,Done,MEDIUM,"""Document automatic declaration of DQL for each consumer queue""","""The scope is to document the automatic creation and binding of DLQ for each 'pipe' (consumer queue).""","""""""The scope is to document the automatic creation and binding of DLQ for each 'pipe' (consumer queue).""""""",,0.0